---
# ----------------------------------------------------------------------------
#
#     ***     AUTO GENERATED CODE    ***    Type: MMv1     ***
#
# ----------------------------------------------------------------------------
#
#     This file is automatically generated by Magic Modules and manual
#     changes will be clobbered when the file is regenerated.
#
#     Please read more about how to change this file in
#     .github/CONTRIBUTING.md.
#
# ----------------------------------------------------------------------------
subcategory: "Vertex AI"
description: |-
  An instance of a Schedule periodically schedules runs to make
  API calls based on user specified time specification and API
  request type.
---

# google\_vertex\_ai\_schedule

An instance of a Schedule periodically schedules runs to make
API calls based on user specified time specification and API
request type.


To get more information about Schedule, see:

* [API documentation](https://cloud.google.com/vertex-ai/docs/reference/rest/v1beta1/projects.locations.schedules)
* How-to Guides
    * [Official Documentation](https://cloud.google.com/vertex-ai/docs)

## Argument Reference

The following arguments are supported:


* `name` -
  (Required)
  The resource name of the Schedule.

* `display_name` -
  (Required)
  User provided name of the Schedule. The name can be up to 128 characters long and can consist of any UTF-8 characters.

* `max_concurrent_run_count` -
  (Required)
  Maximum number of runs that can be started concurrently for this
  Schedule. This is the limit for starting the scheduled requests and not the
  execution of the operations/jobs created by the requests (if applicable).

* `region` -
  (Required)
  Region where the scheduler job resides


- - -


* `cron` -
  (Optional)
  The scheduled run time based on the user-specified schedule.
  A timestamp in RFC3339 UTC "Zulu" format, with nanosecond
  resolution and up to nine fractional digits. Examples:
  "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".

* `start_time` -
  (Optional)
  Timestamp after which the first run can be scheduled. Default to
  Schedule create time if not specified. A timestamp in RFC3339 UTC "Zulu"
  format, with nanosecond resolution and up to nine fractional digits.
  Examples: "2014-10-02T15:01:23Z" and "2014-10-02T15:01:23.045123456Z".

* `end_time` -
  (Optional)
  Timestamp after which no new runs can be scheduled. If specified,
  The schedule will be completed when either endTime is reached or when
  scheduled_run_count >= maxRunCount. If not specified, new runs will keep
  getting scheduled until this Schedule is paused or deleted. Already scheduled
  runs will be allowed to complete. Unset if not specified. A timestamp in
  RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine
  fractional digits. Examples: "2014-10-02T15:01:23Z" and
  "2014-10-02T15:01:23.045123456Z".

* `max_run_count` -
  (Optional)
  Maximum run count of the schedule. If specified, The schedule will
  be completed when either startedRunCount >= maxRunCount or when endTime is
  reached. If not specified, new runs will keep getting scheduled until this
  Schedule is paused or deleted. Already scheduled runs will be allowed to
  complete. Unset if not specified.

* `paused` -
  (Optional)
  Sets the job to a paused state. Jobs default to being enabled when this property is not set.

* `catch_up` -
  (Optional)
  Whether to backfill missed runs when the schedule is resumed from PAUSED state.
  If set to true, all missed runs will be scheduled. New runs will be scheduled after the
  backfill is complete. Default to false.

* `allow_queueing` -
  (Optional)
  Whether new scheduled runs can be queued when max_concurrent_runs
  limit is reached. If set to true, new runs will be queued instead of skipped.
  Default to false.

* `create_pipeline_job_request` -
  (Optional)
  Request for PipelineService.CreatePipelineJob.
  CreatePipelineJobRequest.parent field is required
  (format: projects/{project}/locations/{location}).
  Structure is [documented below](#nested_create_pipeline_job_request).

* `project` - (Optional) The ID of the project in which the resource belongs.
    If it is not provided, the provider project is used.


<a name="nested_create_pipeline_job_request"></a>The `create_pipeline_job_request` block supports:

* `parent` -
  (Required)
  The resource name of the Location to create the PipelineJob in. Format: projects/{project}/locations/{location}

* `pipeline_job` -
  (Required)
  The PipelineJob to create.
  Structure is [documented below](#nested_pipeline_job).


<a name="nested_pipeline_job"></a>The `pipeline_job` block supports:

* `display_name` -
  (Optional)
  The display name of the Pipeline. The name can be up to 128 characters long
  and can consist of any UTF-8 characters.

* `pipeline_spec` -
  (Optional)
  The spec of the pipeline. Expects a JSON-encoded string.

* `labels` -
  (Optional)
  The labels with user-defined metadata to organize PipelineJob.
  Label keys and values can be no longer than 64 characters
  (Unicode codepoints), can only contain lowercase letters,
  numeric characters, underscores and dashes. International
  characters are allowed. See https://goo.gl/xmQnxf for more
  information and examples of labels.

* `runtime_config` -
  (Required)
  Runtime config of the pipeline.
  Structure is [documented below](#nested_runtime_config).

* `service_account` -
  (Optional)
  The service account that the pipeline workload runs as.
  If not specified, the Compute Engine default service account
  in the project will be used.
  See https://cloud.google.com/compute/docs/access/service-accounts#default_service_account
  Users starting the pipeline must have the iam.serviceAccounts.actAs
  permission on this service account.

* `network` -
  (Optional)
  The full name of the Compute Engine network to which the Pipeline Job's workload
  should be peered. For example, projects/12345/global/networks/myVPC. Format is
  of the form projects/{project}/global/networks/{network}. Where {project} is a
  project number, as in 12345, and {network} is a network name.
  Private services access must already be configured for the network.
  Pipeline job will apply the network configuration to the Google Cloud resources
  being launched, if applied, such as Vertex AI Training or Dataflow job.
  If left unspecified, the workload is not peered with any network.

* `template_uri` -
  (Optional)
  A template uri from where the PipelineJob.pipeline_spec, if empty, will be downloaded.


<a name="nested_runtime_config"></a>The `runtime_config` block supports:

* `parameters` -
  (Optional)
  Deprecated. Use RuntimeConfig.parameter_values instead. The runtime parameters of the PipelineJob.
  The parameters will be passed into PipelineJob.pipeline_spec to replace the placeholders at runtime.
  This field is used by pipelines built using PipelineJob.pipeline_spec.schema_version 2.0.0 or lower,
  such as pipelines built using Kubeflow Pipelines SDK 1.8 or lower. Expects a JSON-encoded string.

* `gcs_output_directory` -
  (Required)
  A path in a Cloud Storage bucket, which will be treated as the root
  output directory of the pipeline. It is used by the system to generate the paths
  of output artifacts. The artifact paths are generated with a sub-path pattern
  {job_id}/{taskId}/{output_key} under the specified output directory.
  The service account specified in this pipeline must have the storage.objects.get
  and storage.objects.create permissions for this bucket.

* `parameter_values` -
  (Optional)
  The runtime parameters of the PipelineJob. The parameters will be passed into
  PipelineJob.pipeline_spec to replace the placeholders at runtime. This field
  is used by pipelines built using PipelineJob.pipeline_spec.schema_version 2.1.0,
  such as pipelines built using Kubeflow Pipelines SDK 1.9 or higher and the v2 DSL.
  Expects a JSON-encoded string.

* `failure_policy` -
  (Optional)
  Represents the failure policy of a pipeline. Currently, the default of a pipeline
  is that the pipeline will continue to run until no more tasks can be executed,
  also known as PIPELINE_FAILURE_POLICY_FAIL_SLOW. However, if a pipeline is set
  to PIPELINE_FAILURE_POLICY_FAIL_FAST, it will stop scheduling any new tasks when
  a task has failed. Any scheduled tasks will continue to completion.
  Default value is `PIPELINE_FAILURE_POLICY_UNSPECIFIED`.
  Possible values are: `PIPELINE_FAILURE_POLICY_UNSPECIFIED`, `PIPELINE_FAILURE_POLICY_FAIL_SLOW`, `PIPELINE_FAILURE_POLICY_FAIL_FAST`.

* `input_artifacts` -
  (Optional)
  The runtime artifacts of the PipelineJob. The key will be the input artifact name and the value
  would be an Artifact resource id from MLMD, which is the last portion of an artifact resource name:
  projects/{project}/locations/{location}/metadataStores/default/artifacts/{artifactId}.
  The artifact must stay within the same project, location and default metadatastore as the pipeline.

## Attributes Reference

In addition to the arguments listed above, the following computed attributes are exported:

* `id` - an identifier for the resource with format `projects/{{project}}/locations/{{region}}/schedules/{{name}}`

* `state` -
  State of the job.


## Timeouts

This resource provides the following
[Timeouts](https://developer.hashicorp.com/terraform/plugin/sdkv2/resources/retries-and-customizable-timeouts) configuration options:

- `create` - Default is 20 minutes.
- `update` - Default is 20 minutes.
- `delete` - Default is 20 minutes.

## Import


Schedule can be imported using any of these accepted formats:

```
$ terraform import google_vertex_ai_schedule.default projects/{{project}}/locations/{{region}}/schedules/{{name}}
$ terraform import google_vertex_ai_schedule.default {{project}}/{{region}}/{{name}}
$ terraform import google_vertex_ai_schedule.default {{region}}/{{name}}
$ terraform import google_vertex_ai_schedule.default {{name}}
```

## User Project Overrides

This resource supports [User Project Overrides](https://registry.terraform.io/providers/hashicorp/google/latest/docs/guides/provider_reference#user_project_override).
