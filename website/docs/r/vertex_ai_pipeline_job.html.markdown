---
# ----------------------------------------------------------------------------
#
#     ***     AUTO GENERATED CODE    ***    Type: MMv1     ***
#
# ----------------------------------------------------------------------------
#
#     This file is automatically generated by Magic Modules and manual
#     changes will be clobbered when the file is regenerated.
#
#     Please read more about how to change this file in
#     .github/CONTRIBUTING.md.
#
# ----------------------------------------------------------------------------
subcategory: "Vertex AI"
description: |-
  An instance of a machine learning PipelineJob.
---

# google\_vertex\_ai\_pipeline\_job

An instance of a machine learning PipelineJob.


To get more information about PipelineJob, see:

* [API documentation](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.pipelineJobs)
* How-to Guides
    * [Official Documentation](https://cloud.google.com/vertex-ai/docs)

<div class = "oics-button" style="float: right; margin: 0 0 -15px">
  <a href="https://console.cloud.google.com/cloudshell/open?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2Fterraform-google-modules%2Fdocs-examples.git&cloudshell_working_dir=vertex_ai_pipeline_job_basic&cloudshell_image=gcr.io%2Fcloudshell-images%2Fcloudshell%3Alatest&open_in_editor=main.tf&cloudshell_print=.%2Fmotd&cloudshell_tutorial=.%2Ftutorial.md" target="_blank">
    <img alt="Open in Cloud Shell" src="//gstatic.com/cloudssh/images/open-btn.svg" style="max-height: 44px; margin: 32px auto; max-width: 100%;">
  </a>
</div>
## Example Usage - Vertex Ai Pipeline Job Basic


```hcl
resource "google_vertex_ai_pipeline_job" "pipeline_job" {
  name = "pipeline-job"
  location = "us-central1"
  runtime_config {
    gcs_output_directory = google_storage_bucket.bucket.url
    parameter_values = {
      model_display_name = "The display name for your model in the UI"
      large_model_reference = "text-bison@001"
      train_steps = 20
      project = data.google_project.project.project_id
      location = "us-central1"
      dataset_uri = google_storage_bucket_object.object.media_link
    }
  }
  template_uri = "https://us-kfp.pkg.dev/ml-pipeline/large-language-model-pipelines/tune-large-model/v2.0.0"
}

data "google_project" "project" {}

resource "google_storage_bucket" "bucket" {
  name     = "pipeline-job-bucket"
  location = "US"
}

resource "google_storage_bucket_object" "object" {
  name   = "pipeline-job-dataset"
  bucket = google_storage_bucket.bucket.name
  source = "./test-fixtures/dataset.jsonl"
}
```

## Argument Reference

The following arguments are supported:


* `name` -
  (Required)
  The resource name of the Pipeline. This value should be less than 128 characters, and valid characters are /[a-z][0-9]-/

* `location` -
  (Required)
  The location for the resource.


- - -


* `display_name` -
  (Optional)
  The display name of the Pipeline. The name can be up to 128 characters long and can consist of any UTF-8 characters.

* `labels` -
  (Optional)
  The labels with user-defined metadata to organize PipelineJob. Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters, underscores and dashes. International characters are allowed. See https://goo.gl/xmQnxf for more information and examples of labels. Note there is some reserved label key for Vertex AI Pipelines. - vertex-ai-pipelines-run-billing-id, user set value will get overrided.

* `runtime_config` -
  (Optional)
  Runtime config of the pipeline.
  Structure is [documented below](#nested_runtime_config).

* `encryption_spec` -
  (Optional)
  Customer-managed encryption key spec for a pipelineJob. If set, this PipelineJob and all of its sub-resources will be secured by this key.
  Structure is [documented below](#nested_encryption_spec).

* `service_account` -
  (Optional)
  The service account that the pipeline workload runs as. If not specified, the Compute Engine default service account in the project will be used. See https://cloud.google.com/compute/docs/access/service-accounts#default_service_account Users starting the pipeline must have the iam.serviceAccounts.actAs permission on this service account.

* `network` -
  (Optional)
  The full name of the Compute Engine network to which the Pipeline Job's workload should be peered. For example, projects/12345/global/networks/myVPC. Format is of the form projects/{project}/global/networks/{network}. Where {project} is a project number, as in 12345, and {network} is a network name.
  Private services access must already be configured for the network. Pipeline job will apply the network configuration to the Google Cloud resources being launched, if applied, such as Vertex AI Training or Dataflow job. If left unspecified, the workload is not peered with any network.

* `template_uri` -
  (Optional)
  A template uri from where the PipelineJob.pipeline_spec, if empty, will be downloaded.

* `project` - (Optional) The ID of the project in which the resource belongs.
    If it is not provided, the provider project is used.


<a name="nested_runtime_config"></a>The `runtime_config` block supports:

* `gcs_output_directory` -
  (Required)
  A path in a Cloud Storage bucket, which will be treated as the root output directory of the pipeline. It is used by the system to generate the paths of output artifacts. The artifact paths are generated with a sub-path pattern {job_id}/{taskId}/{output_key} under the specified output directory. The service account specified in this pipeline must have the storage.objects.get and storage.objects.create permissions for this bucket.

* `parameter_values` -
  (Optional)
  The runtime parameters of the PipelineJob. The parameters will be passed into PipelineJob.pipeline_spec to replace the placeholders at runtime. This field is used by pipelines built using PipelineJob.pipeline_spec.schema_version 2.1.0, such as pipelines built using Kubeflow Pipelines SDK 1.9 or higher and the v2 DSL.

* `failure_policy` -
  (Optional)
  Represents the failure policy of a pipeline. Currently, the default of a pipeline is that the pipeline will continue to run until no more tasks can be executed, also known as PIPELINE_FAILURE_POLICY_FAIL_SLOW. However, if a pipeline is set to PIPELINE_FAILURE_POLICY_FAIL_FAST, it will stop scheduling any new tasks when a task has failed. Any scheduled tasks will continue to completion.
  Possible values are: `PIPELINE_FAILURE_POLICY_FAIL_SLOW`, `PIPELINE_FAILURE_POLICY_FAIL_FAST`.

* `input_artifacts` -
  (Optional)
  The runtime artifacts of the PipelineJob.
  Structure is [documented below](#nested_input_artifacts).


<a name="nested_input_artifacts"></a>The `input_artifacts` block supports:

* `artifact` - (Required) The identifier for this object. Format specified above.

* `version` -
  (Optional)
  The type of an input artifact.

<a name="nested_encryption_spec"></a>The `encryption_spec` block supports:

* `kms_key_name` -
  (Required)
  The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource. Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the compute resource is created.

## Attributes Reference

In addition to the arguments listed above, the following computed attributes are exported:

* `id` - an identifier for the resource with format `projects/{{project}}/locations/{{location}}/pipelineJobs/{{name}}`

* `state` -
  The detailed state of the job.

* `template_metadata` -
  Pipeline template metadata. Will fill up fields if PipelineJob.template_uri is from supported template registry.
  Structure is [documented below](#nested_template_metadata).

* `schedule_name` -
  The schedule resource name. Only returned if the Pipeline is created by Schedule API.


<a name="nested_template_metadata"></a>The `template_metadata` block contains:

* `version` -
  (Output)
  The version_name in artifact registry. Will always be presented in output if the PipelineJob.template_uri is from supported template registry. Format is "sha256:abcdef123456...".

## Timeouts

This resource provides the following
[Timeouts](https://developer.hashicorp.com/terraform/plugin/sdkv2/resources/retries-and-customizable-timeouts) configuration options:

- `create` - Default is 20 minutes.
- `delete` - Default is 20 minutes.

## Import


PipelineJob can be imported using any of these accepted formats:

```
$ terraform import google_vertex_ai_pipeline_job.default projects/{{project}}/locations/{{location}}/pipelineJobs/{{name}}
$ terraform import google_vertex_ai_pipeline_job.default {{project}}/{{location}}/{{name}}
$ terraform import google_vertex_ai_pipeline_job.default {{location}}/{{name}}
```

## User Project Overrides

This resource supports [User Project Overrides](https://registry.terraform.io/providers/hashicorp/google/latest/docs/guides/provider_reference#user_project_override).
