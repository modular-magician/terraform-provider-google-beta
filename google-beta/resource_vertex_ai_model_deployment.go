// ----------------------------------------------------------------------------
//
//     ***     AUTO GENERATED CODE    ***    Type: MMv1     ***
//
// ----------------------------------------------------------------------------
//
//     This file is automatically generated by Magic Modules and manual
//     changes will be clobbered when the file is regenerated.
//
//     Please read more about how to change this file in
//     .github/CONTRIBUTING.md.
//
// ----------------------------------------------------------------------------

package google

import (
	"fmt"
	"log"
	"reflect"
	"time"

	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/schema"
)

func resourceVertexAIModelDeployment() *schema.Resource {
	return &schema.Resource{
		Create: resourceVertexAIModelDeploymentCreate,
		Read:   resourceVertexAIModelDeploymentRead,
		Delete: resourceVertexAIModelDeploymentDelete,

		Importer: &schema.ResourceImporter{
			State: resourceVertexAIModelDeploymentImport,
		},

		Timeouts: &schema.ResourceTimeout{
			Create: schema.DefaultTimeout(20 * time.Minute),
			Delete: schema.DefaultTimeout(20 * time.Minute),
		},

		Schema: map[string]*schema.Schema{
			"dedicated_resources": {
				Type:        schema.TypeList,
				Required:    true,
				ForceNew:    true,
				Description: `A description of resources that are dedicated to the DeployedModel, and that need a higher degree of manual configuration.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"machine_spec": {
							Type:        schema.TypeList,
							Required:    true,
							ForceNew:    true,
							Description: `Required. Immutable. The specification of a single machine used by the prediction.`,
							MaxItems:    1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"machine_type": {
										Type:        schema.TypeString,
										Required:    true,
										ForceNew:    true,
										Description: `Immutable. The type of the machine. See the [list of machine types supported for prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types) See the [list of machine types supported for custom training](https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types). For DeployedModel this field is optional, and the default value is 'n1-standard-2'. For BatchPredictionJob or as part of WorkerPoolSpec this field is required. TODO(rsurowka): Try to better unify the required vs optional.`,
									},
								},
							},
						},
						"min_replica_count": {
							Type:        schema.TypeInt,
							Required:    true,
							ForceNew:    true,
							Description: `Required. Immutable. The minimum number of machine replicas this DeployedModel will be always deployed on. This value must be greater than or equal to 1. If traffic against the DeployedModel increases, it may dynamically be deployed onto more replicas, and as traffic decreases, some of these extra replicas may be freed.`,
						},
						"max_replica_count": {
							Type:        schema.TypeInt,
							Optional:    true,
							ForceNew:    true,
							Description: `Immutable. The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, will use min_replica_count as the default value. The value of this field impacts the charge against Vertex CPU and GPU quotas. Specifically, you will be charged for max_replica_count * number of cores in the selected machine type) and (max_replica_count * number of GPUs per replica in the selected machine type).`,
						},
					},
				},
			},
			"endpoint": {
				Type:             schema.TypeString,
				Required:         true,
				ForceNew:         true,
				DiffSuppressFunc: compareSelfLinkOrResourceName,
				Description:      `The name of the endpoint to deploy to`,
			},
			"model": {
				Type:        schema.TypeString,
				Required:    true,
				ForceNew:    true,
				Description: `The name of the model to deploy`,
			},
			"location": {
				Type:        schema.TypeString,
				Optional:    true,
				ForceNew:    true,
				Description: `The location of the endpoint`,
			},
			"deployed_model_id": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The deployed ID of the model in the endpoint`,
			},
			"project": {
				Type:     schema.TypeString,
				Optional: true,
				Computed: true,
				ForceNew: true,
			},
		},
		UseJSONNumber: true,
	}
}

func resourceVertexAIModelDeploymentCreate(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*Config)
	userAgent, err := generateUserAgentString(d, config.userAgent)
	if err != nil {
		return err
	}

	obj := make(map[string]interface{})
	modelProp, err := expandNestedVertexAIModelDeploymentModel(d.Get("model"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("model"); !isEmptyValue(reflect.ValueOf(modelProp)) && (ok || !reflect.DeepEqual(v, modelProp)) {
		obj["model"] = modelProp
	}
	dedicatedResourcesProp, err := expandNestedVertexAIModelDeploymentDedicatedResources(d.Get("dedicated_resources"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("dedicated_resources"); !isEmptyValue(reflect.ValueOf(dedicatedResourcesProp)) && (ok || !reflect.DeepEqual(v, dedicatedResourcesProp)) {
		obj["dedicatedResources"] = dedicatedResourcesProp
	}

	obj, err = resourceVertexAIModelDeploymentEncoder(d, meta, obj)
	if err != nil {
		return err
	}

	url, err := replaceVars(d, config, "{{VertexAIBasePath}}projects/{{project}}/locations/{{location}}/endpoints/{{endpoint}}:deployModel")
	if err != nil {
		return err
	}

	log.Printf("[DEBUG] Creating new ModelDeployment: %#v", obj)
	billingProject := ""

	project, err := getProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for ModelDeployment: %s", err)
	}
	billingProject = project

	// err == nil indicates that the billing_project value was found
	if bp, err := getBillingProject(d, config); err == nil {
		billingProject = bp
	}

	res, err := sendRequestWithTimeout(config, "POST", billingProject, url, userAgent, obj, d.Timeout(schema.TimeoutCreate))
	if err != nil {
		return fmt.Errorf("Error creating ModelDeployment: %s", err)
	}

	// Store the ID now
	id, err := replaceVars(d, config, "{{project}}/{{location}}/{{endpoint}}/{{model}}")
	if err != nil {
		return fmt.Errorf("Error constructing id: %s", err)
	}
	d.SetId(id)

	err = vertexAIOperationWaitTime(
		config, res, project, "Creating ModelDeployment", userAgent,
		d.Timeout(schema.TimeoutCreate))

	if err != nil {
		// The resource didn't actually create
		d.SetId("")
		return fmt.Errorf("Error waiting to create ModelDeployment: %s", err)
	}

	log.Printf("[DEBUG] Finished creating ModelDeployment %q: %#v", d.Id(), res)

	return resourceVertexAIModelDeploymentRead(d, meta)
}

func resourceVertexAIModelDeploymentRead(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*Config)
	userAgent, err := generateUserAgentString(d, config.userAgent)
	if err != nil {
		return err
	}

	url, err := replaceVars(d, config, "{{VertexAIBasePath}}projects/{{project}}/locations/{{location}}/endpoints/{{endpoint}}")
	if err != nil {
		return err
	}

	billingProject := ""

	project, err := getProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for ModelDeployment: %s", err)
	}
	billingProject = project

	// err == nil indicates that the billing_project value was found
	if bp, err := getBillingProject(d, config); err == nil {
		billingProject = bp
	}

	res, err := sendRequest(config, "GET", billingProject, url, userAgent, nil)
	if err != nil {
		return handleNotFoundError(err, d, fmt.Sprintf("VertexAIModelDeployment %q", d.Id()))
	}

	res, err = flattenNestedVertexAIModelDeployment(d, meta, res)
	if err != nil {
		return err
	}

	if res == nil {
		// Object isn't there any more - remove it from the state.
		log.Printf("[DEBUG] Removing VertexAIModelDeployment because it couldn't be matched.")
		d.SetId("")
		return nil
	}

	if err := d.Set("project", project); err != nil {
		return fmt.Errorf("Error reading ModelDeployment: %s", err)
	}

	if err := d.Set("deployed_model_id", flattenNestedVertexAIModelDeploymentDeployedModelId(res["id"], d, config)); err != nil {
		return fmt.Errorf("Error reading ModelDeployment: %s", err)
	}
	if err := d.Set("dedicated_resources", flattenNestedVertexAIModelDeploymentDedicatedResources(res["dedicatedResources"], d, config)); err != nil {
		return fmt.Errorf("Error reading ModelDeployment: %s", err)
	}

	return nil
}

func resourceVertexAIModelDeploymentDelete(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*Config)
	userAgent, err := generateUserAgentString(d, config.userAgent)
	if err != nil {
		return err
	}

	billingProject := ""

	project, err := getProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for ModelDeployment: %s", err)
	}
	billingProject = project

	url, err := replaceVars(d, config, "{{VertexAIBasePath}}projects/{{project}}/locations/{{location}}/endpoints/{{endpoint}}:undeployModel")
	if err != nil {
		return err
	}

	var obj map[string]interface{}
	obj = map[string]any{
		"deployedModelId": d.Get("deployed_model_id"),
	}
	log.Printf("[DEBUG] Deleting ModelDeployment %q", d.Id())

	// err == nil indicates that the billing_project value was found
	if bp, err := getBillingProject(d, config); err == nil {
		billingProject = bp
	}

	res, err := sendRequestWithTimeout(config, "POST", billingProject, url, userAgent, obj, d.Timeout(schema.TimeoutDelete))
	if err != nil {
		return handleNotFoundError(err, d, "ModelDeployment")
	}

	err = vertexAIOperationWaitTime(
		config, res, project, "Deleting ModelDeployment", userAgent,
		d.Timeout(schema.TimeoutDelete))

	if err != nil {
		return err
	}

	log.Printf("[DEBUG] Finished deleting ModelDeployment %q: %#v", d.Id(), res)
	return nil
}

func resourceVertexAIModelDeploymentImport(d *schema.ResourceData, meta interface{}) ([]*schema.ResourceData, error) {
	config := meta.(*Config)
	if err := parseImportId([]string{
		"(?P<project>[^/]+)/(?P<location>[^/]+)/(?P<endpoint>[^/]+)/(?P<model>.+)",
	}, d, config); err != nil {
		return nil, err
	}

	// Replace import id for the resource id
	id, err := replaceVars(d, config, "{{project}}/{{location}}/{{endpoint}}/{{model}}")
	if err != nil {
		return nil, fmt.Errorf("Error constructing id: %s", err)
	}
	d.SetId(id)

	return []*schema.ResourceData{d}, nil
}

func flattenNestedVertexAIModelDeploymentDeployedModelId(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenNestedVertexAIModelDeploymentDedicatedResources(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["machine_spec"] =
		flattenNestedVertexAIModelDeploymentDedicatedResourcesMachineSpec(original["machineSpec"], d, config)
	transformed["min_replica_count"] =
		flattenNestedVertexAIModelDeploymentDedicatedResourcesMinReplicaCount(original["minReplicaCount"], d, config)
	transformed["max_replica_count"] =
		flattenNestedVertexAIModelDeploymentDedicatedResourcesMaxReplicaCount(original["maxReplicaCount"], d, config)
	return []interface{}{transformed}
}
func flattenNestedVertexAIModelDeploymentDedicatedResourcesMachineSpec(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["machine_type"] =
		flattenNestedVertexAIModelDeploymentDedicatedResourcesMachineSpecMachineType(original["machineType"], d, config)
	return []interface{}{transformed}
}
func flattenNestedVertexAIModelDeploymentDedicatedResourcesMachineSpecMachineType(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenNestedVertexAIModelDeploymentDedicatedResourcesMinReplicaCount(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := stringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenNestedVertexAIModelDeploymentDedicatedResourcesMaxReplicaCount(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := stringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func expandNestedVertexAIModelDeploymentModel(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandNestedVertexAIModelDeploymentDedicatedResources(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedMachineSpec, err := expandNestedVertexAIModelDeploymentDedicatedResourcesMachineSpec(original["machine_spec"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMachineSpec); val.IsValid() && !isEmptyValue(val) {
		transformed["machineSpec"] = transformedMachineSpec
	}

	transformedMinReplicaCount, err := expandNestedVertexAIModelDeploymentDedicatedResourcesMinReplicaCount(original["min_replica_count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMinReplicaCount); val.IsValid() && !isEmptyValue(val) {
		transformed["minReplicaCount"] = transformedMinReplicaCount
	}

	transformedMaxReplicaCount, err := expandNestedVertexAIModelDeploymentDedicatedResourcesMaxReplicaCount(original["max_replica_count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMaxReplicaCount); val.IsValid() && !isEmptyValue(val) {
		transformed["maxReplicaCount"] = transformedMaxReplicaCount
	}

	return transformed, nil
}

func expandNestedVertexAIModelDeploymentDedicatedResourcesMachineSpec(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedMachineType, err := expandNestedVertexAIModelDeploymentDedicatedResourcesMachineSpecMachineType(original["machine_type"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMachineType); val.IsValid() && !isEmptyValue(val) {
		transformed["machineType"] = transformedMachineType
	}

	return transformed, nil
}

func expandNestedVertexAIModelDeploymentDedicatedResourcesMachineSpecMachineType(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandNestedVertexAIModelDeploymentDedicatedResourcesMinReplicaCount(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandNestedVertexAIModelDeploymentDedicatedResourcesMaxReplicaCount(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func resourceVertexAIModelDeploymentEncoder(d *schema.ResourceData, meta interface{}, obj map[string]interface{}) (map[string]interface{}, error) {
	newObj := make(map[string]interface{})
	newObj["deployedModel"] = obj
	return newObj, nil
}

func flattenNestedVertexAIModelDeployment(d *schema.ResourceData, meta interface{}, res map[string]interface{}) (map[string]interface{}, error) {
	var v interface{}
	var ok bool

	v, ok = res["deployedModels"]
	if !ok || v == nil {
		return nil, nil
	}

	switch v.(type) {
	case []interface{}:
		break
	case map[string]interface{}:
		// Construct list out of single nested resource
		v = []interface{}{v}
	default:
		return nil, fmt.Errorf("expected list or map for value deployedModels. Actual value: %v", v)
	}

	_, item, err := resourceVertexAIModelDeploymentFindNestedObjectInList(d, meta, v.([]interface{}))
	if err != nil {
		return nil, err
	}
	return item, nil
}

func resourceVertexAIModelDeploymentFindNestedObjectInList(d *schema.ResourceData, meta interface{}, items []interface{}) (index int, item map[string]interface{}, err error) {

	// Search list for this resource.
	for idx, itemRaw := range items {
		if itemRaw == nil {
			continue
		}
		item := itemRaw.(map[string]interface{})

		log.Printf("[DEBUG] Found item for resource %q: %#v)", d.Id(), item)
		return idx, item, nil
	}
	return -1, nil, nil
}
