// ----------------------------------------------------------------------------
//
//     ***     AUTO GENERATED CODE    ***    Type: MMv1     ***
//
// ----------------------------------------------------------------------------
//
//     This file is automatically generated by Magic Modules and manual
//     changes will be clobbered when the file is regenerated.
//
//     Please read more about how to change this file in
//     .github/CONTRIBUTING.md.
//
// ----------------------------------------------------------------------------

package google

import (
	"fmt"
	"log"
	"reflect"
	"strings"
	"time"

	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/schema"
)

func resourceVertexAIModel() *schema.Resource {
	return &schema.Resource{
		Create: resourceVertexAIModelCreate,
		Read:   resourceVertexAIModelRead,
		Update: resourceVertexAIModelUpdate,
		Delete: resourceVertexAIModelDelete,

		Importer: &schema.ResourceImporter{
			State: resourceVertexAIModelImport,
		},

		Timeouts: &schema.ResourceTimeout{
			Create: schema.DefaultTimeout(20 * time.Minute),
			Update: schema.DefaultTimeout(20 * time.Minute),
			Delete: schema.DefaultTimeout(20 * time.Minute),
		},

		Schema: map[string]*schema.Schema{
			"container_spec": {
				Type:        schema.TypeList,
				Required:    true,
				ForceNew:    true,
				Description: `The specification of the container that is to be used when deploying this Model. The specification is ingested upon ModelService.UploadModel, and all binaries it contains are copied and stored internally by Vertex AI. Not present for AutoML Models.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"image_uri": {
							Type:        schema.TypeString,
							Required:    true,
							ForceNew:    true,
							Description: `Required. Immutable. URI of the Docker image to be used as the custom container for serving predictions. This URI must identify an image in Artifact Registry or Container Registry. Learn more about the [container publishing requirements](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#publishing), including permissions requirements for the Vertex AI Service Agent. The container image is ingested upon ModelService.UploadModel, stored internally, and this original path is afterwards not used. To learn about the requirements for the Docker image itself, see [Custom container requirements](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#). You can use the URI to one of Vertex AI's [pre-built container images for prediction](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers) in this field.`,
						},
						"args": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `Immutable. Specifies arguments for the command that runs when the container starts. This overrides the container's ['CMD'](https://docs.docker.com/engine/reference/builder/#cmd). Specify this field as an array of executable and arguments, similar to a Docker 'CMD''s "default parameters" form. If you don't specify this field but do specify the command field, then the command from the 'command' field runs without any additional arguments. See the [Kubernetes documentation about how the 'command' and 'args' fields interact with a container's 'ENTRYPOINT' and 'CMD'](https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#notes). If you don't specify this field and don't specify the 'command' field, then the container's ['ENTRYPOINT'](https://docs.docker.com/engine/reference/builder/#cmd) and 'CMD' determine what runs based on their default behavior. See the Docker documentation about [how 'CMD' and 'ENTRYPOINT' interact](https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact). In this field, you can reference [environment variables set by Vertex AI](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables) and environment variables set in the env field. You cannot reference environment variables set in the Docker image. In order for environment variables to be expanded, reference them by using the following syntax: '$(VARIABLE_NAME)' Note that this differs from Bash variable expansion, which does not use parentheses. If a variable cannot be resolved, the reference in the input string is used unchanged. To avoid variable expansion, you can escape this syntax with '$$'; for example: '$$(VARIABLE_NAME)' This field corresponds to the 'args' field of the Kubernetes Containers [v1 core API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).`,
							Elem: &schema.Schema{
								Type: schema.TypeString,
							},
						},
						"command": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `Immutable. Specifies the command that runs when the container starts. This overrides the container's [ENTRYPOINT](https://docs.docker.com/engine/reference/builder/#entrypoint). Specify this field as an array of executable and arguments, similar to a Docker 'ENTRYPOINT''s "exec" form, not its "shell" form. If you do not specify this field, then the container's 'ENTRYPOINT' runs, in conjunction with the args field or the container's ['CMD'](https://docs.docker.com/engine/reference/builder/#cmd), if either exists. If this field is not specified and the container does not have an 'ENTRYPOINT', then refer to the Docker documentation about [how 'CMD' and 'ENTRYPOINT' interact](https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact). If you specify this field, then you can also specify the 'args' field to provide additional arguments for this command. However, if you specify this field, then the container's 'CMD' is ignored. See the [Kubernetes documentation about how the 'command' and 'args' fields interact with a container's 'ENTRYPOINT' and 'CMD'](https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#notes). In this field, you can reference [environment variables set by Vertex AI](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables) and environment variables set in the env field. You cannot reference environment variables set in the Docker image. In order for environment variables to be expanded, reference them by using the following syntax: '$(VARIABLE_NAME)' Note that this differs from Bash variable expansion, which does not use parentheses. If a variable cannot be resolved, the reference in the input string is used unchanged. To avoid variable expansion, you can escape this syntax with '$$'; for example: '$$(VARIABLE_NAME)' This field corresponds to the 'command' field of the Kubernetes Containers [v1 core API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).`,
							Elem: &schema.Schema{
								Type: schema.TypeString,
							},
						},
						"env": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `Immutable. List of environment variables to set in the container. After the container starts running, code running in the container can read these environment variables. Additionally, the command and args fields can reference these variables. Later entries in this list can also reference earlier entries. For example, the following example sets the variable 'VAR_2' to have the value 'foo bar': '''json [ { "name": "VAR_1", "value": "foo" }, { "name": "VAR_2", "value": "$(VAR_1) bar" } ] ''' If you switch the order of the variables in the example, then the expansion does not occur. This field corresponds to the 'env' field of the Kubernetes Containers [v1 core API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).`,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"name": {
										Type:        schema.TypeString,
										Required:    true,
										ForceNew:    true,
										Description: `Required. Name of the environment variable. Must be a valid C identifier.`,
									},
									"value": {
										Type:        schema.TypeString,
										Required:    true,
										ForceNew:    true,
										Description: `Required. Variables that reference a $(VAR_NAME) are expanded using the previous defined environment variables in the container and any service environment variables. If a variable cannot be resolved, the reference in the input string will be unchanged. The $(VAR_NAME) syntax can be escaped with a double $$, ie: $$(VAR_NAME). Escaped references will never be expanded, regardless of whether the variable exists or not.`,
									},
								},
							},
						},
						"health_route": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: `Immutable. HTTP path on the container to send health checks to. Vertex AI intermittently sends GET requests to this path on the container's IP address and port to check that the container is healthy. Read more about [health checks](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#health). For example, if you set this field to '/bar', then Vertex AI intermittently sends a GET request to the '/bar' path on the port of your container specified by the first value of this 'ModelContainerSpec''s ports field. If you don't specify this field, it defaults to the following value when you deploy this Model to an Endpoint: '/v1/endpoints/ENDPOINT/deployedModels/DEPLOYED_MODEL:predict' The placeholders in this value are replaced as follows: * ENDPOINT: The last segment (following 'endpoints/')of the Endpoint.name][] field of the Endpoint where this Model has been deployed. (Vertex AI makes this value available to your container code as the ['AIP_ENDPOINT_ID' environment variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).) * DEPLOYED_MODEL: DeployedModel.id of the 'DeployedModel'. (Vertex AI makes this value available to your container code as the ['AIP_DEPLOYED_MODEL_ID' environment variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).)`,
						},
						"ports": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `Immutable. List of ports to expose from the container. Vertex AI sends any prediction requests that it receives to the first port on this list. Vertex AI also sends [liveness and health checks](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#liveness) to this port. If you do not specify this field, it defaults to following value: '''json [ { "containerPort": 8080 } ] ''' Vertex AI does not use ports other than the first one listed. This field corresponds to the 'ports' field of the Kubernetes Containers [v1 core API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#container-v1-core).`,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"container_port": {
										Type:        schema.TypeInt,
										Optional:    true,
										ForceNew:    true,
										Description: `The number of the port to expose on the pod's IP address. Must be a valid port number, between 1 and 65535 inclusive.`,
									},
								},
							},
						},
						"predict_route": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: `Immutable. HTTP path on the container to send prediction requests to. Vertex AI forwards requests sent using projects.locations.endpoints.predict to this path on the container's IP address and port. Vertex AI then returns the container's response in the API response. For example, if you set this field to '/foo', then when Vertex AI receives a prediction request, it forwards the request body in a POST request to the '/foo' path on the port of your container specified by the first value of this 'ModelContainerSpec''s ports field. If you don't specify this field, it defaults to the following value when you deploy this Model to an Endpoint: '/v1/endpoints/ENDPOINT/deployedModels/DEPLOYED_MODEL:predict' The placeholders in this value are replaced as follows: * ENDPOINT: The last segment (following 'endpoints/')of the Endpoint.name][] field of the Endpoint where this Model has been deployed. (Vertex AI makes this value available to your container code as the ['AIP_ENDPOINT_ID' environment variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).) * DEPLOYED_MODEL: DeployedModel.id of the 'DeployedModel'. (Vertex AI makes this value available to your container code as the ['AIP_DEPLOYED_MODEL_ID' environment variable](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#aip-variables).)`,
						},
					},
				},
			},
			"display_name": {
				Type:        schema.TypeString,
				Required:    true,
				Description: `Required. The display name of the Model. The name can be up to 128 characters long and can be consist of any UTF-8 characters.`,
			},
			"location": {
				Type:        schema.TypeString,
				Required:    true,
				ForceNew:    true,
				Description: `The location for the resource`,
			},
			"artifact_uri": {
				Type:        schema.TypeString,
				Optional:    true,
				ForceNew:    true,
				Description: `Immutable. The path to the directory containing the Model artifact and any of its supporting files. Not present for AutoML Models.`,
			},
			"description": {
				Type:        schema.TypeString,
				Optional:    true,
				Description: `The description of the Model.`,
			},
			"encryption_spec": {
				Type:        schema.TypeList,
				Optional:    true,
				ForceNew:    true,
				Description: `Customer-managed encryption key spec for a Model. If set, this Model and all sub-resources of this Model will be secured by this key.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"kms_key_name": {
							Type:        schema.TypeString,
							Required:    true,
							ForceNew:    true,
							Description: `Required. The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource. Has the form: 'projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key'. The key needs to be in the same region as where the compute resource is created.`,
						},
					},
				},
			},
			"labels": {
				Type:        schema.TypeMap,
				Optional:    true,
				Description: `The labels with user-defined metadata to organize your Models. Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters, underscores and dashes. International characters are allowed. See https://goo.gl/xmQnxf for more information and examples of labels.`,
				Elem:        &schema.Schema{Type: schema.TypeString},
			},
			"version_aliases": {
				Type:        schema.TypeList,
				Computed:    true,
				Optional:    true,
				ForceNew:    true,
				Description: `User provided version aliases so that a model version can be referenced via alias (i.e. projects/{project}/locations/{location}/models/{model_id}@{version_alias} instead of auto-generated version id (i.e. projects/{project}/locations/{location}/models/{model_id}@{version_id}). The format is a-z{0,126}[a-z0-9] to distinguish from version_id. A default version alias will be created for the first version of the model, and there must be exactly one default version alias for a model.`,
				Elem: &schema.Schema{
					Type: schema.TypeString,
				},
			},
			"version_description": {
				Type:        schema.TypeString,
				Optional:    true,
				ForceNew:    true,
				Description: `The description of this version.`,
			},
			"create_time": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `Output only. Timestamp when this Model was uploaded into Vertex AI.`,
			},
			"deployed_models": {
				Type:        schema.TypeList,
				Computed:    true,
				Description: `Output only. The pointers to DeployedModels created from this Model. Note that Model could have been deployed to Endpoints in different Locations.`,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"deployed_model_id": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: `Immutable. An ID of a DeployedModel in the above Endpoint.`,
						},
						"endpoint": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: `Immutable. A resource name of an Endpoint.`,
						},
					},
				},
			},
			"etag": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `Used to perform consistent read-modify-write updates. If not set, a blind "overwrite" update happens.`,
			},
			"name": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The resource name of the Model.`,
			},
			"original_model_info": {
				Type:        schema.TypeList,
				Computed:    true,
				Description: `Output only. If this Model is a copy of another Model, this contains info about the original.`,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"model": {
							Type:        schema.TypeString,
							Computed:    true,
							Description: `Output only. The resource name of the Model this Model is a copy of, including the revision. Format: 'projects/{project}/locations/{location}/models/{model_id}@{version_id}'`,
						},
					},
				},
			},
			"supported_deployment_resources_types": {
				Type:        schema.TypeList,
				Computed:    true,
				Description: `Output only. When this Model is deployed, its prediction resources are described by the 'prediction_resources' field of the Endpoint.deployed_models object. Because not all Models support all resource configuration types, the configuration types this Model supports are listed here. If no configuration types are listed, the Model cannot be deployed to an Endpoint and does not support online predictions (PredictionService.Predict or PredictionService.Explain). Such a Model can serve predictions by using a BatchPredictionJob, if it has at least one entry each in supported_input_storage_formats and supported_output_storage_formats.`,
				Elem: &schema.Schema{
					Type: schema.TypeString,
				},
			},
			"supported_export_formats": {
				Type:        schema.TypeList,
				Computed:    true,
				Description: `Output only. The formats in which this Model may be exported. If empty, this Model is not available for export.`,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"exportable_contents": {
							Type:        schema.TypeList,
							Computed:    true,
							Description: `Output only. The content of this Model that may be exported.`,
							Elem: &schema.Schema{
								Type: schema.TypeString,
							},
						},
						"id": {
							Type:        schema.TypeString,
							Computed:    true,
							Description: `Output only. The ID of the export format. The possible format IDs are: * 'tflite' Used for Android mobile devices. * 'edgetpu-tflite' Used for [Edge TPU](https://cloud.google.com/edge-tpu/) devices. * 'tf-saved-model' A tensorflow model in SavedModel format. * 'tf-js' A [TensorFlow.js](https://www.tensorflow.org/js) model that can be used in the browser and in Node.js using JavaScript. * 'core-ml' Used for iOS mobile devices. * 'custom-trained' A Model that was uploaded or trained by custom code.`,
						},
					},
				},
			},
			"supported_input_storage_formats": {
				Type:        schema.TypeList,
				Computed:    true,
				Description: `Output only. The formats this Model supports in BatchPredictionJob.input_config. If PredictSchemata.instance_schema_uri exists, the instances should be given as per that schema. The possible formats are: * 'jsonl' The JSON Lines format, where each instance is a single line. Uses GcsSource. * 'csv' The CSV format, where each instance is a single comma-separated line. The first line in the file is the header, containing comma-separated field names. Uses GcsSource. * 'tf-record' The TFRecord format, where each instance is a single record in tfrecord syntax. Uses GcsSource. * 'tf-record-gzip' Similar to 'tf-record', but the file is gzipped. Uses GcsSource. * 'bigquery' Each instance is a single row in BigQuery. Uses BigQuerySource. * 'file-list' Each line of the file is the location of an instance to process, uses 'gcs_source' field of the InputConfig object. If this Model doesn't support any of these formats it means it cannot be used with a BatchPredictionJob. However, if it has supported_deployment_resources_types, it could serve online predictions by using PredictionService.Predict or PredictionService.Explain. TODO(rsurowka): Give a link describing how OpenAPI schema instances are expressed in JSONL and BigQuery. TODO(rsurowka): Should we provide a schema for TFRecord? Or maybe say that at least for now TFRecord input is not supported via schemata (that would also simplify giving them back as part of predictions). TODO(rsurowka): Define CSV format (decide how much we want to support). E.g. no nesting? Or no arrays, or no nested arrays? E.g. https://json-csv.com/ seems to be able to do pretty advanced conversions, but we may decide to make it relatively simple for now.`,
				Elem: &schema.Schema{
					Type: schema.TypeString,
				},
			},
			"supported_output_storage_formats": {
				Type:        schema.TypeList,
				Computed:    true,
				Description: `Output only. The formats this Model supports in BatchPredictionJob.output_config. If both PredictSchemata.instance_schema_uri and PredictSchemata.prediction_schema_uri exist, the predictions are returned together with their instances. In other words, the prediction has the original instance data first, followed by the actual prediction content (as per the schema). The possible formats are: * 'jsonl' The JSON Lines format, where each prediction is a single line. Uses GcsDestination. * 'csv' The CSV format, where each prediction is a single comma-separated line. The first line in the file is the header, containing comma-separated field names. Uses GcsDestination. * 'bigquery' Each prediction is a single row in a BigQuery table, uses BigQueryDestination . If this Model doesn't support any of these formats it means it cannot be used with a BatchPredictionJob. However, if it has supported_deployment_resources_types, it could serve online predictions by using PredictionService.Predict or PredictionService.Explain. TODO(rsurowka): Analogous TODOs as for instances field above.`,
				Elem: &schema.Schema{
					Type: schema.TypeString,
				},
			},
			"training_pipeline": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `Output only. The resource name of the TrainingPipeline that uploaded this Model, if any.`,
			},
			"update_time": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `Output only. Timestamp when this Model was most recently updated.`,
			},
			"version_create_time": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `Output only. Timestamp when this version was created.`,
			},
			"version_id": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `Output only. Immutable. The version ID of the model. A new version is committed when a new model version is uploaded or trained under an existing model id. It is an auto-incrementing decimal number in string representation.`,
			},
			"version_update_time": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `Output only. Timestamp when this version was most recently updated.`,
			},
			"project": {
				Type:     schema.TypeString,
				Optional: true,
				Computed: true,
				ForceNew: true,
			},
		},
		UseJSONNumber: true,
	}
}

func resourceVertexAIModelCreate(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*Config)
	userAgent, err := generateUserAgentString(d, config.userAgent)
	if err != nil {
		return err
	}

	obj := make(map[string]interface{})
	versionAliasesProp, err := expandVertexAIModelVersionAliases(d.Get("version_aliases"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("version_aliases"); !isEmptyValue(reflect.ValueOf(versionAliasesProp)) && (ok || !reflect.DeepEqual(v, versionAliasesProp)) {
		obj["versionAliases"] = versionAliasesProp
	}
	displayNameProp, err := expandVertexAIModelDisplayName(d.Get("display_name"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("display_name"); !isEmptyValue(reflect.ValueOf(displayNameProp)) && (ok || !reflect.DeepEqual(v, displayNameProp)) {
		obj["displayName"] = displayNameProp
	}
	descriptionProp, err := expandVertexAIModelDescription(d.Get("description"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("description"); !isEmptyValue(reflect.ValueOf(descriptionProp)) && (ok || !reflect.DeepEqual(v, descriptionProp)) {
		obj["description"] = descriptionProp
	}
	versionDescriptionProp, err := expandVertexAIModelVersionDescription(d.Get("version_description"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("version_description"); !isEmptyValue(reflect.ValueOf(versionDescriptionProp)) && (ok || !reflect.DeepEqual(v, versionDescriptionProp)) {
		obj["versionDescription"] = versionDescriptionProp
	}
	containerSpecProp, err := expandVertexAIModelContainerSpec(d.Get("container_spec"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("container_spec"); !isEmptyValue(reflect.ValueOf(containerSpecProp)) && (ok || !reflect.DeepEqual(v, containerSpecProp)) {
		obj["containerSpec"] = containerSpecProp
	}
	artifactUriProp, err := expandVertexAIModelArtifactUri(d.Get("artifact_uri"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("artifact_uri"); !isEmptyValue(reflect.ValueOf(artifactUriProp)) && (ok || !reflect.DeepEqual(v, artifactUriProp)) {
		obj["artifactUri"] = artifactUriProp
	}
	labelsProp, err := expandVertexAIModelLabels(d.Get("labels"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("labels"); !isEmptyValue(reflect.ValueOf(labelsProp)) && (ok || !reflect.DeepEqual(v, labelsProp)) {
		obj["labels"] = labelsProp
	}
	encryptionSpecProp, err := expandVertexAIModelEncryptionSpec(d.Get("encryption_spec"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("encryption_spec"); !isEmptyValue(reflect.ValueOf(encryptionSpecProp)) && (ok || !reflect.DeepEqual(v, encryptionSpecProp)) {
		obj["encryptionSpec"] = encryptionSpecProp
	}

	obj, err = resourceVertexAIModelEncoder(d, meta, obj)
	if err != nil {
		return err
	}

	url, err := replaceVars(d, config, "{{VertexAIBasePath}}projects/{{project}}/locations/{{location}}/models:upload")
	if err != nil {
		return err
	}

	log.Printf("[DEBUG] Creating new Model: %#v", obj)
	billingProject := ""

	project, err := getProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for Model: %s", err)
	}
	billingProject = project

	// err == nil indicates that the billing_project value was found
	if bp, err := getBillingProject(d, config); err == nil {
		billingProject = bp
	}

	res, err := sendRequestWithTimeout(config, "POST", billingProject, url, userAgent, obj, d.Timeout(schema.TimeoutCreate))
	if err != nil {
		return fmt.Errorf("Error creating Model: %s", err)
	}

	// Store the ID now
	id, err := replaceVars(d, config, "projects/{{project}}/locations/{{location}}/models/{{name}}")
	if err != nil {
		return fmt.Errorf("Error constructing id: %s", err)
	}
	d.SetId(id)

	// Use the resource in the operation response to populate
	// identity fields and d.Id() before read
	var opRes map[string]interface{}
	err = vertexAIOperationWaitTimeWithResponse(
		config, res, &opRes, project, "Creating Model", userAgent,
		d.Timeout(schema.TimeoutCreate))
	if err != nil {
		// The resource didn't actually create
		d.SetId("")

		return fmt.Errorf("Error waiting to create Model: %s", err)
	}

	opRes, err = resourceVertexAIModelDecoder(d, meta, opRes)
	if err != nil {
		return fmt.Errorf("Error decoding response from operation: %s", err)
	}
	if opRes == nil {
		return fmt.Errorf("Error decoding response from operation, could not find object")
	}

	if err := d.Set("name", flattenVertexAIModelName(opRes["name"], d, config)); err != nil {
		return err
	}

	// This may have caused the ID to update - update it if so.
	id, err = replaceVars(d, config, "projects/{{project}}/locations/{{location}}/models/{{name}}")
	if err != nil {
		return fmt.Errorf("Error constructing id: %s", err)
	}
	d.SetId(id)

	log.Printf("[DEBUG] Finished creating Model %q: %#v", d.Id(), res)

	return resourceVertexAIModelRead(d, meta)
}

func resourceVertexAIModelRead(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*Config)
	userAgent, err := generateUserAgentString(d, config.userAgent)
	if err != nil {
		return err
	}

	url, err := replaceVars(d, config, "{{VertexAIBasePath}}projects/{{project}}/locations/{{location}}/models/{{name}}")
	if err != nil {
		return err
	}

	billingProject := ""

	project, err := getProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for Model: %s", err)
	}
	billingProject = project

	// err == nil indicates that the billing_project value was found
	if bp, err := getBillingProject(d, config); err == nil {
		billingProject = bp
	}

	res, err := sendRequest(config, "GET", billingProject, url, userAgent, nil)
	if err != nil {
		return handleNotFoundError(err, d, fmt.Sprintf("VertexAIModel %q", d.Id()))
	}

	res, err = resourceVertexAIModelDecoder(d, meta, res)
	if err != nil {
		return err
	}

	if res == nil {
		// Decoding the object has resulted in it being gone. It may be marked deleted
		log.Printf("[DEBUG] Removing VertexAIModel because it no longer exists.")
		d.SetId("")
		return nil
	}

	if err := d.Set("project", project); err != nil {
		return fmt.Errorf("Error reading Model: %s", err)
	}

	if err := d.Set("name", flattenVertexAIModelName(res["name"], d, config)); err != nil {
		return fmt.Errorf("Error reading Model: %s", err)
	}
	if err := d.Set("version_id", flattenVertexAIModelVersionId(res["versionId"], d, config)); err != nil {
		return fmt.Errorf("Error reading Model: %s", err)
	}
	if err := d.Set("version_aliases", flattenVertexAIModelVersionAliases(res["versionAliases"], d, config)); err != nil {
		return fmt.Errorf("Error reading Model: %s", err)
	}
	if err := d.Set("version_create_time", flattenVertexAIModelVersionCreateTime(res["versionCreateTime"], d, config)); err != nil {
		return fmt.Errorf("Error reading Model: %s", err)
	}
	if err := d.Set("version_update_time", flattenVertexAIModelVersionUpdateTime(res["versionUpdateTime"], d, config)); err != nil {
		return fmt.Errorf("Error reading Model: %s", err)
	}
	if err := d.Set("display_name", flattenVertexAIModelDisplayName(res["displayName"], d, config)); err != nil {
		return fmt.Errorf("Error reading Model: %s", err)
	}
	if err := d.Set("description", flattenVertexAIModelDescription(res["description"], d, config)); err != nil {
		return fmt.Errorf("Error reading Model: %s", err)
	}
	if err := d.Set("version_description", flattenVertexAIModelVersionDescription(res["versionDescription"], d, config)); err != nil {
		return fmt.Errorf("Error reading Model: %s", err)
	}
	if err := d.Set("supported_export_formats", flattenVertexAIModelSupportedExportFormats(res["supportedExportFormats"], d, config)); err != nil {
		return fmt.Errorf("Error reading Model: %s", err)
	}
	if err := d.Set("training_pipeline", flattenVertexAIModelTrainingPipeline(res["trainingPipeline"], d, config)); err != nil {
		return fmt.Errorf("Error reading Model: %s", err)
	}
	if err := d.Set("original_model_info", flattenVertexAIModelOriginalModelInfo(res["originalModelInfo"], d, config)); err != nil {
		return fmt.Errorf("Error reading Model: %s", err)
	}
	if err := d.Set("container_spec", flattenVertexAIModelContainerSpec(res["containerSpec"], d, config)); err != nil {
		return fmt.Errorf("Error reading Model: %s", err)
	}
	if err := d.Set("artifact_uri", flattenVertexAIModelArtifactUri(res["artifactUri"], d, config)); err != nil {
		return fmt.Errorf("Error reading Model: %s", err)
	}
	if err := d.Set("supported_deployment_resources_types", flattenVertexAIModelSupportedDeploymentResourcesTypes(res["supportedDeploymentResourcesTypes"], d, config)); err != nil {
		return fmt.Errorf("Error reading Model: %s", err)
	}
	if err := d.Set("supported_input_storage_formats", flattenVertexAIModelSupportedInputStorageFormats(res["supportedInputStorageFormats"], d, config)); err != nil {
		return fmt.Errorf("Error reading Model: %s", err)
	}
	if err := d.Set("supported_output_storage_formats", flattenVertexAIModelSupportedOutputStorageFormats(res["supportedOutputStorageFormats"], d, config)); err != nil {
		return fmt.Errorf("Error reading Model: %s", err)
	}
	if err := d.Set("create_time", flattenVertexAIModelCreateTime(res["createTime"], d, config)); err != nil {
		return fmt.Errorf("Error reading Model: %s", err)
	}
	if err := d.Set("update_time", flattenVertexAIModelUpdateTime(res["updateTime"], d, config)); err != nil {
		return fmt.Errorf("Error reading Model: %s", err)
	}
	if err := d.Set("deployed_models", flattenVertexAIModelDeployedModels(res["deployedModels"], d, config)); err != nil {
		return fmt.Errorf("Error reading Model: %s", err)
	}
	if err := d.Set("labels", flattenVertexAIModelLabels(res["labels"], d, config)); err != nil {
		return fmt.Errorf("Error reading Model: %s", err)
	}
	if err := d.Set("encryption_spec", flattenVertexAIModelEncryptionSpec(res["encryptionSpec"], d, config)); err != nil {
		return fmt.Errorf("Error reading Model: %s", err)
	}

	return nil
}

func resourceVertexAIModelUpdate(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*Config)
	userAgent, err := generateUserAgentString(d, config.userAgent)
	if err != nil {
		return err
	}

	billingProject := ""

	project, err := getProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for Model: %s", err)
	}
	billingProject = project

	obj := make(map[string]interface{})
	displayNameProp, err := expandVertexAIModelDisplayName(d.Get("display_name"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("display_name"); !isEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, displayNameProp)) {
		obj["displayName"] = displayNameProp
	}
	descriptionProp, err := expandVertexAIModelDescription(d.Get("description"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("description"); !isEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, descriptionProp)) {
		obj["description"] = descriptionProp
	}
	labelsProp, err := expandVertexAIModelLabels(d.Get("labels"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("labels"); !isEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, labelsProp)) {
		obj["labels"] = labelsProp
	}

	obj, err = resourceVertexAIModelEncoder(d, meta, obj)
	if err != nil {
		return err
	}

	url, err := replaceVars(d, config, "{{VertexAIBasePath}}projects/{{project}}/locations/{{location}}/models/{{name}}")
	if err != nil {
		return err
	}

	log.Printf("[DEBUG] Updating Model %q: %#v", d.Id(), obj)
	updateMask := []string{}

	if d.HasChange("display_name") {
		updateMask = append(updateMask, "displayName")
	}

	if d.HasChange("description") {
		updateMask = append(updateMask, "description")
	}

	if d.HasChange("labels") {
		updateMask = append(updateMask, "labels")
	}
	// updateMask is a URL parameter but not present in the schema, so replaceVars
	// won't set it
	url, err = addQueryParams(url, map[string]string{"updateMask": strings.Join(updateMask, ",")})
	if err != nil {
		return err
	}

	// err == nil indicates that the billing_project value was found
	if bp, err := getBillingProject(d, config); err == nil {
		billingProject = bp
	}

	res, err := sendRequestWithTimeout(config, "PATCH", billingProject, url, userAgent, obj, d.Timeout(schema.TimeoutUpdate))

	if err != nil {
		return fmt.Errorf("Error updating Model %q: %s", d.Id(), err)
	} else {
		log.Printf("[DEBUG] Finished updating Model %q: %#v", d.Id(), res)
	}

	return resourceVertexAIModelRead(d, meta)
}

func resourceVertexAIModelDelete(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*Config)
	userAgent, err := generateUserAgentString(d, config.userAgent)
	if err != nil {
		return err
	}

	billingProject := ""

	project, err := getProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for Model: %s", err)
	}
	billingProject = project

	url, err := replaceVars(d, config, "{{VertexAIBasePath}}projects/{{project}}/locations/{{location}}/models/{{name}}")
	if err != nil {
		return err
	}

	var obj map[string]interface{}
	log.Printf("[DEBUG] Deleting Model %q", d.Id())

	// err == nil indicates that the billing_project value was found
	if bp, err := getBillingProject(d, config); err == nil {
		billingProject = bp
	}

	res, err := sendRequestWithTimeout(config, "DELETE", billingProject, url, userAgent, obj, d.Timeout(schema.TimeoutDelete))
	if err != nil {
		return handleNotFoundError(err, d, "Model")
	}

	err = vertexAIOperationWaitTime(
		config, res, project, "Deleting Model", userAgent,
		d.Timeout(schema.TimeoutDelete))

	if err != nil {
		return err
	}

	log.Printf("[DEBUG] Finished deleting Model %q: %#v", d.Id(), res)
	return nil
}

func resourceVertexAIModelImport(d *schema.ResourceData, meta interface{}) ([]*schema.ResourceData, error) {
	config := meta.(*Config)
	if err := parseImportId([]string{
		"projects/(?P<project>[^/]+)/locations/(?P<location>[^/]+)/models/(?P<name>[^/]+)",
		"(?P<project>[^/]+)/(?P<location>[^/]+)/(?P<name>[^/]+)",
		"(?P<location>[^/]+)/(?P<name>[^/]+)",
	}, d, config); err != nil {
		return nil, err
	}

	// Replace import id for the resource id
	id, err := replaceVars(d, config, "projects/{{project}}/locations/{{location}}/models/{{name}}")
	if err != nil {
		return nil, fmt.Errorf("Error constructing id: %s", err)
	}
	d.SetId(id)

	return []*schema.ResourceData{d}, nil
}

func flattenVertexAIModelName(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	if v == nil {
		return v
	}
	return NameFromSelfLinkStateFunc(v)
}

func flattenVertexAIModelVersionId(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIModelVersionAliases(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIModelVersionCreateTime(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIModelVersionUpdateTime(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIModelDisplayName(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIModelDescription(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIModelVersionDescription(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIModelSupportedExportFormats(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"id":                  flattenVertexAIModelSupportedExportFormatsId(original["id"], d, config),
			"exportable_contents": flattenVertexAIModelSupportedExportFormatsExportableContents(original["exportableContents"], d, config),
		})
	}
	return transformed
}
func flattenVertexAIModelSupportedExportFormatsId(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIModelSupportedExportFormatsExportableContents(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIModelTrainingPipeline(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIModelOriginalModelInfo(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["model"] =
		flattenVertexAIModelOriginalModelInfoModel(original["model"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelOriginalModelInfoModel(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIModelContainerSpec(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["image_uri"] =
		flattenVertexAIModelContainerSpecImageUri(original["imageUri"], d, config)
	transformed["command"] =
		flattenVertexAIModelContainerSpecCommand(original["command"], d, config)
	transformed["args"] =
		flattenVertexAIModelContainerSpecArgs(original["args"], d, config)
	transformed["env"] =
		flattenVertexAIModelContainerSpecEnv(original["env"], d, config)
	transformed["ports"] =
		flattenVertexAIModelContainerSpecPorts(original["ports"], d, config)
	transformed["predict_route"] =
		flattenVertexAIModelContainerSpecPredictRoute(original["predictRoute"], d, config)
	transformed["health_route"] =
		flattenVertexAIModelContainerSpecHealthRoute(original["healthRoute"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelContainerSpecImageUri(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIModelContainerSpecCommand(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIModelContainerSpecArgs(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIModelContainerSpecEnv(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"name":  flattenVertexAIModelContainerSpecEnvName(original["name"], d, config),
			"value": flattenVertexAIModelContainerSpecEnvValue(original["value"], d, config),
		})
	}
	return transformed
}
func flattenVertexAIModelContainerSpecEnvName(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIModelContainerSpecEnvValue(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIModelContainerSpecPorts(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"container_port": flattenVertexAIModelContainerSpecPortsContainerPort(original["containerPort"], d, config),
		})
	}
	return transformed
}
func flattenVertexAIModelContainerSpecPortsContainerPort(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := stringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIModelContainerSpecPredictRoute(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIModelContainerSpecHealthRoute(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIModelArtifactUri(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIModelSupportedDeploymentResourcesTypes(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIModelSupportedInputStorageFormats(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIModelSupportedOutputStorageFormats(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIModelCreateTime(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIModelUpdateTime(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIModelDeployedModels(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"endpoint":          flattenVertexAIModelDeployedModelsEndpoint(original["endpoint"], d, config),
			"deployed_model_id": flattenVertexAIModelDeployedModelsDeployedModelId(original["id"], d, config),
		})
	}
	return transformed
}
func flattenVertexAIModelDeployedModelsEndpoint(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIModelDeployedModelsDeployedModelId(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIModelLabels(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIModelEncryptionSpec(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["kms_key_name"] =
		flattenVertexAIModelEncryptionSpecKmsKeyName(original["kmsKeyName"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelEncryptionSpecKmsKeyName(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func expandVertexAIModelVersionAliases(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelDisplayName(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelDescription(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelVersionDescription(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelContainerSpec(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedImageUri, err := expandVertexAIModelContainerSpecImageUri(original["image_uri"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedImageUri); val.IsValid() && !isEmptyValue(val) {
		transformed["imageUri"] = transformedImageUri
	}

	transformedCommand, err := expandVertexAIModelContainerSpecCommand(original["command"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedCommand); val.IsValid() && !isEmptyValue(val) {
		transformed["command"] = transformedCommand
	}

	transformedArgs, err := expandVertexAIModelContainerSpecArgs(original["args"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedArgs); val.IsValid() && !isEmptyValue(val) {
		transformed["args"] = transformedArgs
	}

	transformedEnv, err := expandVertexAIModelContainerSpecEnv(original["env"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedEnv); val.IsValid() && !isEmptyValue(val) {
		transformed["env"] = transformedEnv
	}

	transformedPorts, err := expandVertexAIModelContainerSpecPorts(original["ports"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPorts); val.IsValid() && !isEmptyValue(val) {
		transformed["ports"] = transformedPorts
	}

	transformedPredictRoute, err := expandVertexAIModelContainerSpecPredictRoute(original["predict_route"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPredictRoute); val.IsValid() && !isEmptyValue(val) {
		transformed["predictRoute"] = transformedPredictRoute
	}

	transformedHealthRoute, err := expandVertexAIModelContainerSpecHealthRoute(original["health_route"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedHealthRoute); val.IsValid() && !isEmptyValue(val) {
		transformed["healthRoute"] = transformedHealthRoute
	}

	return transformed, nil
}

func expandVertexAIModelContainerSpecImageUri(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelContainerSpecCommand(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelContainerSpecArgs(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelContainerSpecEnv(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	l := v.([]interface{})
	req := make([]interface{}, 0, len(l))
	for _, raw := range l {
		if raw == nil {
			continue
		}
		original := raw.(map[string]interface{})
		transformed := make(map[string]interface{})

		transformedName, err := expandVertexAIModelContainerSpecEnvName(original["name"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedName); val.IsValid() && !isEmptyValue(val) {
			transformed["name"] = transformedName
		}

		transformedValue, err := expandVertexAIModelContainerSpecEnvValue(original["value"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedValue); val.IsValid() && !isEmptyValue(val) {
			transformed["value"] = transformedValue
		}

		req = append(req, transformed)
	}
	return req, nil
}

func expandVertexAIModelContainerSpecEnvName(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelContainerSpecEnvValue(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelContainerSpecPorts(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	l := v.([]interface{})
	req := make([]interface{}, 0, len(l))
	for _, raw := range l {
		if raw == nil {
			continue
		}
		original := raw.(map[string]interface{})
		transformed := make(map[string]interface{})

		transformedContainerPort, err := expandVertexAIModelContainerSpecPortsContainerPort(original["container_port"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedContainerPort); val.IsValid() && !isEmptyValue(val) {
			transformed["containerPort"] = transformedContainerPort
		}

		req = append(req, transformed)
	}
	return req, nil
}

func expandVertexAIModelContainerSpecPortsContainerPort(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelContainerSpecPredictRoute(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelContainerSpecHealthRoute(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelArtifactUri(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelLabels(v interface{}, d TerraformResourceData, config *Config) (map[string]string, error) {
	if v == nil {
		return map[string]string{}, nil
	}
	m := make(map[string]string)
	for k, val := range v.(map[string]interface{}) {
		m[k] = val.(string)
	}
	return m, nil
}

func expandVertexAIModelEncryptionSpec(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedKmsKeyName, err := expandVertexAIModelEncryptionSpecKmsKeyName(original["kms_key_name"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedKmsKeyName); val.IsValid() && !isEmptyValue(val) {
		transformed["kmsKeyName"] = transformedKmsKeyName
	}

	return transformed, nil
}

func expandVertexAIModelEncryptionSpecKmsKeyName(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func resourceVertexAIModelEncoder(d *schema.ResourceData, meta interface{}, obj map[string]interface{}) (map[string]interface{}, error) {
	newObj := make(map[string]interface{})
	newObj["model"] = obj
	return newObj, nil
}

func resourceVertexAIModelDecoder(d *schema.ResourceData, meta interface{}, res map[string]interface{}) (map[string]interface{}, error) {
	// Model's operation response puts the resource name under "model" instead of "name".
	if model, ok := res["model"].(string); ok {
		delete(res, "model")
		res["name"] = model
	}

	return res, nil
}
