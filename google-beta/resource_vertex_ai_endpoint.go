// ----------------------------------------------------------------------------
//
//     ***     AUTO GENERATED CODE    ***    Type: MMv1     ***
//
// ----------------------------------------------------------------------------
//
//     This file is automatically generated by Magic Modules and manual
//     changes will be clobbered when the file is regenerated.
//
//     Please read more about how to change this file in
//     .github/CONTRIBUTING.md.
//
// ----------------------------------------------------------------------------

package google

import (
	"fmt"
	"log"
	"reflect"
	"strings"
	"time"

	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/schema"
)

func resourceVertexAIEndpoint() *schema.Resource {
	return &schema.Resource{
		Create: resourceVertexAIEndpointCreate,
		Read:   resourceVertexAIEndpointRead,
		Update: resourceVertexAIEndpointUpdate,
		Delete: resourceVertexAIEndpointDelete,

		Importer: &schema.ResourceImporter{
			State: resourceVertexAIEndpointImport,
		},

		Timeouts: &schema.ResourceTimeout{
			Create: schema.DefaultTimeout(6 * time.Minute),
			Update: schema.DefaultTimeout(6 * time.Minute),
			Delete: schema.DefaultTimeout(10 * time.Minute),
		},

		Schema: map[string]*schema.Schema{
			"display_name": {
				Type:        schema.TypeString,
				Required:    true,
				Description: `The user-defined name of the Endpoint. The name can be up to 128 characters long and can be consist of any UTF-8 characters.`,
			},
			"deployed_models": {
				Type:        schema.TypeList,
				Optional:    true,
				ForceNew:    true,
				Description: `The list of models deployed in this Endpoint`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"model": {
							Type:     schema.TypeString,
							Required: true,
							Description: `The name of the Model that this is the deployment of.  
Note that the Model may be in a different location than the DeployedModel's Endpoint.`,
						},
						"automatic_resources": {
							Type:     schema.TypeList,
							Optional: true,
							Description: `A description of resources that to large degree are decided by Vertex AI, and require only a modest additional configuration.  
Cannot be used with 'dedicatedResources'`,
							MaxItems: 1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"max_replica_count": {
										Type:        schema.TypeInt,
										Optional:    true,
										Description: `Immutable. The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, a no upper bound for scaling under heavy traffic will be assume, though Vertex AI may be unable to scale beyond certain replica number.`,
									},
									"min_replica_count": {
										Type:        schema.TypeInt,
										Optional:    true,
										Description: `Immutable. The minimum number of replicas this DeployedModel will be always deployed on. If traffic against it increases, it may dynamically be deployed onto more replicas up to 'maxReplicaCount'', and as traffic decreases, some of these extra replicas may be freed. If the requested value is too large, the deployment will error.`,
									},
								},
							},
						},
						"dedicated_resources": {
							Type:     schema.TypeList,
							Optional: true,
							Description: `A description of resources that are dedicated to the DeployedModel, and that need a higher degree of manual configuration.  
Cannot be used with 'automaticResources'`,
							MaxItems: 1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"machine_spec": {
										Type:        schema.TypeList,
										Required:    true,
										Description: `Immutable. The specification of a single machine used by the prediction.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"accelerator_count": {
													Type:        schema.TypeInt,
													Optional:    true,
													Description: `The number of accelerators to attach to the machine.`,
												},
												"accelerator_type": {
													Type:         schema.TypeString,
													Optional:     true,
													ValidateFunc: validateEnum([]string{"ACCELERATOR_TYPE_UNSPECIFIED", "NVIDIA_TESLA_K80", "NVIDIA_TESLA_P100", "NVIDIA_TESLA_V100", "NVIDIA_TESLA_P4", "NVIDIA_TESLA_T4", "NVIDIA_TESLA_A100", ""}),
													Description:  `Immutable. The type of accelerator(s) that may be attached to the machine as per acceleratorCount. Possible values: ["ACCELERATOR_TYPE_UNSPECIFIED", "NVIDIA_TESLA_K80", "NVIDIA_TESLA_P100", "NVIDIA_TESLA_V100", "NVIDIA_TESLA_P4", "NVIDIA_TESLA_T4", "NVIDIA_TESLA_A100"]`,
												},
												"machine_type": {
													Type:     schema.TypeString,
													Optional: true,
													Description: `Immutable. The type of the machine.  
See the [list of machine types supported for prediction](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types)  
See the [list of machine types supported for custom training](https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types).  
For 'DeployedModel' this field is optional, and the default value is 'n1-standard-2'. For 'BatchPredictionJob' or as part of 'WorkerPoolSpec' this field is required.`,
												},
											},
										},
									},
									"min_replica_count": {
										Type:        schema.TypeInt,
										Required:    true,
										Description: `Immutable. The minimum number of machine replicas this DeployedModel will be always deployed on. This value must be greater than or equal to 1. If traffic against the DeployedModel increases, it may dynamically be deployed onto more replicas, and as traffic decreases, some of these extra replicas may be freed.`,
									},
									"max_replica_count": {
										Type:        schema.TypeInt,
										Optional:    true,
										ForceNew:    true,
										Description: `Immutable. The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, will use 'minReplicaCount' as the default value.`,
									},
								},
							},
						},
						"enable_access_logging": {
							Type:     schema.TypeString,
							Optional: true,
							Description: `These logs are like standard server access logs, containing information like timestamp and latency for each prediction request.  
Note that Stackdriver logs may incur a cost, especially if your project receives prediction requests at a high queries per second rate (QPS). Estimate your costs before enabling this option.`,
						},
						"enable_container_logging": {
							Type:     schema.TypeBool,
							Optional: true,
							Description: `If true, the container of the DeployedModel instances will send stderr and stdout streams to Stackdriver Logging.  
Only supported for custom-trained Models and AutoML Tabular Models.`,
						},
						"service_account": {
							Type:     schema.TypeString,
							Optional: true,
							Description: `The service account that the DeployedModel's container runs as. Specify the email address of the service account. If this service account is not specified, the container runs as a service account that doesn't have access to the resource project.  
<br>
Users deploying the Model must have the iam.serviceAccounts.actAs permission on this service account.`,
						},
						"create_time": {
							Type:        schema.TypeString,
							Computed:    true,
							Description: `The timestamp of when the endpoint was created in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits.`,
						},
						"id": {
							Type:        schema.TypeString,
							Computed:    true,
							Description: `The resource name of the deployed model.`,
						},
						"private_endpoints": {
							Type:        schema.TypeList,
							Computed:    true,
							Description: `Provide paths for users to send predict/explain/health requests directly to the deployed model services running on Cloud via private services access. This field is populated if network is configured.`,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"explain_http_uri": {
										Type:        schema.TypeString,
										Computed:    true,
										Description: `Http(s) path to send explain requests.`,
									},
									"health_http_uri": {
										Type:        schema.TypeString,
										Computed:    true,
										Description: `Http(s) path to send health requests.`,
									},
									"predict_http_uri": {
										Type:        schema.TypeString,
										Computed:    true,
										Description: `Http(s) path to send prediction requests.`,
									},
								},
							},
						},
					},
				},
			},
			"description": {
				Type:        schema.TypeString,
				Optional:    true,
				Description: `The description of the Endpoint.`,
			},
			"encryption_spec": {
				Type:        schema.TypeList,
				Optional:    true,
				ForceNew:    true,
				Description: `Customer-managed encryption key spec for the Endpoint. If set, this Endpoint and any sub-resources of this Endpoint will be secured by this key.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"kms_key_name": {
							Type:     schema.TypeString,
							Optional: true,
							ForceNew: true,
							Description: `Required. The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource.  
Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the resource is created.`,
						},
					},
				},
			},
			"labels": {
				Type:        schema.TypeMap,
				Computed:    true,
				Optional:    true,
				Description: `A set of key/value label pairs to assign to this Endpoint.`,
				Elem:        &schema.Schema{Type: schema.TypeString},
			},
			"model_deployment_monitoring_job": {
				Type:        schema.TypeString,
				Optional:    true,
				ForceNew:    true,
				Description: `Resource name of the Model Monitoring job associated with this Endpoint if monitoring is enabled.`,
			},
			"network": {
				Type:     schema.TypeString,
				Optional: true,
				Description: `The full name of the Google Compute Engine network to which the Endpoint should be peered.  
Private services access must already be configured for the network. If left unspecified, the Endpoint is not peered with any network.  
Format: projects/{project}/global/networks/{network}. Where {project} is a project number, as in '12345', and {network} is network name.`,
			},
			"region": {
				Type:        schema.TypeString,
				Computed:    true,
				Optional:    true,
				ForceNew:    true,
				Description: `The region of the endpoint. eg us-central1`,
			},
			"traffic_split": {
				Type:     schema.TypeMap,
				Optional: true,
				Description: `A map from a DeployedModel's ID to the percentage of this Endpoint's traffic that should be forwarded to that DeployedModel.  
If a DeployedModel's ID is not listed in this map, then it receives no traffic.      
The traffic percentage values must add up to 100, or map must be empty if the Endpoint is to not accept any traffic at a moment.`,
				Elem: &schema.Schema{Type: schema.TypeString},
			},
			"etag": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `Used to perform consistent read-modify-write updates.`,
			},
			"name": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The resource name of the Endpoint.`,
			},
			"project": {
				Type:     schema.TypeString,
				Optional: true,
				Computed: true,
				ForceNew: true,
			},
		},
		UseJSONNumber: true,
	}
}

func resourceVertexAIEndpointCreate(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*Config)
	userAgent, err := generateUserAgentString(d, config.userAgent)
	if err != nil {
		return err
	}

	obj := make(map[string]interface{})
	displayNameProp, err := expandVertexAIEndpointDisplayName(d.Get("display_name"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("display_name"); !isEmptyValue(reflect.ValueOf(displayNameProp)) && (ok || !reflect.DeepEqual(v, displayNameProp)) {
		obj["displayName"] = displayNameProp
	}
	descriptionProp, err := expandVertexAIEndpointDescription(d.Get("description"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("description"); !isEmptyValue(reflect.ValueOf(descriptionProp)) && (ok || !reflect.DeepEqual(v, descriptionProp)) {
		obj["description"] = descriptionProp
	}
	deployedModelsProp, err := expandVertexAIEndpointDeployedModels(d.Get("deployed_models"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("deployed_models"); !isEmptyValue(reflect.ValueOf(deployedModelsProp)) && (ok || !reflect.DeepEqual(v, deployedModelsProp)) {
		obj["deployedModels"] = deployedModelsProp
	}
	trafficSplitProp, err := expandVertexAIEndpointTrafficSplit(d.Get("traffic_split"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("traffic_split"); !isEmptyValue(reflect.ValueOf(trafficSplitProp)) && (ok || !reflect.DeepEqual(v, trafficSplitProp)) {
		obj["trafficSplit"] = trafficSplitProp
	}
	labelsProp, err := expandVertexAIEndpointLabels(d.Get("labels"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("labels"); !isEmptyValue(reflect.ValueOf(labelsProp)) && (ok || !reflect.DeepEqual(v, labelsProp)) {
		obj["labels"] = labelsProp
	}
	encryptionSpecProp, err := expandVertexAIEndpointEncryptionSpec(d.Get("encryption_spec"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("encryption_spec"); !isEmptyValue(reflect.ValueOf(encryptionSpecProp)) && (ok || !reflect.DeepEqual(v, encryptionSpecProp)) {
		obj["encryptionSpec"] = encryptionSpecProp
	}
	networkProp, err := expandVertexAIEndpointNetwork(d.Get("network"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("network"); !isEmptyValue(reflect.ValueOf(networkProp)) && (ok || !reflect.DeepEqual(v, networkProp)) {
		obj["network"] = networkProp
	}
	modelDeploymentMonitoringJobProp, err := expandVertexAIEndpointModelDeploymentMonitoringJob(d.Get("model_deployment_monitoring_job"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("model_deployment_monitoring_job"); !isEmptyValue(reflect.ValueOf(modelDeploymentMonitoringJobProp)) && (ok || !reflect.DeepEqual(v, modelDeploymentMonitoringJobProp)) {
		obj["modelDeploymentMonitoringJob"] = modelDeploymentMonitoringJobProp
	}

	url, err := replaceVars(d, config, "{{VertexAIBasePath}}projects/{{project}}/locations/{{region}}/endpoints")
	if err != nil {
		return err
	}

	log.Printf("[DEBUG] Creating new Endpoint: %#v", obj)
	billingProject := ""

	project, err := getProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for Endpoint: %s", err)
	}
	billingProject = project

	// err == nil indicates that the billing_project value was found
	if bp, err := getBillingProject(d, config); err == nil {
		billingProject = bp
	}

	res, err := sendRequestWithTimeout(config, "POST", billingProject, url, userAgent, obj, d.Timeout(schema.TimeoutCreate))
	if err != nil {
		return fmt.Errorf("Error creating Endpoint: %s", err)
	}

	// Store the ID now
	id, err := replaceVars(d, config, "projects/{{project}}/locations/{{region}}/endpoints/{{name}}")
	if err != nil {
		return fmt.Errorf("Error constructing id: %s", err)
	}
	d.SetId(id)

	// Use the resource in the operation response to populate
	// identity fields and d.Id() before read
	var opRes map[string]interface{}
	err = vertexAIOperationWaitTimeWithResponse(
		config, res, &opRes, project, "Creating Endpoint", userAgent,
		d.Timeout(schema.TimeoutCreate))
	if err != nil {
		// The resource didn't actually create
		d.SetId("")
		return fmt.Errorf("Error waiting to create Endpoint: %s", err)
	}

	if err := d.Set("name", flattenVertexAIEndpointName(opRes["name"], d, config)); err != nil {
		return err
	}

	// This may have caused the ID to update - update it if so.
	id, err = replaceVars(d, config, "projects/{{project}}/locations/{{region}}/endpoints/{{name}}")
	if err != nil {
		return fmt.Errorf("Error constructing id: %s", err)
	}
	d.SetId(id)

	log.Printf("[DEBUG] Finished creating Endpoint %q: %#v", d.Id(), res)

	return resourceVertexAIEndpointRead(d, meta)
}

func resourceVertexAIEndpointRead(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*Config)
	userAgent, err := generateUserAgentString(d, config.userAgent)
	if err != nil {
		return err
	}

	url, err := replaceVars(d, config, "{{VertexAIBasePath}}projects/{{project}}/locations/{{region}}/endpoints/{{name}}")
	if err != nil {
		return err
	}

	billingProject := ""

	project, err := getProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for Endpoint: %s", err)
	}
	billingProject = project

	// err == nil indicates that the billing_project value was found
	if bp, err := getBillingProject(d, config); err == nil {
		billingProject = bp
	}

	res, err := sendRequest(config, "GET", billingProject, url, userAgent, nil)
	if err != nil {
		return handleNotFoundError(err, d, fmt.Sprintf("VertexAIEndpoint %q", d.Id()))
	}

	if err := d.Set("project", project); err != nil {
		return fmt.Errorf("Error reading Endpoint: %s", err)
	}

	if err := d.Set("name", flattenVertexAIEndpointName(res["name"], d, config)); err != nil {
		return fmt.Errorf("Error reading Endpoint: %s", err)
	}
	if err := d.Set("display_name", flattenVertexAIEndpointDisplayName(res["displayName"], d, config)); err != nil {
		return fmt.Errorf("Error reading Endpoint: %s", err)
	}
	if err := d.Set("description", flattenVertexAIEndpointDescription(res["description"], d, config)); err != nil {
		return fmt.Errorf("Error reading Endpoint: %s", err)
	}
	if err := d.Set("deployed_models", flattenVertexAIEndpointDeployedModels(res["deployedModels"], d, config)); err != nil {
		return fmt.Errorf("Error reading Endpoint: %s", err)
	}
	if err := d.Set("traffic_split", flattenVertexAIEndpointTrafficSplit(res["trafficSplit"], d, config)); err != nil {
		return fmt.Errorf("Error reading Endpoint: %s", err)
	}
	if err := d.Set("labels", flattenVertexAIEndpointLabels(res["labels"], d, config)); err != nil {
		return fmt.Errorf("Error reading Endpoint: %s", err)
	}
	if err := d.Set("encryption_spec", flattenVertexAIEndpointEncryptionSpec(res["encryptionSpec"], d, config)); err != nil {
		return fmt.Errorf("Error reading Endpoint: %s", err)
	}
	if err := d.Set("network", flattenVertexAIEndpointNetwork(res["network"], d, config)); err != nil {
		return fmt.Errorf("Error reading Endpoint: %s", err)
	}
	if err := d.Set("model_deployment_monitoring_job", flattenVertexAIEndpointModelDeploymentMonitoringJob(res["modelDeploymentMonitoringJob"], d, config)); err != nil {
		return fmt.Errorf("Error reading Endpoint: %s", err)
	}

	return nil
}

func resourceVertexAIEndpointUpdate(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*Config)
	userAgent, err := generateUserAgentString(d, config.userAgent)
	if err != nil {
		return err
	}

	billingProject := ""

	project, err := getProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for Endpoint: %s", err)
	}
	billingProject = project

	obj := make(map[string]interface{})
	displayNameProp, err := expandVertexAIEndpointDisplayName(d.Get("display_name"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("display_name"); !isEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, displayNameProp)) {
		obj["displayName"] = displayNameProp
	}
	descriptionProp, err := expandVertexAIEndpointDescription(d.Get("description"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("description"); !isEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, descriptionProp)) {
		obj["description"] = descriptionProp
	}
	trafficSplitProp, err := expandVertexAIEndpointTrafficSplit(d.Get("traffic_split"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("traffic_split"); !isEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, trafficSplitProp)) {
		obj["trafficSplit"] = trafficSplitProp
	}
	labelsProp, err := expandVertexAIEndpointLabels(d.Get("labels"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("labels"); !isEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, labelsProp)) {
		obj["labels"] = labelsProp
	}
	networkProp, err := expandVertexAIEndpointNetwork(d.Get("network"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("network"); !isEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, networkProp)) {
		obj["network"] = networkProp
	}

	url, err := replaceVars(d, config, "{{VertexAIBasePath}}projects/{{project}}/locations/{{region}}/endpoints/{{name}}")
	if err != nil {
		return err
	}

	log.Printf("[DEBUG] Updating Endpoint %q: %#v", d.Id(), obj)
	updateMask := []string{}

	if d.HasChange("display_name") {
		updateMask = append(updateMask, "displayName")
	}

	if d.HasChange("description") {
		updateMask = append(updateMask, "description")
	}

	if d.HasChange("traffic_split") {
		updateMask = append(updateMask, "trafficSplit")
	}

	if d.HasChange("labels") {
		updateMask = append(updateMask, "labels")
	}

	if d.HasChange("network") {
		updateMask = append(updateMask, "network")
	}
	// updateMask is a URL parameter but not present in the schema, so replaceVars
	// won't set it
	url, err = addQueryParams(url, map[string]string{"updateMask": strings.Join(updateMask, ",")})
	if err != nil {
		return err
	}

	// err == nil indicates that the billing_project value was found
	if bp, err := getBillingProject(d, config); err == nil {
		billingProject = bp
	}

	res, err := sendRequestWithTimeout(config, "PATCH", billingProject, url, userAgent, obj, d.Timeout(schema.TimeoutUpdate))

	if err != nil {
		return fmt.Errorf("Error updating Endpoint %q: %s", d.Id(), err)
	} else {
		log.Printf("[DEBUG] Finished updating Endpoint %q: %#v", d.Id(), res)
	}

	err = vertexAIOperationWaitTime(
		config, res, project, "Updating Endpoint", userAgent,
		d.Timeout(schema.TimeoutUpdate))

	if err != nil {
		return err
	}

	return resourceVertexAIEndpointRead(d, meta)
}

func resourceVertexAIEndpointDelete(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*Config)
	userAgent, err := generateUserAgentString(d, config.userAgent)
	if err != nil {
		return err
	}

	billingProject := ""

	project, err := getProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for Endpoint: %s", err)
	}
	billingProject = project

	url, err := replaceVars(d, config, "{{VertexAIBasePath}}projects/{{project}}/locations/{{region}}/endpoints/{{name}}")
	if err != nil {
		return err
	}

	var obj map[string]interface{}
	log.Printf("[DEBUG] Deleting Endpoint %q", d.Id())

	// err == nil indicates that the billing_project value was found
	if bp, err := getBillingProject(d, config); err == nil {
		billingProject = bp
	}

	res, err := sendRequestWithTimeout(config, "DELETE", billingProject, url, userAgent, obj, d.Timeout(schema.TimeoutDelete))
	if err != nil {
		return handleNotFoundError(err, d, "Endpoint")
	}

	err = vertexAIOperationWaitTime(
		config, res, project, "Deleting Endpoint", userAgent,
		d.Timeout(schema.TimeoutDelete))

	if err != nil {
		return err
	}

	log.Printf("[DEBUG] Finished deleting Endpoint %q: %#v", d.Id(), res)
	return nil
}

func resourceVertexAIEndpointImport(d *schema.ResourceData, meta interface{}) ([]*schema.ResourceData, error) {
	config := meta.(*Config)
	if err := parseImportId([]string{
		"projects/(?P<project>[^/]+)/locations/(?P<region>[^/]+)/endpoints/(?P<name>[^/]+)",
		"(?P<project>[^/]+)/(?P<region>[^/]+)/(?P<name>[^/]+)",
		"(?P<region>[^/]+)/(?P<name>[^/]+)",
		"(?P<name>[^/]+)",
	}, d, config); err != nil {
		return nil, err
	}

	// Replace import id for the resource id
	id, err := replaceVars(d, config, "projects/{{project}}/locations/{{region}}/endpoints/{{name}}")
	if err != nil {
		return nil, fmt.Errorf("Error constructing id: %s", err)
	}
	d.SetId(id)

	return []*schema.ResourceData{d}, nil
}

func flattenVertexAIEndpointName(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIEndpointDisplayName(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIEndpointDescription(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIEndpointDeployedModels(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["id"] =
		flattenVertexAIEndpointDeployedModelsId(original["id"], d, config)
	transformed["model"] =
		flattenVertexAIEndpointDeployedModelsModel(original["model"], d, config)
	transformed["create_time"] =
		flattenVertexAIEndpointDeployedModelsCreateTime(original["createTime"], d, config)
	transformed["service_account"] =
		flattenVertexAIEndpointDeployedModelsServiceAccount(original["serviceAccount"], d, config)
	transformed["enable_container_logging"] =
		flattenVertexAIEndpointDeployedModelsEnableContainerLogging(original["enableContainerLogging"], d, config)
	transformed["enable_access_logging"] =
		flattenVertexAIEndpointDeployedModelsEnableAccessLogging(original["enableAccessLogging"], d, config)
	transformed["private_endpoints"] =
		flattenVertexAIEndpointDeployedModelsPrivateEndpoints(original["privateEndpoints"], d, config)
	transformed["dedicated_resources"] =
		flattenVertexAIEndpointDeployedModelsDedicatedResources(original["dedicatedResources"], d, config)
	transformed["automatic_resources"] =
		flattenVertexAIEndpointDeployedModelsAutomaticResources(original["automaticResources"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIEndpointDeployedModelsId(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIEndpointDeployedModelsModel(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIEndpointDeployedModelsCreateTime(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIEndpointDeployedModelsServiceAccount(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIEndpointDeployedModelsEnableContainerLogging(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIEndpointDeployedModelsEnableAccessLogging(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIEndpointDeployedModelsPrivateEndpoints(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["predict_http_uri"] =
		flattenVertexAIEndpointDeployedModelsPrivateEndpointsPredictHttpUri(original["predictHttpUri"], d, config)
	transformed["explain_http_uri"] =
		flattenVertexAIEndpointDeployedModelsPrivateEndpointsExplainHttpUri(original["explainHttpUri"], d, config)
	transformed["health_http_uri"] =
		flattenVertexAIEndpointDeployedModelsPrivateEndpointsHealthHttpUri(original["healthHttpUri"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIEndpointDeployedModelsPrivateEndpointsPredictHttpUri(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIEndpointDeployedModelsPrivateEndpointsExplainHttpUri(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIEndpointDeployedModelsPrivateEndpointsHealthHttpUri(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIEndpointDeployedModelsDedicatedResources(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["machine_spec"] =
		flattenVertexAIEndpointDeployedModelsDedicatedResourcesMachineSpec(original["machineSpec"], d, config)
	transformed["min_replica_count"] =
		flattenVertexAIEndpointDeployedModelsDedicatedResourcesMinReplicaCount(original["minReplicaCount"], d, config)
	transformed["max_replica_count"] =
		flattenVertexAIEndpointDeployedModelsDedicatedResourcesMaxReplicaCount(original["maxReplicaCount"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIEndpointDeployedModelsDedicatedResourcesMachineSpec(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["machine_type"] =
		flattenVertexAIEndpointDeployedModelsDedicatedResourcesMachineSpecMachineType(original["machineType"], d, config)
	transformed["accelerator_type"] =
		flattenVertexAIEndpointDeployedModelsDedicatedResourcesMachineSpecAcceleratorType(original["acceleratorType"], d, config)
	transformed["accelerator_count"] =
		flattenVertexAIEndpointDeployedModelsDedicatedResourcesMachineSpecAcceleratorCount(original["acceleratorCount"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIEndpointDeployedModelsDedicatedResourcesMachineSpecMachineType(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIEndpointDeployedModelsDedicatedResourcesMachineSpecAcceleratorType(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIEndpointDeployedModelsDedicatedResourcesMachineSpecAcceleratorCount(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := stringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointDeployedModelsDedicatedResourcesMinReplicaCount(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := stringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointDeployedModelsDedicatedResourcesMaxReplicaCount(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := stringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointDeployedModelsAutomaticResources(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["min_replica_count"] =
		flattenVertexAIEndpointDeployedModelsAutomaticResourcesMinReplicaCount(original["minReplicaCount"], d, config)
	transformed["max_replica_count"] =
		flattenVertexAIEndpointDeployedModelsAutomaticResourcesMaxReplicaCount(original["maxReplicaCount"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIEndpointDeployedModelsAutomaticResourcesMinReplicaCount(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := stringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointDeployedModelsAutomaticResourcesMaxReplicaCount(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := stringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIEndpointTrafficSplit(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIEndpointLabels(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIEndpointEncryptionSpec(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["kms_key_name"] =
		flattenVertexAIEndpointEncryptionSpecKmsKeyName(original["kmsKeyName"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIEndpointEncryptionSpecKmsKeyName(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIEndpointNetwork(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func flattenVertexAIEndpointModelDeploymentMonitoringJob(v interface{}, d *schema.ResourceData, config *Config) interface{} {
	return v
}

func expandVertexAIEndpointDisplayName(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointDescription(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointDeployedModels(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedId, err := expandVertexAIEndpointDeployedModelsId(original["id"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedId); val.IsValid() && !isEmptyValue(val) {
		transformed["id"] = transformedId
	}

	transformedModel, err := expandVertexAIEndpointDeployedModelsModel(original["model"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedModel); val.IsValid() && !isEmptyValue(val) {
		transformed["model"] = transformedModel
	}

	transformedCreateTime, err := expandVertexAIEndpointDeployedModelsCreateTime(original["create_time"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedCreateTime); val.IsValid() && !isEmptyValue(val) {
		transformed["createTime"] = transformedCreateTime
	}

	transformedServiceAccount, err := expandVertexAIEndpointDeployedModelsServiceAccount(original["service_account"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedServiceAccount); val.IsValid() && !isEmptyValue(val) {
		transformed["serviceAccount"] = transformedServiceAccount
	}

	transformedEnableContainerLogging, err := expandVertexAIEndpointDeployedModelsEnableContainerLogging(original["enable_container_logging"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedEnableContainerLogging); val.IsValid() && !isEmptyValue(val) {
		transformed["enableContainerLogging"] = transformedEnableContainerLogging
	}

	transformedEnableAccessLogging, err := expandVertexAIEndpointDeployedModelsEnableAccessLogging(original["enable_access_logging"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedEnableAccessLogging); val.IsValid() && !isEmptyValue(val) {
		transformed["enableAccessLogging"] = transformedEnableAccessLogging
	}

	transformedPrivateEndpoints, err := expandVertexAIEndpointDeployedModelsPrivateEndpoints(original["private_endpoints"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPrivateEndpoints); val.IsValid() && !isEmptyValue(val) {
		transformed["privateEndpoints"] = transformedPrivateEndpoints
	}

	transformedDedicatedResources, err := expandVertexAIEndpointDeployedModelsDedicatedResources(original["dedicated_resources"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedDedicatedResources); val.IsValid() && !isEmptyValue(val) {
		transformed["dedicatedResources"] = transformedDedicatedResources
	}

	transformedAutomaticResources, err := expandVertexAIEndpointDeployedModelsAutomaticResources(original["automatic_resources"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedAutomaticResources); val.IsValid() && !isEmptyValue(val) {
		transformed["automaticResources"] = transformedAutomaticResources
	}

	return transformed, nil
}

func expandVertexAIEndpointDeployedModelsId(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointDeployedModelsModel(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointDeployedModelsCreateTime(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointDeployedModelsServiceAccount(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointDeployedModelsEnableContainerLogging(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointDeployedModelsEnableAccessLogging(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointDeployedModelsPrivateEndpoints(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedPredictHttpUri, err := expandVertexAIEndpointDeployedModelsPrivateEndpointsPredictHttpUri(original["predict_http_uri"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPredictHttpUri); val.IsValid() && !isEmptyValue(val) {
		transformed["predictHttpUri"] = transformedPredictHttpUri
	}

	transformedExplainHttpUri, err := expandVertexAIEndpointDeployedModelsPrivateEndpointsExplainHttpUri(original["explain_http_uri"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedExplainHttpUri); val.IsValid() && !isEmptyValue(val) {
		transformed["explainHttpUri"] = transformedExplainHttpUri
	}

	transformedHealthHttpUri, err := expandVertexAIEndpointDeployedModelsPrivateEndpointsHealthHttpUri(original["health_http_uri"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedHealthHttpUri); val.IsValid() && !isEmptyValue(val) {
		transformed["healthHttpUri"] = transformedHealthHttpUri
	}

	return transformed, nil
}

func expandVertexAIEndpointDeployedModelsPrivateEndpointsPredictHttpUri(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointDeployedModelsPrivateEndpointsExplainHttpUri(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointDeployedModelsPrivateEndpointsHealthHttpUri(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointDeployedModelsDedicatedResources(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedMachineSpec, err := expandVertexAIEndpointDeployedModelsDedicatedResourcesMachineSpec(original["machine_spec"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMachineSpec); val.IsValid() && !isEmptyValue(val) {
		transformed["machineSpec"] = transformedMachineSpec
	}

	transformedMinReplicaCount, err := expandVertexAIEndpointDeployedModelsDedicatedResourcesMinReplicaCount(original["min_replica_count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMinReplicaCount); val.IsValid() && !isEmptyValue(val) {
		transformed["minReplicaCount"] = transformedMinReplicaCount
	}

	transformedMaxReplicaCount, err := expandVertexAIEndpointDeployedModelsDedicatedResourcesMaxReplicaCount(original["max_replica_count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMaxReplicaCount); val.IsValid() && !isEmptyValue(val) {
		transformed["maxReplicaCount"] = transformedMaxReplicaCount
	}

	return transformed, nil
}

func expandVertexAIEndpointDeployedModelsDedicatedResourcesMachineSpec(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedMachineType, err := expandVertexAIEndpointDeployedModelsDedicatedResourcesMachineSpecMachineType(original["machine_type"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMachineType); val.IsValid() && !isEmptyValue(val) {
		transformed["machineType"] = transformedMachineType
	}

	transformedAcceleratorType, err := expandVertexAIEndpointDeployedModelsDedicatedResourcesMachineSpecAcceleratorType(original["accelerator_type"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedAcceleratorType); val.IsValid() && !isEmptyValue(val) {
		transformed["acceleratorType"] = transformedAcceleratorType
	}

	transformedAcceleratorCount, err := expandVertexAIEndpointDeployedModelsDedicatedResourcesMachineSpecAcceleratorCount(original["accelerator_count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedAcceleratorCount); val.IsValid() && !isEmptyValue(val) {
		transformed["acceleratorCount"] = transformedAcceleratorCount
	}

	return transformed, nil
}

func expandVertexAIEndpointDeployedModelsDedicatedResourcesMachineSpecMachineType(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointDeployedModelsDedicatedResourcesMachineSpecAcceleratorType(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointDeployedModelsDedicatedResourcesMachineSpecAcceleratorCount(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointDeployedModelsDedicatedResourcesMinReplicaCount(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointDeployedModelsDedicatedResourcesMaxReplicaCount(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointDeployedModelsAutomaticResources(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedMinReplicaCount, err := expandVertexAIEndpointDeployedModelsAutomaticResourcesMinReplicaCount(original["min_replica_count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMinReplicaCount); val.IsValid() && !isEmptyValue(val) {
		transformed["minReplicaCount"] = transformedMinReplicaCount
	}

	transformedMaxReplicaCount, err := expandVertexAIEndpointDeployedModelsAutomaticResourcesMaxReplicaCount(original["max_replica_count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMaxReplicaCount); val.IsValid() && !isEmptyValue(val) {
		transformed["maxReplicaCount"] = transformedMaxReplicaCount
	}

	return transformed, nil
}

func expandVertexAIEndpointDeployedModelsAutomaticResourcesMinReplicaCount(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointDeployedModelsAutomaticResourcesMaxReplicaCount(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointTrafficSplit(v interface{}, d TerraformResourceData, config *Config) (map[string]string, error) {
	if v == nil {
		return map[string]string{}, nil
	}
	m := make(map[string]string)
	for k, val := range v.(map[string]interface{}) {
		m[k] = val.(string)
	}
	return m, nil
}

func expandVertexAIEndpointLabels(v interface{}, d TerraformResourceData, config *Config) (map[string]string, error) {
	if v == nil {
		return map[string]string{}, nil
	}
	m := make(map[string]string)
	for k, val := range v.(map[string]interface{}) {
		m[k] = val.(string)
	}
	return m, nil
}

func expandVertexAIEndpointEncryptionSpec(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedKmsKeyName, err := expandVertexAIEndpointEncryptionSpecKmsKeyName(original["kms_key_name"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedKmsKeyName); val.IsValid() && !isEmptyValue(val) {
		transformed["kmsKeyName"] = transformedKmsKeyName
	}

	return transformed, nil
}

func expandVertexAIEndpointEncryptionSpecKmsKeyName(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointNetwork(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIEndpointModelDeploymentMonitoringJob(v interface{}, d TerraformResourceData, config *Config) (interface{}, error) {
	return v, nil
}
