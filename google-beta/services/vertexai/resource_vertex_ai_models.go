// Copyright (c) HashiCorp, Inc.
// SPDX-License-Identifier: MPL-2.0

// ----------------------------------------------------------------------------
//
//     ***     AUTO GENERATED CODE    ***    Type: MMv1     ***
//
// ----------------------------------------------------------------------------
//
//     This file is automatically generated by Magic Modules and manual
//     changes will be clobbered when the file is regenerated.
//
//     Please read more about how to change this file in
//     .github/CONTRIBUTING.md.
//
// ----------------------------------------------------------------------------

package vertexai

import (
	"fmt"
	"log"
	"net/http"
	"reflect"
	"regexp"
	"time"

	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/customdiff"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/schema"

	"github.com/hashicorp/terraform-provider-google-beta/google-beta/tpgresource"
	transport_tpg "github.com/hashicorp/terraform-provider-google-beta/google-beta/transport"
	"github.com/hashicorp/terraform-provider-google-beta/google-beta/verify"
)

func ResourceVertexAIModels() *schema.Resource {
	return &schema.Resource{
		Create: resourceVertexAIModelsCreate,
		Read:   resourceVertexAIModelsRead,
		Update: resourceVertexAIModelsUpdate,
		Delete: resourceVertexAIModelsDelete,

		Timeouts: &schema.ResourceTimeout{
			Create: schema.DefaultTimeout(20 * time.Minute),
			Update: schema.DefaultTimeout(20 * time.Minute),
			Delete: schema.DefaultTimeout(20 * time.Minute),
		},

		CustomizeDiff: customdiff.All(
			tpgresource.SetLabelsDiff,
			tpgresource.DefaultProviderProject,
		),

		Schema: map[string]*schema.Schema{
			"artifact_uri": {
				Type:        schema.TypeString,
				Optional:    true,
				ForceNew:    true,
				Description: `The path to the directory containing the Model artifact and any of its supporting files.`,
			},
			"container_spec": {
				Type:        schema.TypeList,
				Optional:    true,
				Description: `The specification of the container that is to be used when deploying this Model.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"image_uri": {
							Type:        schema.TypeString,
							Required:    true,
							ForceNew:    true,
							Description: `URI of the Docker image to be used as the custom container for serving predictions.`,
						},
						"args": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `Specifies arguments for the command that runs when the container starts.`,
							Elem: &schema.Schema{
								Type: schema.TypeString,
							},
						},
						"command": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `Specifies the command that runs when the container starts. This overrides the container's ENTRYPOINT.`,
							Elem: &schema.Schema{
								Type: schema.TypeString,
							},
						},
						"deployment_timeout": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: `Deployment timeout. Limit for deployment timeout is 2 hours.`,
						},
						"env": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `List of environment variables to set in the container.`,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"name": {
										Type:        schema.TypeString,
										Required:    true,
										Description: `name of the environment variable. Must be a valid C identifier.`,
									},
									"value": {
										Type:        schema.TypeString,
										Required:    true,
										Description: `Variables that reference a $(VAR_NAME) are expanded using the previous defined environment variables in the container and any service environment variables.`,
									},
								},
							},
						},
						"grpc_ports": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `List of ports to expose from the container.`,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"container_port": {
										Type:        schema.TypeInt,
										Required:    true,
										Description: `The number of the port to expose on the pod's IP address. Must be a valid port number, between 1 and 65535 inclusive.`,
									},
								},
							},
						},
						"health_probe": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `Specification for Kubernetes readiness probe.`,
							MaxItems:    1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"exec": {
										Type:        schema.TypeList,
										Optional:    true,
										Description: `Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. Must be greater or equal to periodSeconds.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"command": {
													Type:        schema.TypeList,
													Optional:    true,
													Description: `Command is the command line to execute inside the container, the working directory for the command is root ('/') in the container's filesystem.`,
													Elem: &schema.Schema{
														Type: schema.TypeString,
													},
												},
											},
										},
									},
									"period_seconds": {
										Type:        schema.TypeInt,
										Optional:    true,
										ForceNew:    true,
										Description: `How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1. Must be less than timeoutSeconds.`,
										Default:     10,
									},
									"timeout_seconds": {
										Type:        schema.TypeInt,
										Optional:    true,
										ForceNew:    true,
										Description: `Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. Must be greater or equal to periodSeconds.`,
										Default:     1,
									},
								},
							},
						},
						"health_route": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: `HTTP path on the container to send health checks to.`,
						},
						"ports": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `List of ports to expose from the container.`,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"container_port": {
										Type:        schema.TypeInt,
										Required:    true,
										Description: `The number of the port to expose on the pod's IP address. Must be a valid port number, between 1 and 65535 inclusive.`,
									},
								},
							},
						},
						"predict_route": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: `HTTP path on the container to send prediction requests to.`,
						},
						"shared_memory_size_mb": {
							Type:        schema.TypeInt,
							Optional:    true,
							ForceNew:    true,
							Description: `The amount of the VM memory to reserve as the shared memory for the model in megabytes.`,
						},
						"startup_probe": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `Specification for Kubernetes startup probe.`,
							MaxItems:    1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"exec": {
										Type:        schema.TypeList,
										Optional:    true,
										Description: `Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. Must be greater or equal to periodSeconds.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"command": {
													Type:        schema.TypeList,
													Optional:    true,
													Description: `Command is the command line to execute inside the container, the working directory for the command is root ('/') in the container's filesystem.`,
													Elem: &schema.Schema{
														Type: schema.TypeString,
													},
												},
											},
										},
									},
									"period_seconds": {
										Type:        schema.TypeInt,
										Optional:    true,
										ForceNew:    true,
										Description: `How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1. Must be less than timeoutSeconds.`,
										Default:     10,
									},
									"timeout_seconds": {
										Type:        schema.TypeInt,
										Optional:    true,
										ForceNew:    true,
										Description: `Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. Must be greater or equal to periodSeconds.`,
										Default:     1,
									},
								},
							},
						},
					},
				},
			},
			"description": {
				Type:        schema.TypeString,
				Optional:    true,
				ForceNew:    true,
				Description: `Description of the Model.`,
			},
			"display_name": {
				Type:         schema.TypeString,
				Optional:     true,
				Description:  `The display name of the Model. The name can be up to 128 characters long and can consist of any UTF-8 characters.`,
				AtLeastOneOf: []string{"model", "display_name"},
			},
			"encryption_spec": {
				Type:        schema.TypeList,
				Optional:    true,
				ForceNew:    true,
				Description: `Customer-managed encryption key spec for a MetadataStore. If set, this MetadataStore and all sub-resources of this MetadataStore will be secured by this key.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"kms_key_name": {
							Type:     schema.TypeString,
							Optional: true,
							ForceNew: true,
							Description: `Required. The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource.
Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the resource is created.`,
						},
					},
				},
			},
			"explanation_spec": {
				Type:        schema.TypeList,
				Optional:    true,
				Description: `The pointers to DeployedModels created from this Model. Note that Model could have been deployed to Endpoints in different Locations.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"parameters": {
							Type:        schema.TypeList,
							Required:    true,
							Description: `Parameters that configure explaining of the Model's predictions.`,
							MaxItems:    1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"examples": {
										Type:        schema.TypeList,
										Optional:    true,
										Description: `Description of the Model.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"example_gcs_source": {
													Type:        schema.TypeList,
													Optional:    true,
													Description: `The number of neighbors to return when querying for examples.`,
													MaxItems:    1,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"data_format": {
																Type:         schema.TypeString,
																Optional:     true,
																ValidateFunc: verify.ValidateEnum([]string{"JSONL", ""}),
																Description:  `The format in which instances are given, if not specified, assume its JSONL format. Currently only JSONL format is supported. Possible values: ["JSONL"]`,
															},
															"gcs_source": {
																Type:        schema.TypeList,
																Optional:    true,
																Description: `The Cloud Storage location for the input instances.`,
																MaxItems:    1,
																Elem: &schema.Resource{
																	Schema: map[string]*schema.Schema{
																		"uris": {
																			Type:        schema.TypeList,
																			Required:    true,
																			Description: `Google Cloud Storage URI(-s) to the input file(s). May contain wildcards.`,
																			Elem: &schema.Schema{
																				Type: schema.TypeString,
																			},
																		},
																	},
																},
															},
														},
													},
												},
												"nearest_neighbor_search_config": {
													Type:        schema.TypeList,
													Optional:    true,
													Description: `The number of neighbors to return when querying for examples.`,
													MaxItems:    1,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"contents_delta_uri": {
																Type:     schema.TypeString,
																Required: true,
																Description: `Allows inserting, updating  or deleting the contents of the Matching Engine Index.
The string must be a valid Cloud Storage directory path. If this
field is set when calling IndexService.UpdateIndex, then no other
Index field can be also updated as part of the same call.
The expected structure and format of the files this URI points to is
described at https://cloud.google.com/vertex-ai/docs/matching-engine/using-matching-engine#input-data-format`,
															},
															"config": {
																Type:        schema.TypeList,
																Optional:    true,
																ForceNew:    true,
																Description: `The configuration of the Matching Engine Index.`,
																MaxItems:    1,
																Elem: &schema.Resource{
																	Schema: map[string]*schema.Schema{
																		"dimensions": {
																			Type:        schema.TypeInt,
																			Required:    true,
																			Description: `The number of dimensions of the input vectors.`,
																		},
																		"algorithm_config": {
																			Type:        schema.TypeList,
																			Optional:    true,
																			Description: `The configuration with regard to the algorithms used for efficient search.`,
																			MaxItems:    1,
																			Elem: &schema.Resource{
																				Schema: map[string]*schema.Schema{
																					"brute_force_config": {
																						Type:     schema.TypeList,
																						Optional: true,
																						Description: `Configuration options for using brute force search, which simply implements the
standard linear search in the database for each query.`,
																						MaxItems: 1,
																						Elem: &schema.Resource{
																							Schema: map[string]*schema.Schema{},
																						},
																						ExactlyOneOf: []string{},
																					},
																					"tree_ah_config": {
																						Type:     schema.TypeList,
																						Optional: true,
																						Description: `Configuration options for using the tree-AH algorithm (Shallow tree + Asymmetric Hashing).
Please refer to this paper for more details: https://arxiv.org/abs/1908.10396`,
																						MaxItems: 1,
																						Elem: &schema.Resource{
																							Schema: map[string]*schema.Schema{
																								"leaf_node_embedding_count": {
																									Type:        schema.TypeInt,
																									Optional:    true,
																									Description: `Number of embeddings on each leaf node. The default value is 1000 if not set.`,
																									Default:     1000,
																								},
																								"leaf_nodes_to_search_percent": {
																									Type:     schema.TypeInt,
																									Optional: true,
																									Description: `The default percentage of leaf nodes that any query may be searched. Must be in
range 1-100, inclusive. The default value is 10 (means 10%) if not set.`,
																									Default: 10,
																								},
																							},
																						},
																						ExactlyOneOf: []string{},
																					},
																				},
																			},
																		},
																		"approximate_neighbors_count": {
																			Type:     schema.TypeInt,
																			Optional: true,
																			Description: `The default number of neighbors to find via approximate search before exact reordering is
performed. Exact reordering is a procedure where results returned by an
approximate search algorithm are reordered via a more expensive distance computation.
Required if tree-AH algorithm is used.`,
																		},
																		"distance_measure_type": {
																			Type:     schema.TypeString,
																			Optional: true,
																			Description: `The distance measure used in nearest neighbor search. The value must be one of the followings:
* SQUARED_L2_DISTANCE: Euclidean (L_2) Distance
* L1_DISTANCE: Manhattan (L_1) Distance
* COSINE_DISTANCE: Cosine Distance. Defined as 1 - cosine similarity.
* DOT_PRODUCT_DISTANCE: Dot Product Distance. Defined as a negative of the dot product`,
																			Default: "DOT_PRODUCT_DISTANCE",
																		},
																		"feature_norm_type": {
																			Type:     schema.TypeString,
																			Optional: true,
																			Description: `Type of normalization to be carried out on each vector. The value must be one of the followings:
* UNIT_L2_NORM: Unit L2 normalization type
* NONE: No normalization type is specified.`,
																			Default: "NONE",
																		},
																		"shard_size": {
																			Type:     schema.TypeString,
																			Computed: true,
																			Optional: true,
																			ForceNew: true,
																			Description: `Index data is split into equal parts to be processed. These are called "shards".
The shard size must be specified when creating an index. The value must be one of the followings:
* SHARD_SIZE_SMALL: Small (2GB)
* SHARD_SIZE_MEDIUM: Medium (20GB)
* SHARD_SIZE_LARGE: Large (50GB)`,
																		},
																	},
																},
															},
															"is_complete_overwrite": {
																Type:     schema.TypeBool,
																Optional: true,
																Description: `If this field is set together with contentsDeltaUri when calling IndexService.UpdateIndex,
then existing content of the Index will be replaced by the data from the contentsDeltaUri.`,
																Default: false,
															},
														},
													},
													ExactlyOneOf: []string{},
												},
												"neighbor_count": {
													Type:        schema.TypeInt,
													Optional:    true,
													Description: `The number of neighbors to return when querying for examples.`,
												},
												"presets": {
													Type:        schema.TypeList,
													Optional:    true,
													Description: `Simplified preset configuration, which automatically sets configuration values based on the desired query speed-precision trade-off and modality.`,
													MaxItems:    1,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"modality": {
																Type:         schema.TypeString,
																Optional:     true,
																ValidateFunc: verify.ValidateEnum([]string{"IMAGE", "TEXT", "TABULAR", ""}),
																Description:  `The modality of the uploaded model, which automatically configures the distance measurement and feature normalization for the underlying example index and queries Possible values: ["IMAGE", "TEXT", "TABULAR"]`,
															},
															"query": {
																Type:         schema.TypeString,
																Optional:     true,
																ValidateFunc: verify.ValidateEnum([]string{"PRECISE", "FAST", ""}),
																Description:  `Preset option controlling parameters for speed-precision trade-off when querying for examples. If omitted, defaults to PRECISE. Possible values: ["PRECISE", "FAST"]`,
															},
														},
													},
													ExactlyOneOf: []string{},
												},
											},
										},
									},
									"integrated_gradients_attribution": {
										Type:        schema.TypeList,
										Optional:    true,
										Description: `An attribution method that computes Aumann-Shapley values taking advantage of the model's fully differentiable structure.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"blur_baseline_config": {
													Type:        schema.TypeList,
													Required:    true,
													Description: `Config for IG with blur baseline.`,
													MaxItems:    1,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"max_blur_sigma": {
																Type:        schema.TypeFloat,
																Optional:    true,
																Description: `The standard deviation of the blur kernel for the blurred baseline.`,
															},
														},
													},
												},
												"step_count": {
													Type:        schema.TypeInt,
													Required:    true,
													Description: `The number of feature permutations to consider when approximating the Shapley values. Valid range of its value is [1, 100], inclusively.`,
												},
												"smooth_grad_config": {
													Type:        schema.TypeList,
													Optional:    true,
													Description: `When enabled, the gradients are approximated by averaging the gradients from noisy samples in the vicinity of the inputs.`,
													MaxItems:    1,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"feature_noise_sigma": {
																Type:        schema.TypeList,
																Optional:    true,
																Description: `The number of gradient samples to use for approximation.`,
																MaxItems:    1,
																Elem: &schema.Resource{
																	Schema: map[string]*schema.Schema{
																		"noise_sigma": {
																			Type:        schema.TypeList,
																			Optional:    true,
																			Description: `The number of gradient samples to use for approximation.`,
																			Elem: &schema.Resource{
																				Schema: map[string]*schema.Schema{
																					"name": {
																						Type:        schema.TypeString,
																						Optional:    true,
																						Description: `The name of the input feature for which noise sigma is provided.`,
																					},
																					"sigma": {
																						Type:        schema.TypeFloat,
																						Optional:    true,
																						Description: `This represents the standard deviation of the Gaussian kernel that will be used to add noise to the feature prior to computing gradients.`,
																						Default:     0.1,
																					},
																				},
																			},
																		},
																	},
																},
																AtLeastOneOf: []string{},
															},
															"noise_sigma": {
																Type:         schema.TypeFloat,
																Optional:     true,
																Description:  `This is a single float value and will be used to add noise to all the features.`,
																AtLeastOneOf: []string{},
															},
															"noisy_sample_count": {
																Type:        schema.TypeInt,
																Optional:    true,
																Description: `The number of gradient samples to use for approximation. Valid range of its value is [1, 50].`,
																Default:     3,
															},
														},
													},
												},
											},
										},
									},
									"output_indices": {
										Type:        schema.TypeList,
										Optional:    true,
										ForceNew:    true,
										Description: `If populated, only returns attributions that have outputIndex contained in outputIndices.`,
										Elem: &schema.Schema{
											Type: schema.TypeFloat,
										},
									},
									"sampled_shapley_attribution": {
										Type:        schema.TypeList,
										Optional:    true,
										ForceNew:    true,
										Description: `An attribution method that approximates Shapley values for features that contribute to the label being predicted.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"path_count": {
													Type:        schema.TypeInt,
													Required:    true,
													Description: `The number of feature permutations to consider when approximating the Shapley values. Valid range of its value is [1, 50], inclusively.`,
												},
											},
										},
									},
									"top_k": {
										Type:        schema.TypeInt,
										Optional:    true,
										ForceNew:    true,
										Description: `If populated, returns attributions for top K indices of outputs (defaults to 1).`,
										Default:     1,
									},
									"xrai_attribution": {
										Type:        schema.TypeList,
										Optional:    true,
										Description: `An attribution method that redistributes Integrated Gradients attribution to segmented regions, taking advantage of the model's fully differentiable structure.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"blur_baseline_config": {
													Type:        schema.TypeList,
													Required:    true,
													Description: `Config for IG with blur baseline.`,
													MaxItems:    1,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"max_blur_sigma": {
																Type:        schema.TypeFloat,
																Optional:    true,
																Description: `The standard deviation of the blur kernel for the blurred baseline.`,
																Default:     0.0,
															},
														},
													},
												},
												"step_count": {
													Type:        schema.TypeInt,
													Required:    true,
													Description: `The number of feature permutations to consider when approximating the Shapley values. Valid range of its value is [1, 100], inclusively.`,
												},
												"smooth_grad_config": {
													Type:        schema.TypeList,
													Optional:    true,
													Description: `When enabled, the gradients are approximated by averaging the gradients from noisy samples in the vicinity of the inputs.`,
													MaxItems:    1,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"feature_noise_sigma": {
																Type:        schema.TypeList,
																Optional:    true,
																Description: `The number of gradient samples to use for approximation.`,
																MaxItems:    1,
																Elem: &schema.Resource{
																	Schema: map[string]*schema.Schema{
																		"noise_sigma": {
																			Type:        schema.TypeList,
																			Optional:    true,
																			Description: `The number of gradient samples to use for approximation.`,
																			Elem: &schema.Resource{
																				Schema: map[string]*schema.Schema{
																					"name": {
																						Type:        schema.TypeString,
																						Optional:    true,
																						Description: `The name of the input feature for which noise sigma is provided.`,
																					},
																					"sigma": {
																						Type:        schema.TypeFloat,
																						Optional:    true,
																						Description: `This represents the standard deviation of the Gaussian kernel that will be used to add noise to the feature prior to computing gradients.`,
																						Default:     0.1,
																					},
																				},
																			},
																		},
																	},
																},
															},
															"noise_sigma": {
																Type:        schema.TypeFloat,
																Optional:    true,
																Description: `This is a single float value and will be used to add noise to all the features.`,
																Default:     0.1,
															},
															"noisy_sample_count": {
																Type:        schema.TypeInt,
																Optional:    true,
																Description: `The number of gradient samples to use for approximation. Valid range of its value is [1, 50].`,
																Default:     3,
															},
														},
													},
												},
											},
										},
									},
								},
							},
						},
						"metadata": {
							Type:        schema.TypeList,
							Optional:    true,
							Description: `metadata describing the Model's input and output for explanation.`,
							MaxItems:    1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"inputs": {
										Type:        schema.TypeSet,
										Required:    true,
										Description: `The resource name of the Artifact that was created in MetadataStore when creating the Model.`,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"name": {
													Type:     schema.TypeString,
													Required: true,
												},
												"group_name": {
													Type:        schema.TypeString,
													Required:    true,
													Description: `name of the group that the input belongs to. Features with the same group name will be treated as one feature when computing attributions.`,
												},
												"index_feature_mapping": {
													Type:        schema.TypeList,
													Required:    true,
													Description: `A list of feature names for each index in the input tensor.`,
													Elem: &schema.Schema{
														Type: schema.TypeString,
													},
												},
												"visualization": {
													Type:        schema.TypeList,
													Required:    true,
													Description: `Visualization configurations for image explanation.`,
													MaxItems:    1,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"clip_percent_lowerbound": {
																Type:        schema.TypeInt,
																Optional:    true,
																Description: `Excludes attributions below the specified percentile, from the highlighted areas.`,
																Default:     62,
															},
															"clip_percent_upperbound": {
																Type:        schema.TypeFloat,
																Optional:    true,
																Description: `Excludes attributions above the specified percentile from the highlighted areas.`,
																Default:     99.9,
															},
															"color_map": {
																Type:         schema.TypeString,
																Optional:     true,
																ValidateFunc: verify.ValidateEnum([]string{"PINK_GREEN", "VIRIDIS", "RED", "GREEN", "RED_GREEN", "PINK_WHITE_GREEN", ""}),
																Description:  `The color scheme used for highlighting areas. Possible values: ["PINK_GREEN", "VIRIDIS", "RED", "GREEN", "RED_GREEN", "PINK_WHITE_GREEN"]`,
															},
															"overlay_type": {
																Type:         schema.TypeString,
																Optional:     true,
																ValidateFunc: verify.ValidateEnum([]string{"NONE", "ORIGINAL", "GRAYSCALE", "MASK_BLACK", ""}),
																Description:  `How the original image is displayed in the visualization. Possible values: ["NONE", "ORIGINAL", "GRAYSCALE", "MASK_BLACK"]`,
															},
															"polarity": {
																Type:         schema.TypeString,
																Optional:     true,
																ValidateFunc: verify.ValidateEnum([]string{"POSITIVE", "NEGATIVE", "BOTH", ""}),
																Description:  `Whether to only highlight pixels with positive contributions, negative or both. Defaults to POSITIVE. Default value: "BOTH" Possible values: ["POSITIVE", "NEGATIVE", "BOTH"]`,
																Default:      "BOTH",
															},
															"type": {
																Type:         schema.TypeString,
																Optional:     true,
																ValidateFunc: verify.ValidateEnum([]string{"PIXELS", "OUTLINES", ""}),
																Description:  `type of the image visualization. Only applicable to Integrated Gradients attribution. Possible values: ["PIXELS", "OUTLINES"]`,
															},
														},
													},
												},
												"dense_shape_tensor_name": {
													Type:        schema.TypeString,
													Optional:    true,
													Description: `Specifies the shape of the values of the input if the input is a sparse representation.`,
												},
												"encoded_baselines": {
													Type:        schema.TypeList,
													Optional:    true,
													Description: `A list of baselines for the encoded tensor.`,
													Elem: &schema.Schema{
														Type: schema.TypeFloat,
													},
												},
												"encoded_tensor_name": {
													Type:        schema.TypeString,
													Optional:    true,
													Description: `Encoded tensor is a transformation of the input tensor.`,
												},
												"encoding": {
													Type:         schema.TypeString,
													Optional:     true,
													ValidateFunc: verify.ValidateEnum([]string{"IDENTITY", "BAG_OF_FEATURES", "BAG_OF_FEATURES_SPARSE", "IDENTITY_SPARSE", "INDICATOR", "COMBINED_EMBEDDING", "CONCAT_EMBEDDING", ""}),
													Description:  `Defines how the feature is encoded into the input tensor. Defaults to IDENTITY. Possible values: ["IDENTITY", "BAG_OF_FEATURES", "BAG_OF_FEATURES_SPARSE", "IDENTITY_SPARSE", "INDICATOR", "COMBINED_EMBEDDING", "CONCAT_EMBEDDING"]`,
												},
												"feature_value_domain": {
													Type:        schema.TypeList,
													Optional:    true,
													Description: `The domain details of the input feature value. Like min/max, original mean or standard deviation if normalized.`,
													MaxItems:    1,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"max_value": {
																Type:        schema.TypeFloat,
																Optional:    true,
																Description: `The maximum permissible value for this feature.`,
															},
															"min_value": {
																Type:        schema.TypeFloat,
																Optional:    true,
																Description: `The minimum permissible value for this feature.`,
															},
															"original_mean": {
																Type:        schema.TypeFloat,
																Optional:    true,
																Description: `If this input feature has been normalized to a mean value of 0, the originalMean specifies the mean value of the domain prior to normalization.`,
															},
															"original_stddev": {
																Type:        schema.TypeFloat,
																Optional:    true,
																Description: `If this input feature has been normalized to a standard deviation of 1.0, the originalStddev specifies the standard deviation of the domain prior to normalization.`,
															},
														},
													},
												},
												"indices_tensor_name": {
													Type:        schema.TypeString,
													Optional:    true,
													Description: `Specifies the index of the values of the input tensor.`,
												},
												"input_tensor_name": {
													Type:        schema.TypeString,
													Optional:    true,
													Description: `name of the input tensor for this feature. Required and is only applicable to Vertex AI-provided images for Tensorflow.`,
												},
												"modality": {
													Type:        schema.TypeString,
													Optional:    true,
													Description: `Modality of the feature. Valid values are: numeric, image.`,
													Default:     "numeric",
												},
											},
										},
									},
									"outputs": {
										Type:        schema.TypeSet,
										Required:    true,
										Description: `The resource name of the Artifact that was created in MetadataStore when creating the Model.`,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"name": {
													Type:     schema.TypeString,
													Required: true,
												},
												"output_tensor_name": {
													Type:        schema.TypeString,
													Required:    true,
													Description: `name of the output tensor.`,
												},
												"display_name_mapping_key": {
													Type:        schema.TypeString,
													Optional:    true,
													Description: `Specify a field name in the prediction to look for the display name.`,
												},
												"index_display_name_mapping": {
													Type:        schema.TypeList,
													Optional:    true,
													Description: `Static mapping between the index and display name.`,
													Elem: &schema.Schema{
														Type: schema.TypeString,
													},
												},
											},
										},
									},
									"latent_space_source": {
										Type:        schema.TypeString,
										Optional:    true,
										Description: `name of the source to generate embeddings for example based explanations.`,
									},
									"feature_attributions_schema_uri": {
										Type:        schema.TypeString,
										Computed:    true,
										Description: `Points to a YAML file stored on Google Cloud Storage describing the format of the feature attributions.`,
									},
								},
							},
						},
					},
				},
			},
			"labels": {
				Type:     schema.TypeMap,
				Optional: true,
				Description: `The labels with user-defined metadata to organize your Models.

**Note**: This field is non-authoritative, and will only manage the labels present in your configuration.
Please refer to the field 'effective_labels' for all of the labels present on the resource.`,
				Elem: &schema.Schema{Type: schema.TypeString},
			},
			"metadata": {
				Type:        schema.TypeList,
				Optional:    true,
				Description: `An additional information about the Index`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"contents_delta_uri": {
							Type:     schema.TypeString,
							Required: true,
							Description: `Allows inserting, updating  or deleting the contents of the Matching Engine Index.
The string must be a valid Cloud Storage directory path. If this
field is set when calling IndexService.UpdateIndex, then no other
Index field can be also updated as part of the same call.
The expected structure and format of the files this URI points to is
described at https://cloud.google.com/vertex-ai/docs/matching-engine/using-matching-engine#input-data-format`,
						},
						"config": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `The configuration of the Matching Engine Index.`,
							MaxItems:    1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"dimensions": {
										Type:        schema.TypeInt,
										Required:    true,
										Description: `The number of dimensions of the input vectors.`,
									},
									"algorithm_config": {
										Type:        schema.TypeList,
										Optional:    true,
										Description: `The configuration with regard to the algorithms used for efficient search.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"brute_force_config": {
													Type:     schema.TypeList,
													Optional: true,
													Description: `Configuration options for using brute force search, which simply implements the
standard linear search in the database for each query.`,
													MaxItems: 1,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{},
													},
													ExactlyOneOf: []string{},
												},
												"tree_ah_config": {
													Type:     schema.TypeList,
													Optional: true,
													Description: `Configuration options for using the tree-AH algorithm (Shallow tree + Asymmetric Hashing).
Please refer to this paper for more details: https://arxiv.org/abs/1908.10396`,
													MaxItems: 1,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"leaf_node_embedding_count": {
																Type:        schema.TypeInt,
																Optional:    true,
																Description: `Number of embeddings on each leaf node. The default value is 1000 if not set.`,
																Default:     1000,
															},
															"leaf_nodes_to_search_percent": {
																Type:     schema.TypeInt,
																Optional: true,
																Description: `The default percentage of leaf nodes that any query may be searched. Must be in
range 1-100, inclusive. The default value is 10 (means 10%) if not set.`,
																Default: 10,
															},
														},
													},
													ExactlyOneOf: []string{},
												},
											},
										},
									},
									"approximate_neighbors_count": {
										Type:     schema.TypeInt,
										Optional: true,
										Description: `The default number of neighbors to find via approximate search before exact reordering is
performed. Exact reordering is a procedure where results returned by an
approximate search algorithm are reordered via a more expensive distance computation.
Required if tree-AH algorithm is used.`,
									},
									"distance_measure_type": {
										Type:     schema.TypeString,
										Optional: true,
										Description: `The distance measure used in nearest neighbor search. The value must be one of the followings:
* SQUARED_L2_DISTANCE: Euclidean (L_2) Distance
* L1_DISTANCE: Manhattan (L_1) Distance
* COSINE_DISTANCE: Cosine Distance. Defined as 1 - cosine similarity.
* DOT_PRODUCT_DISTANCE: Dot Product Distance. Defined as a negative of the dot product`,
										Default: "DOT_PRODUCT_DISTANCE",
									},
									"feature_norm_type": {
										Type:     schema.TypeString,
										Optional: true,
										Description: `Type of normalization to be carried out on each vector. The value must be one of the followings:
* UNIT_L2_NORM: Unit L2 normalization type
* NONE: No normalization type is specified.`,
										Default: "NONE",
									},
									"shard_size": {
										Type:     schema.TypeString,
										Computed: true,
										Optional: true,
										ForceNew: true,
										Description: `Index data is split into equal parts to be processed. These are called "shards".
The shard size must be specified when creating an index. The value must be one of the followings:
* SHARD_SIZE_SMALL: Small (2GB)
* SHARD_SIZE_MEDIUM: Medium (20GB)
* SHARD_SIZE_LARGE: Large (50GB)`,
									},
								},
							},
						},
						"is_complete_overwrite": {
							Type:     schema.TypeBool,
							Optional: true,
							Description: `If this field is set together with contentsDeltaUri when calling IndexService.UpdateIndex,
then existing content of the Index will be replaced by the data from the contentsDeltaUri.`,
							Default: false,
						},
					},
				},
			},
			"metadata_schema_uri": {
				Type:        schema.TypeString,
				Optional:    true,
				ForceNew:    true,
				Description: `Points to a YAML file stored on Google Cloud Storage describing additional information about the Model, that is specific to it.`,
			},
			"model": {
				Type:             schema.TypeString,
				Optional:         true,
				DiffSuppressFunc: tpgresource.CompareSelfLinkOrResourceName,
				Description:      `The Model to create.`,
			},
			"model_id": {
				Type:        schema.TypeString,
				Optional:    true,
				Description: `Copy sourceModel into a new Model with this ID. The ID will become the final component of the model resource name.`,
			},
			"parent_model": {
				Type:        schema.TypeString,
				Optional:    true,
				Description: `Specify this field to copy sourceModel into this existing Model as a new version.`,
			},
			"predict_schemata": {
				Type:        schema.TypeList,
				Optional:    true,
				Description: `The schemata that describe formats of the Model's predictions and explanations as given and returned via PredictionService.Predict and PredictionService.Explain.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"instance_schema_uri": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: `Points to a YAML file stored on Google Cloud Storage describing the format of a single instance, which are used in PredictRequest.instances, ExplainRequest.instances and BatchPredictionJob.input_config.`,
						},
						"parameters_schema_uri": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: `Points to a YAML file stored on Google Cloud Storage describing the parameters of prediction and explanation via PredictRequest.parameters, ExplainRequest.parameters and BatchPredictionJob.model_parameters.`,
						},
						"prediction_schema_uri": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: `Points to a YAML file stored on Google Cloud Storage describing the format of a single prediction produced by this Model, which are returned via PredictResponse.predictions, ExplainResponse.explanations, and BatchPredictionJob.output_config.`,
						},
					},
				},
			},
			"region": {
				Type:        schema.TypeString,
				Computed:    true,
				Optional:    true,
				ForceNew:    true,
				Description: `The region of the Model. eg us-central1`,
			},
			"source_model": {
				Type:         schema.TypeString,
				Optional:     true,
				Description:  `The resource name of the Model to copy. That Model must be in the same Project`,
				AtLeastOneOf: []string{"model", "display_name"},
			},
			"version_aliases": {
				Type:        schema.TypeList,
				Computed:    true,
				Optional:    true,
				Description: `user provided version aliases so that a model version can be referenced via alias.`,
				Elem: &schema.Schema{
					Type: schema.TypeString,
				},
			},
			"create_time": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The timestamp of when the Model was created in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits.`,
			},
			"deployed_models": {
				Type:        schema.TypeList,
				Computed:    true,
				Description: `The pointers to DeployedModels created from this Model. Note that Model could have been deployed to Endpoints in different Locations.`,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"deployed_model_id": {
							Type:        schema.TypeString,
							Computed:    true,
							Description: `An ID of a DeployedModel in the above Endpoint.`,
						},
						"endpoint": {
							Type:        schema.TypeString,
							Computed:    true,
							Description: `A resource name of an Endpoint.`,
						},
					},
				},
			},
			"effective_labels": {
				Type:        schema.TypeMap,
				Computed:    true,
				Description: `All of labels (key/value pairs) present on the resource in GCP, including the labels configured through Terraform, other clients and services.`,
				Elem:        &schema.Schema{Type: schema.TypeString},
			},
			"etag": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `Used to perform consistent read-modify-write updates.`,
			},
			"metadata_artifact": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The resource name of the Artifact that was created in MetadataStore when creating the Model.`,
			},
			"name": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The name of the Model. This value may be up to 60 characters, and valid characters are [a-z0-9_]. The first character cannot be a number.`,
			},
			"pipeline_job": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `This field is populated if the model is produced by a pipeline job.`,
			},
			"supported_deployment_resources_types": {
				Type:        schema.TypeList,
				Computed:    true,
				Description: `When this Model is deployed, its prediction resources are described by the prediction_resources field of the Endpoint.deployed_models object.`,
				Elem: &schema.Schema{
					Type: schema.TypeString,
				},
			},
			"supported_export_formats": {
				Type:        schema.TypeList,
				Computed:    true,
				Description: `The formats in which this Model may be exported. If empty, this Model is not available for export.`,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"exportable_content": {
							Type:        schema.TypeString,
							Computed:    true,
							Description: `The content of this Model that may be exported.`,
						},
						"id": {
							Type:        schema.TypeString,
							Computed:    true,
							Description: `The ID of the export format.`,
						},
					},
				},
			},
			"supported_input_storage_formats": {
				Type:        schema.TypeList,
				Computed:    true,
				Description: `The formats this Model supports in BatchPredictionJob.input_config. If PredictSchemata.instance_schema_uri exists, the instances should be given as per that schema.`,
				Elem: &schema.Schema{
					Type: schema.TypeString,
				},
			},
			"supported_output_storage_formats": {
				Type:        schema.TypeList,
				Computed:    true,
				Description: `The formats this Model supports in BatchPredictionJob.input_config. If PredictSchemata.instance_schema_uri exists, the instances should be given as per that schema.`,
				Elem: &schema.Schema{
					Type: schema.TypeString,
				},
			},
			"terraform_labels": {
				Type:     schema.TypeMap,
				Computed: true,
				Description: `The combination of labels configured directly on the resource
 and default labels configured on the provider.`,
				Elem: &schema.Schema{Type: schema.TypeString},
			},
			"trainer_pipeline": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The resource name of the TrainingPipeline that uploaded this Model, if any.`,
			},
			"update_time": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The timestamp of when the MetadataStore was last updated in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits.`,
			},
			"version_create_time": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The timestamp of when this version was created in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits.`,
			},
			"version_description": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The description of this version`,
			},
			"version_id": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The version ID of the Model.`,
			},
			"version_update_time": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The timestamp of when this version was created in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits.`,
			},
			"project": {
				Type:     schema.TypeString,
				Optional: true,
				Computed: true,
				ForceNew: true,
			},
		},
		UseJSONNumber: true,
	}
}

func resourceVertexAIModelsCreate(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*transport_tpg.Config)
	userAgent, err := tpgresource.GenerateUserAgentString(d, config.UserAgent)
	if err != nil {
		return err
	}

	obj := make(map[string]interface{})
	artifactUriProp, err := expandVertexAIModelsArtifactUri(d.Get("artifact_uri"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("artifact_uri"); !tpgresource.IsEmptyValue(reflect.ValueOf(artifactUriProp)) && (ok || !reflect.DeepEqual(v, artifactUriProp)) {
		obj["artifactUri"] = artifactUriProp
	}
	displayNameProp, err := expandVertexAIModelsDisplayName(d.Get("display_name"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("display_name"); !tpgresource.IsEmptyValue(reflect.ValueOf(displayNameProp)) && (ok || !reflect.DeepEqual(v, displayNameProp)) {
		obj["displayName"] = displayNameProp
	}
	versionAliasesProp, err := expandVertexAIModelsVersionAliases(d.Get("version_aliases"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("version_aliases"); !tpgresource.IsEmptyValue(reflect.ValueOf(versionAliasesProp)) && (ok || !reflect.DeepEqual(v, versionAliasesProp)) {
		obj["versionAliases"] = versionAliasesProp
	}
	sourceModelProp, err := expandVertexAIModelsSourceModel(d.Get("source_model"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("source_model"); !tpgresource.IsEmptyValue(reflect.ValueOf(sourceModelProp)) && (ok || !reflect.DeepEqual(v, sourceModelProp)) {
		obj["sourceModel"] = sourceModelProp
	}
	modelIdProp, err := expandVertexAIModelsModelId(d.Get("model_id"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("model_id"); !tpgresource.IsEmptyValue(reflect.ValueOf(modelIdProp)) && (ok || !reflect.DeepEqual(v, modelIdProp)) {
		obj["modelId"] = modelIdProp
	}
	parentModelProp, err := expandVertexAIModelsParentModel(d.Get("parent_model"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("parent_model"); !tpgresource.IsEmptyValue(reflect.ValueOf(parentModelProp)) && (ok || !reflect.DeepEqual(v, parentModelProp)) {
		obj["parentModel"] = parentModelProp
	}
	descriptionProp, err := expandVertexAIModelsDescription(d.Get("description"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("description"); !tpgresource.IsEmptyValue(reflect.ValueOf(descriptionProp)) && (ok || !reflect.DeepEqual(v, descriptionProp)) {
		obj["description"] = descriptionProp
	}
	metadataSchemaUriProp, err := expandVertexAIModelsMetadataSchemaUri(d.Get("metadata_schema_uri"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("metadata_schema_uri"); !tpgresource.IsEmptyValue(reflect.ValueOf(metadataSchemaUriProp)) && (ok || !reflect.DeepEqual(v, metadataSchemaUriProp)) {
		obj["metadataSchemaUri"] = metadataSchemaUriProp
	}
	predictSchemataProp, err := expandVertexAIModelsPredictSchemata(d.Get("predict_schemata"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("predict_schemata"); !tpgresource.IsEmptyValue(reflect.ValueOf(predictSchemataProp)) && (ok || !reflect.DeepEqual(v, predictSchemataProp)) {
		obj["predictSchemata"] = predictSchemataProp
	}
	containerSpecProp, err := expandVertexAIModelsContainerSpec(d.Get("container_spec"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("container_spec"); !tpgresource.IsEmptyValue(reflect.ValueOf(containerSpecProp)) && (ok || !reflect.DeepEqual(v, containerSpecProp)) {
		obj["containerSpec"] = containerSpecProp
	}
	encryptionSpecProp, err := expandVertexAIModelsEncryptionSpec(d.Get("encryption_spec"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("encryption_spec"); !tpgresource.IsEmptyValue(reflect.ValueOf(encryptionSpecProp)) && (ok || !reflect.DeepEqual(v, encryptionSpecProp)) {
		obj["encryptionSpec"] = encryptionSpecProp
	}
	metadataProp, err := expandVertexAIModelsMetadata(d.Get("metadata"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("metadata"); !tpgresource.IsEmptyValue(reflect.ValueOf(metadataProp)) && (ok || !reflect.DeepEqual(v, metadataProp)) {
		obj["metadata"] = metadataProp
	}
	modelProp, err := expandVertexAIModelsModel(d.Get("model"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("model"); !tpgresource.IsEmptyValue(reflect.ValueOf(modelProp)) && (ok || !reflect.DeepEqual(v, modelProp)) {
		obj["model"] = modelProp
	}
	explanationSpecProp, err := expandVertexAIModelsExplanationSpec(d.Get("explanation_spec"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("explanation_spec"); !tpgresource.IsEmptyValue(reflect.ValueOf(explanationSpecProp)) && (ok || !reflect.DeepEqual(v, explanationSpecProp)) {
		obj["explanationSpec"] = explanationSpecProp
	}
	labelsProp, err := expandVertexAIModelsEffectiveLabels(d.Get("effective_labels"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("effective_labels"); !tpgresource.IsEmptyValue(reflect.ValueOf(labelsProp)) && (ok || !reflect.DeepEqual(v, labelsProp)) {
		obj["labels"] = labelsProp
	}

	obj, err = resourceVertexAIModelsEncoder(d, meta, obj)
	if err != nil {
		return err
	}

	url, err := tpgresource.ReplaceVars(d, config, "{{VertexAIBasePath}}projects/{{project}}/locations/{{region}}/modelsPRE_CREATE_REPLACE_ME")
	if err != nil {
		return err
	}

	log.Printf("[DEBUG] Creating new Models: %#v", obj)
	billingProject := ""

	project, err := tpgresource.GetProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for Models: %s", err)
	}
	billingProject = project

	// err == nil indicates that the billing_project value was found
	if bp, err := tpgresource.GetBillingProject(d, config); err == nil {
		billingProject = bp
	}

	headers := make(http.Header)
	var postRequestType string

	if _, ok := d.GetOk("source_model"); ok {
		postRequestType = ":copy"
	} else {
		postRequestType = ":upload"
	}

	url = regexp.MustCompile("PRE_CREATE_REPLACE_ME").ReplaceAllLiteralString(url, postRequestType)
	res, err := transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
		Config:    config,
		Method:    "POST",
		Project:   billingProject,
		RawURL:    url,
		UserAgent: userAgent,
		Body:      obj,
		Timeout:   d.Timeout(schema.TimeoutCreate),
		Headers:   headers,
	})
	if err != nil {
		return fmt.Errorf("Error creating Models: %s", err)
	}

	// Store the ID now
	id, err := tpgresource.ReplaceVars(d, config, "{{name}}")
	if err != nil {
		return fmt.Errorf("Error constructing id: %s", err)
	}
	d.SetId(id)

	// Use the resource in the operation response to populate
	// identity fields and d.Id() before read
	var opRes map[string]interface{}
	err = VertexAIOperationWaitTimeWithResponse(
		config, res, &opRes, project, "Creating Models", userAgent,
		d.Timeout(schema.TimeoutCreate))
	if err != nil {
		// The resource didn't actually create
		d.SetId("")

		return fmt.Errorf("Error waiting to create Models: %s", err)
	}

	opRes, err = resourceVertexAIModelsDecoder(d, meta, opRes)
	if err != nil {
		return fmt.Errorf("Error decoding response from operation: %s", err)
	}
	if opRes == nil {
		return fmt.Errorf("Error decoding response from operation, could not find object")
	}

	if err := d.Set("name", flattenVertexAIModelsName(opRes["name"], d, config)); err != nil {
		return err
	}

	// This may have caused the ID to update - update it if so.
	id, err = tpgresource.ReplaceVars(d, config, "{{name}}")
	if err != nil {
		return fmt.Errorf("Error constructing id: %s", err)
	}
	d.SetId(id)

	log.Printf("[DEBUG] Finished creating Models %q: %#v", d.Id(), res)

	return resourceVertexAIModelsRead(d, meta)
}

func resourceVertexAIModelsRead(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*transport_tpg.Config)
	userAgent, err := tpgresource.GenerateUserAgentString(d, config.UserAgent)
	if err != nil {
		return err
	}

	url, err := tpgresource.ReplaceVars(d, config, "{{VertexAIBasePath}}{{name}}")
	if err != nil {
		return err
	}

	billingProject := ""

	project, err := tpgresource.GetProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for Models: %s", err)
	}
	billingProject = project

	// err == nil indicates that the billing_project value was found
	if bp, err := tpgresource.GetBillingProject(d, config); err == nil {
		billingProject = bp
	}

	headers := make(http.Header)
	res, err := transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
		Config:    config,
		Method:    "GET",
		Project:   billingProject,
		RawURL:    url,
		UserAgent: userAgent,
		Headers:   headers,
	})
	if err != nil {
		return transport_tpg.HandleNotFoundError(err, d, fmt.Sprintf("VertexAIModels %q", d.Id()))
	}

	res, err = resourceVertexAIModelsDecoder(d, meta, res)
	if err != nil {
		return err
	}

	if res == nil {
		// Decoding the object has resulted in it being gone. It may be marked deleted
		log.Printf("[DEBUG] Removing VertexAIModels because it no longer exists.")
		d.SetId("")
		return nil
	}

	if err := d.Set("project", project); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}

	if err := d.Set("supported_deployment_resources_types", flattenVertexAIModelsSupportedDeploymentResourcesTypes(res["supportedDeploymentResourcesTypes"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("supported_input_storage_formats", flattenVertexAIModelsSupportedInputStorageFormats(res["supportedInputStorageFormats"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("supported_output_storage_formats", flattenVertexAIModelsSupportedOutputStorageFormats(res["supportedOutputStorageFormats"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("metadata_artifact", flattenVertexAIModelsMetadataArtifact(res["metadataArtifact"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("deployed_models", flattenVertexAIModelsDeployedModels(res["deployedModels"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("supported_export_formats", flattenVertexAIModelsSupportedExportFormats(res["supportedExportFormats"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("trainer_pipeline", flattenVertexAIModelsTrainerPipeline(res["trainerPipeline"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("version_id", flattenVertexAIModelsVersionId(res["versionId"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("version_create_time", flattenVertexAIModelsVersionCreateTime(res["versionCreateTime"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("version_update_time", flattenVertexAIModelsVersionUpdateTime(res["versionUpdateTime"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("version_description", flattenVertexAIModelsVersionDescription(res["versionDescription"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("create_time", flattenVertexAIModelsCreateTime(res["createTime"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("update_time", flattenVertexAIModelsUpdateTime(res["updateTime"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("name", flattenVertexAIModelsName(res["name"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("pipeline_job", flattenVertexAIModelsPipelineJob(res["pipelineJob"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("artifact_uri", flattenVertexAIModelsArtifactUri(res["artifactUri"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("display_name", flattenVertexAIModelsDisplayName(res["displayName"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("version_aliases", flattenVertexAIModelsVersionAliases(res["versionAliases"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("labels", flattenVertexAIModelsLabels(res["labels"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("source_model", flattenVertexAIModelsSourceModel(res["sourceModel"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("parent_model", flattenVertexAIModelsParentModel(res["parentModel"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("description", flattenVertexAIModelsDescription(res["description"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("metadata_schema_uri", flattenVertexAIModelsMetadataSchemaUri(res["metadataSchemaUri"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("predict_schemata", flattenVertexAIModelsPredictSchemata(res["predictSchemata"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("container_spec", flattenVertexAIModelsContainerSpec(res["containerSpec"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("encryption_spec", flattenVertexAIModelsEncryptionSpec(res["encryptionSpec"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("metadata", flattenVertexAIModelsMetadata(res["metadata"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("model", flattenVertexAIModelsModel(res["model"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("explanation_spec", flattenVertexAIModelsExplanationSpec(res["explanationSpec"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("terraform_labels", flattenVertexAIModelsTerraformLabels(res["labels"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("effective_labels", flattenVertexAIModelsEffectiveLabels(res["labels"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}

	return nil
}

func resourceVertexAIModelsUpdate(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*transport_tpg.Config)
	userAgent, err := tpgresource.GenerateUserAgentString(d, config.UserAgent)
	if err != nil {
		return err
	}

	billingProject := ""

	project, err := tpgresource.GetProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for Models: %s", err)
	}
	billingProject = project

	obj := make(map[string]interface{})
	artifactUriProp, err := expandVertexAIModelsArtifactUri(d.Get("artifact_uri"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("artifact_uri"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, artifactUriProp)) {
		obj["artifactUri"] = artifactUriProp
	}
	displayNameProp, err := expandVertexAIModelsDisplayName(d.Get("display_name"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("display_name"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, displayNameProp)) {
		obj["displayName"] = displayNameProp
	}
	versionAliasesProp, err := expandVertexAIModelsVersionAliases(d.Get("version_aliases"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("version_aliases"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, versionAliasesProp)) {
		obj["versionAliases"] = versionAliasesProp
	}
	sourceModelProp, err := expandVertexAIModelsSourceModel(d.Get("source_model"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("source_model"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, sourceModelProp)) {
		obj["sourceModel"] = sourceModelProp
	}
	modelIdProp, err := expandVertexAIModelsModelId(d.Get("model_id"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("model_id"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, modelIdProp)) {
		obj["modelId"] = modelIdProp
	}
	parentModelProp, err := expandVertexAIModelsParentModel(d.Get("parent_model"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("parent_model"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, parentModelProp)) {
		obj["parentModel"] = parentModelProp
	}
	descriptionProp, err := expandVertexAIModelsDescription(d.Get("description"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("description"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, descriptionProp)) {
		obj["description"] = descriptionProp
	}
	metadataSchemaUriProp, err := expandVertexAIModelsMetadataSchemaUri(d.Get("metadata_schema_uri"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("metadata_schema_uri"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, metadataSchemaUriProp)) {
		obj["metadataSchemaUri"] = metadataSchemaUriProp
	}
	predictSchemataProp, err := expandVertexAIModelsPredictSchemata(d.Get("predict_schemata"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("predict_schemata"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, predictSchemataProp)) {
		obj["predictSchemata"] = predictSchemataProp
	}
	containerSpecProp, err := expandVertexAIModelsContainerSpec(d.Get("container_spec"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("container_spec"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, containerSpecProp)) {
		obj["containerSpec"] = containerSpecProp
	}
	encryptionSpecProp, err := expandVertexAIModelsEncryptionSpec(d.Get("encryption_spec"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("encryption_spec"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, encryptionSpecProp)) {
		obj["encryptionSpec"] = encryptionSpecProp
	}
	metadataProp, err := expandVertexAIModelsMetadata(d.Get("metadata"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("metadata"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, metadataProp)) {
		obj["metadata"] = metadataProp
	}
	modelProp, err := expandVertexAIModelsModel(d.Get("model"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("model"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, modelProp)) {
		obj["model"] = modelProp
	}
	explanationSpecProp, err := expandVertexAIModelsExplanationSpec(d.Get("explanation_spec"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("explanation_spec"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, explanationSpecProp)) {
		obj["explanationSpec"] = explanationSpecProp
	}
	labelsProp, err := expandVertexAIModelsEffectiveLabels(d.Get("effective_labels"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("effective_labels"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, labelsProp)) {
		obj["labels"] = labelsProp
	}

	obj, err = resourceVertexAIModelsEncoder(d, meta, obj)
	if err != nil {
		return err
	}

	url, err := tpgresource.ReplaceVars(d, config, "{{VertexAIBasePath}}{{name}}")
	if err != nil {
		return err
	}

	log.Printf("[DEBUG] Updating Models %q: %#v", d.Id(), obj)
	headers := make(http.Header)

	// err == nil indicates that the billing_project value was found
	if bp, err := tpgresource.GetBillingProject(d, config); err == nil {
		billingProject = bp
	}

	res, err := transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
		Config:    config,
		Method:    "PUT",
		Project:   billingProject,
		RawURL:    url,
		UserAgent: userAgent,
		Body:      obj,
		Timeout:   d.Timeout(schema.TimeoutUpdate),
		Headers:   headers,
	})

	if err != nil {
		return fmt.Errorf("Error updating Models %q: %s", d.Id(), err)
	} else {
		log.Printf("[DEBUG] Finished updating Models %q: %#v", d.Id(), res)
	}

	return resourceVertexAIModelsRead(d, meta)
}

func resourceVertexAIModelsDelete(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*transport_tpg.Config)
	userAgent, err := tpgresource.GenerateUserAgentString(d, config.UserAgent)
	if err != nil {
		return err
	}

	billingProject := ""

	project, err := tpgresource.GetProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for Models: %s", err)
	}
	billingProject = project

	url, err := tpgresource.ReplaceVars(d, config, "{{VertexAIBasePath}}{{name}}")
	if err != nil {
		return err
	}

	var obj map[string]interface{}

	// err == nil indicates that the billing_project value was found
	if bp, err := tpgresource.GetBillingProject(d, config); err == nil {
		billingProject = bp
	}

	headers := make(http.Header)

	log.Printf("[DEBUG] Deleting Models %q", d.Id())
	res, err := transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
		Config:    config,
		Method:    "DELETE",
		Project:   billingProject,
		RawURL:    url,
		UserAgent: userAgent,
		Body:      obj,
		Timeout:   d.Timeout(schema.TimeoutDelete),
		Headers:   headers,
	})
	if err != nil {
		return transport_tpg.HandleNotFoundError(err, d, "Models")
	}

	err = VertexAIOperationWaitTime(
		config, res, project, "Deleting Models", userAgent,
		d.Timeout(schema.TimeoutDelete))

	if err != nil {
		return err
	}

	log.Printf("[DEBUG] Finished deleting Models %q: %#v", d.Id(), res)
	return nil
}

func flattenVertexAIModelsSupportedDeploymentResourcesTypes(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsSupportedInputStorageFormats(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsSupportedOutputStorageFormats(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsMetadataArtifact(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsDeployedModels(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"endpoint":          flattenVertexAIModelsDeployedModelsEndpoint(original["endpoint"], d, config),
			"deployed_model_id": flattenVertexAIModelsDeployedModelsDeployedModelId(original["deployedModelId"], d, config),
		})
	}
	return transformed
}
func flattenVertexAIModelsDeployedModelsEndpoint(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsDeployedModelsDeployedModelId(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsSupportedExportFormats(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"id":                 flattenVertexAIModelsSupportedExportFormatsId(original["id"], d, config),
			"exportable_content": flattenVertexAIModelsSupportedExportFormatsExportableContent(original["exportableContent"], d, config),
		})
	}
	return transformed
}
func flattenVertexAIModelsSupportedExportFormatsId(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsSupportedExportFormatsExportableContent(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsTrainerPipeline(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsVersionId(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsVersionCreateTime(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsVersionUpdateTime(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsVersionDescription(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsCreateTime(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsUpdateTime(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if d.Get("name").(string) == "" {
		return v.(string)
	}

	return d.Get("name")
}

func flattenVertexAIModelsPipelineJob(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsArtifactUri(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsDisplayName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsVersionAliases(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsLabels(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}

	transformed := make(map[string]interface{})
	if l, ok := d.GetOkExists("labels"); ok {
		for k := range l.(map[string]interface{}) {
			transformed[k] = v.(map[string]interface{})[k]
		}
	}

	return transformed
}

func flattenVertexAIModelsSourceModel(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsParentModel(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsDescription(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsMetadataSchemaUri(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsPredictSchemata(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["instance_schema_uri"] =
		flattenVertexAIModelsPredictSchemataInstanceSchemaUri(original["instanceSchemaUri"], d, config)
	transformed["parameters_schema_uri"] =
		flattenVertexAIModelsPredictSchemataParametersSchemaUri(original["parametersSchemaUri"], d, config)
	transformed["prediction_schema_uri"] =
		flattenVertexAIModelsPredictSchemataPredictionSchemaUri(original["predictionSchemaUri"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsPredictSchemataInstanceSchemaUri(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsPredictSchemataParametersSchemaUri(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsPredictSchemataPredictionSchemaUri(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsContainerSpec(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["image_uri"] =
		flattenVertexAIModelsContainerSpecImageUri(original["imageUri"], d, config)
	transformed["command"] =
		flattenVertexAIModelsContainerSpecCommand(original["command"], d, config)
	transformed["args"] =
		flattenVertexAIModelsContainerSpecArgs(original["args"], d, config)
	transformed["env"] =
		flattenVertexAIModelsContainerSpecEnv(original["env"], d, config)
	transformed["ports"] =
		flattenVertexAIModelsContainerSpecPorts(original["ports"], d, config)
	transformed["predict_route"] =
		flattenVertexAIModelsContainerSpecPredictRoute(original["predictRoute"], d, config)
	transformed["health_route"] =
		flattenVertexAIModelsContainerSpecHealthRoute(original["healthRoute"], d, config)
	transformed["grpc_ports"] =
		flattenVertexAIModelsContainerSpecGrpcPorts(original["grpcPorts"], d, config)
	transformed["deployment_timeout"] =
		flattenVertexAIModelsContainerSpecDeploymentTimeout(original["deploymentTimeout"], d, config)
	transformed["shared_memory_size_mb"] =
		flattenVertexAIModelsContainerSpecSharedMemorySizeMb(original["sharedMemorySizeMb"], d, config)
	transformed["startup_probe"] =
		flattenVertexAIModelsContainerSpecStartupProbe(original["startupProbe"], d, config)
	transformed["health_probe"] =
		flattenVertexAIModelsContainerSpecHealthProbe(original["healthProbe"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsContainerSpecImageUri(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsContainerSpecCommand(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsContainerSpecArgs(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsContainerSpecEnv(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"name":  flattenVertexAIModelsContainerSpecEnvName(original["name"], d, config),
			"value": flattenVertexAIModelsContainerSpecEnvValue(original["value"], d, config),
		})
	}
	return transformed
}
func flattenVertexAIModelsContainerSpecEnvName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsContainerSpecEnvValue(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsContainerSpecPorts(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"container_port": flattenVertexAIModelsContainerSpecPortsContainerPort(original["containerPort"], d, config),
		})
	}
	return transformed
}
func flattenVertexAIModelsContainerSpecPortsContainerPort(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIModelsContainerSpecPredictRoute(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsContainerSpecHealthRoute(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsContainerSpecGrpcPorts(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"container_port": flattenVertexAIModelsContainerSpecGrpcPortsContainerPort(original["containerPort"], d, config),
		})
	}
	return transformed
}
func flattenVertexAIModelsContainerSpecGrpcPortsContainerPort(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIModelsContainerSpecDeploymentTimeout(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsContainerSpecSharedMemorySizeMb(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIModelsContainerSpecStartupProbe(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["period_seconds"] =
		flattenVertexAIModelsContainerSpecStartupProbePeriodSeconds(original["periodSeconds"], d, config)
	transformed["timeout_seconds"] =
		flattenVertexAIModelsContainerSpecStartupProbeTimeoutSeconds(original["timeoutSeconds"], d, config)
	transformed["exec"] =
		flattenVertexAIModelsContainerSpecStartupProbeExec(original["exec"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsContainerSpecStartupProbePeriodSeconds(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIModelsContainerSpecStartupProbeTimeoutSeconds(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIModelsContainerSpecStartupProbeExec(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["command"] =
		flattenVertexAIModelsContainerSpecStartupProbeExecCommand(original["command"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsContainerSpecStartupProbeExecCommand(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsContainerSpecHealthProbe(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["period_seconds"] =
		flattenVertexAIModelsContainerSpecHealthProbePeriodSeconds(original["periodSeconds"], d, config)
	transformed["timeout_seconds"] =
		flattenVertexAIModelsContainerSpecHealthProbeTimeoutSeconds(original["timeoutSeconds"], d, config)
	transformed["exec"] =
		flattenVertexAIModelsContainerSpecHealthProbeExec(original["exec"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsContainerSpecHealthProbePeriodSeconds(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIModelsContainerSpecHealthProbeTimeoutSeconds(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIModelsContainerSpecHealthProbeExec(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["command"] =
		flattenVertexAIModelsContainerSpecHealthProbeExecCommand(original["command"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsContainerSpecHealthProbeExecCommand(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsEncryptionSpec(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["kms_key_name"] =
		flattenVertexAIModelsEncryptionSpecKmsKeyName(original["kmsKeyName"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsEncryptionSpecKmsKeyName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsMetadata(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["contents_delta_uri"] =
		flattenVertexAIModelsMetadataContentsDeltaUri(original["contentsDeltaUri"], d, config)
	transformed["is_complete_overwrite"] =
		flattenVertexAIModelsMetadataIsCompleteOverwrite(original["isCompleteOverwrite"], d, config)
	transformed["config"] =
		flattenVertexAIModelsMetadataConfig(original["config"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsMetadataContentsDeltaUri(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// We want to ignore read on this field, but cannot because it is nested
	return d.Get("metadata.0.contents_delta_uri")
}

func flattenVertexAIModelsMetadataIsCompleteOverwrite(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// We want to ignore read on this field, but cannot because it is nested
	return d.Get("metadata.0.is_complete_overwrite")
}

func flattenVertexAIModelsMetadataConfig(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["dimensions"] =
		flattenVertexAIModelsMetadataConfigDimensions(original["dimensions"], d, config)
	transformed["approximate_neighbors_count"] =
		flattenVertexAIModelsMetadataConfigApproximateNeighborsCount(original["approximateNeighborsCount"], d, config)
	transformed["shard_size"] =
		flattenVertexAIModelsMetadataConfigShardSize(original["shardSize"], d, config)
	transformed["distance_measure_type"] =
		flattenVertexAIModelsMetadataConfigDistanceMeasureType(original["distanceMeasureType"], d, config)
	transformed["feature_norm_type"] =
		flattenVertexAIModelsMetadataConfigFeatureNormType(original["featureNormType"], d, config)
	transformed["algorithm_config"] =
		flattenVertexAIModelsMetadataConfigAlgorithmConfig(original["algorithmConfig"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsMetadataConfigDimensions(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIModelsMetadataConfigApproximateNeighborsCount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIModelsMetadataConfigShardSize(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsMetadataConfigDistanceMeasureType(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsMetadataConfigFeatureNormType(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return d.Get("metadata.0.config.0.feature_norm_type")
}

func flattenVertexAIModelsMetadataConfigAlgorithmConfig(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["tree_ah_config"] =
		flattenVertexAIModelsMetadataConfigAlgorithmConfigTreeAhConfig(original["treeAhConfig"], d, config)
	transformed["brute_force_config"] =
		flattenVertexAIModelsMetadataConfigAlgorithmConfigBruteForceConfig(original["bruteForceConfig"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsMetadataConfigAlgorithmConfigTreeAhConfig(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["leaf_node_embedding_count"] =
		flattenVertexAIModelsMetadataConfigAlgorithmConfigTreeAhConfigLeafNodeEmbeddingCount(original["leafNodeEmbeddingCount"], d, config)
	transformed["leaf_nodes_to_search_percent"] =
		flattenVertexAIModelsMetadataConfigAlgorithmConfigTreeAhConfigLeafNodesToSearchPercent(original["leafNodesToSearchPercent"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsMetadataConfigAlgorithmConfigTreeAhConfigLeafNodeEmbeddingCount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIModelsMetadataConfigAlgorithmConfigTreeAhConfigLeafNodesToSearchPercent(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIModelsMetadataConfigAlgorithmConfigBruteForceConfig(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	transformed := make(map[string]interface{})
	return []interface{}{transformed}
}

func flattenVertexAIModelsModel(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	return tpgresource.ConvertSelfLinkToV1(v.(string))
}

func flattenVertexAIModelsExplanationSpec(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["parameters"] =
		flattenVertexAIModelsExplanationSpecParameters(original["parameters"], d, config)
	transformed["metadata"] =
		flattenVertexAIModelsExplanationSpecMetadata(original["metadata"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsExplanationSpecParameters(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["top_k"] =
		flattenVertexAIModelsExplanationSpecParametersTopK(original["topK"], d, config)
	transformed["output_indices"] =
		flattenVertexAIModelsExplanationSpecParametersOutputIndices(original["outputIndices"], d, config)
	transformed["sampled_shapley_attribution"] =
		flattenVertexAIModelsExplanationSpecParametersSampledShapleyAttribution(original["sampledShapleyAttribution"], d, config)
	transformed["integrated_gradients_attribution"] =
		flattenVertexAIModelsExplanationSpecParametersIntegratedGradientsAttribution(original["integratedGradientsAttribution"], d, config)
	transformed["xrai_attribution"] =
		flattenVertexAIModelsExplanationSpecParametersXraiAttribution(original["xraiAttribution"], d, config)
	transformed["examples"] =
		flattenVertexAIModelsExplanationSpecParametersExamples(original["examples"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsExplanationSpecParametersTopK(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIModelsExplanationSpecParametersOutputIndices(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecParametersSampledShapleyAttribution(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["path_count"] =
		flattenVertexAIModelsExplanationSpecParametersSampledShapleyAttributionPathCount(original["pathCount"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsExplanationSpecParametersSampledShapleyAttributionPathCount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIModelsExplanationSpecParametersIntegratedGradientsAttribution(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["step_count"] =
		flattenVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionStepCount(original["stepCount"], d, config)
	transformed["smooth_grad_config"] =
		flattenVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionSmoothGradConfig(original["smoothGradConfig"], d, config)
	transformed["blur_baseline_config"] =
		flattenVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionBlurBaselineConfig(original["blurBaselineConfig"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionStepCount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionSmoothGradConfig(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["noisy_sample_count"] =
		flattenVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionSmoothGradConfigNoisySampleCount(original["noisySampleCount"], d, config)
	transformed["noise_sigma"] =
		flattenVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionSmoothGradConfigNoiseSigma(original["noiseSigma"], d, config)
	transformed["feature_noise_sigma"] =
		flattenVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionSmoothGradConfigFeatureNoiseSigma(original["featureNoiseSigma"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionSmoothGradConfigNoisySampleCount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionSmoothGradConfigNoiseSigma(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionSmoothGradConfigFeatureNoiseSigma(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["noise_sigma"] =
		flattenVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionSmoothGradConfigFeatureNoiseSigmaNoiseSigma(original["noiseSigma"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionSmoothGradConfigFeatureNoiseSigmaNoiseSigma(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"name":  flattenVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionSmoothGradConfigFeatureNoiseSigmaNoiseSigmaName(original["name"], d, config),
			"sigma": flattenVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionSmoothGradConfigFeatureNoiseSigmaNoiseSigmaSigma(original["sigma"], d, config),
		})
	}
	return transformed
}
func flattenVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionSmoothGradConfigFeatureNoiseSigmaNoiseSigmaName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionSmoothGradConfigFeatureNoiseSigmaNoiseSigmaSigma(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionBlurBaselineConfig(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["max_blur_sigma"] =
		flattenVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionBlurBaselineConfigMaxBlurSigma(original["maxBlurSigma"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionBlurBaselineConfigMaxBlurSigma(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecParametersXraiAttribution(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["step_count"] =
		flattenVertexAIModelsExplanationSpecParametersXraiAttributionStepCount(original["stepCount"], d, config)
	transformed["smooth_grad_config"] =
		flattenVertexAIModelsExplanationSpecParametersXraiAttributionSmoothGradConfig(original["smoothGradConfig"], d, config)
	transformed["blur_baseline_config"] =
		flattenVertexAIModelsExplanationSpecParametersXraiAttributionBlurBaselineConfig(original["blurBaselineConfig"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsExplanationSpecParametersXraiAttributionStepCount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIModelsExplanationSpecParametersXraiAttributionSmoothGradConfig(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["noisy_sample_count"] =
		flattenVertexAIModelsExplanationSpecParametersXraiAttributionSmoothGradConfigNoisySampleCount(original["noisySampleCount"], d, config)
	transformed["noise_sigma"] =
		flattenVertexAIModelsExplanationSpecParametersXraiAttributionSmoothGradConfigNoiseSigma(original["noiseSigma"], d, config)
	transformed["feature_noise_sigma"] =
		flattenVertexAIModelsExplanationSpecParametersXraiAttributionSmoothGradConfigFeatureNoiseSigma(original["featureNoiseSigma"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsExplanationSpecParametersXraiAttributionSmoothGradConfigNoisySampleCount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIModelsExplanationSpecParametersXraiAttributionSmoothGradConfigNoiseSigma(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecParametersXraiAttributionSmoothGradConfigFeatureNoiseSigma(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["noise_sigma"] =
		flattenVertexAIModelsExplanationSpecParametersXraiAttributionSmoothGradConfigFeatureNoiseSigmaNoiseSigma(original["noiseSigma"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsExplanationSpecParametersXraiAttributionSmoothGradConfigFeatureNoiseSigmaNoiseSigma(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"name":  flattenVertexAIModelsExplanationSpecParametersXraiAttributionSmoothGradConfigFeatureNoiseSigmaNoiseSigmaName(original["name"], d, config),
			"sigma": flattenVertexAIModelsExplanationSpecParametersXraiAttributionSmoothGradConfigFeatureNoiseSigmaNoiseSigmaSigma(original["sigma"], d, config),
		})
	}
	return transformed
}
func flattenVertexAIModelsExplanationSpecParametersXraiAttributionSmoothGradConfigFeatureNoiseSigmaNoiseSigmaName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecParametersXraiAttributionSmoothGradConfigFeatureNoiseSigmaNoiseSigmaSigma(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecParametersXraiAttributionBlurBaselineConfig(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["max_blur_sigma"] =
		flattenVertexAIModelsExplanationSpecParametersXraiAttributionBlurBaselineConfigMaxBlurSigma(original["maxBlurSigma"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsExplanationSpecParametersXraiAttributionBlurBaselineConfigMaxBlurSigma(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecParametersExamples(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["neighbor_count"] =
		flattenVertexAIModelsExplanationSpecParametersExamplesNeighborCount(original["neighborCount"], d, config)
	transformed["example_gcs_source"] =
		flattenVertexAIModelsExplanationSpecParametersExamplesExampleGcsSource(original["exampleGcsSource"], d, config)
	transformed["nearest_neighbor_search_config"] =
		flattenVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfig(original["nearestNeighborSearchConfig"], d, config)
	transformed["presets"] =
		flattenVertexAIModelsExplanationSpecParametersExamplesPresets(original["presets"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsExplanationSpecParametersExamplesNeighborCount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIModelsExplanationSpecParametersExamplesExampleGcsSource(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["data_format"] =
		flattenVertexAIModelsExplanationSpecParametersExamplesExampleGcsSourceDataFormat(original["dataFormat"], d, config)
	transformed["gcs_source"] =
		flattenVertexAIModelsExplanationSpecParametersExamplesExampleGcsSourceGcsSource(original["gcsSource"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsExplanationSpecParametersExamplesExampleGcsSourceDataFormat(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecParametersExamplesExampleGcsSourceGcsSource(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["uris"] =
		flattenVertexAIModelsExplanationSpecParametersExamplesExampleGcsSourceGcsSourceUris(original["uris"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsExplanationSpecParametersExamplesExampleGcsSourceGcsSourceUris(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfig(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["contents_delta_uri"] =
		flattenVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigContentsDeltaUri(original["contentsDeltaUri"], d, config)
	transformed["is_complete_overwrite"] =
		flattenVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigIsCompleteOverwrite(original["isCompleteOverwrite"], d, config)
	transformed["config"] =
		flattenVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfig(original["config"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigContentsDeltaUri(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// We want to ignore read on this field, but cannot because it is nested
	return d.Get("metadata.0.contents_delta_uri")
}

func flattenVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigIsCompleteOverwrite(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// We want to ignore read on this field, but cannot because it is nested
	return d.Get("metadata.0.is_complete_overwrite")
}

func flattenVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfig(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["dimensions"] =
		flattenVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigDimensions(original["dimensions"], d, config)
	transformed["approximate_neighbors_count"] =
		flattenVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigApproximateNeighborsCount(original["approximateNeighborsCount"], d, config)
	transformed["shard_size"] =
		flattenVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigShardSize(original["shardSize"], d, config)
	transformed["distance_measure_type"] =
		flattenVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigDistanceMeasureType(original["distanceMeasureType"], d, config)
	transformed["feature_norm_type"] =
		flattenVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigFeatureNormType(original["featureNormType"], d, config)
	transformed["algorithm_config"] =
		flattenVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigAlgorithmConfig(original["algorithmConfig"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigDimensions(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigApproximateNeighborsCount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigShardSize(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigDistanceMeasureType(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigFeatureNormType(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return d.Get("explanation_spec.0.parameters.0.examples.0.nearest_neighbor_search_config.0.config.0.feature_norm_type")
}

func flattenVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigAlgorithmConfig(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["tree_ah_config"] =
		flattenVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigAlgorithmConfigTreeAhConfig(original["treeAhConfig"], d, config)
	transformed["brute_force_config"] =
		flattenVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigAlgorithmConfigBruteForceConfig(original["bruteForceConfig"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigAlgorithmConfigTreeAhConfig(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["leaf_node_embedding_count"] =
		flattenVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigAlgorithmConfigTreeAhConfigLeafNodeEmbeddingCount(original["leafNodeEmbeddingCount"], d, config)
	transformed["leaf_nodes_to_search_percent"] =
		flattenVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigAlgorithmConfigTreeAhConfigLeafNodesToSearchPercent(original["leafNodesToSearchPercent"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigAlgorithmConfigTreeAhConfigLeafNodeEmbeddingCount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigAlgorithmConfigTreeAhConfigLeafNodesToSearchPercent(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigAlgorithmConfigBruteForceConfig(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	transformed := make(map[string]interface{})
	return []interface{}{transformed}
}

func flattenVertexAIModelsExplanationSpecParametersExamplesPresets(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["modality"] =
		flattenVertexAIModelsExplanationSpecParametersExamplesPresetsModality(original["modality"], d, config)
	transformed["query"] =
		flattenVertexAIModelsExplanationSpecParametersExamplesPresetsQuery(original["query"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsExplanationSpecParametersExamplesPresetsModality(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecParametersExamplesPresetsQuery(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecMetadata(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["inputs"] =
		flattenVertexAIModelsExplanationSpecMetadataInputs(original["inputs"], d, config)
	transformed["outputs"] =
		flattenVertexAIModelsExplanationSpecMetadataOutputs(original["outputs"], d, config)
	transformed["feature_attributions_schema_uri"] =
		flattenVertexAIModelsExplanationSpecMetadataFeatureAttributionsSchemaUri(original["featureAttributionsSchemaUri"], d, config)
	transformed["latent_space_source"] =
		flattenVertexAIModelsExplanationSpecMetadataLatentSpaceSource(original["latentSpaceSource"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsExplanationSpecMetadataInputs(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.(map[string]interface{})
	transformed := make([]interface{}, 0, len(l))
	for k, raw := range l {
		original := raw.(map[string]interface{})
		transformed = append(transformed, map[string]interface{}{
			"name":                    k,
			"input_tensor_name":       flattenVertexAIModelsExplanationSpecMetadataInputsInputTensorName(original["inputTensorName"], d, config),
			"encoding":                flattenVertexAIModelsExplanationSpecMetadataInputsEncoding(original["encoding"], d, config),
			"modality":                flattenVertexAIModelsExplanationSpecMetadataInputsModality(original["modality"], d, config),
			"feature_value_domain":    flattenVertexAIModelsExplanationSpecMetadataInputsFeatureValueDomain(original["featureValueDomain"], d, config),
			"indices_tensor_name":     flattenVertexAIModelsExplanationSpecMetadataInputsIndicesTensorName(original["indicesTensorName"], d, config),
			"dense_shape_tensor_name": flattenVertexAIModelsExplanationSpecMetadataInputsDenseShapeTensorName(original["denseShapeTensorName"], d, config),
			"index_feature_mapping":   flattenVertexAIModelsExplanationSpecMetadataInputsIndexFeatureMapping(original["indexFeatureMapping"], d, config),
			"encoded_tensor_name":     flattenVertexAIModelsExplanationSpecMetadataInputsEncodedTensorName(original["encodedTensorName"], d, config),
			"encoded_baselines":       flattenVertexAIModelsExplanationSpecMetadataInputsEncodedBaselines(original["encodedBaselines"], d, config),
			"visualization":           flattenVertexAIModelsExplanationSpecMetadataInputsVisualization(original["visualization"], d, config),
			"group_name":              flattenVertexAIModelsExplanationSpecMetadataInputsGroupName(original["groupName"], d, config),
		})
	}
	return transformed
}
func flattenVertexAIModelsExplanationSpecMetadataInputsInputTensorName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecMetadataInputsEncoding(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecMetadataInputsModality(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecMetadataInputsFeatureValueDomain(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["min_value"] =
		flattenVertexAIModelsExplanationSpecMetadataInputsFeatureValueDomainMinValue(original["minValue"], d, config)
	transformed["max_value"] =
		flattenVertexAIModelsExplanationSpecMetadataInputsFeatureValueDomainMaxValue(original["maxValue"], d, config)
	transformed["original_mean"] =
		flattenVertexAIModelsExplanationSpecMetadataInputsFeatureValueDomainOriginalMean(original["originalMean"], d, config)
	transformed["original_stddev"] =
		flattenVertexAIModelsExplanationSpecMetadataInputsFeatureValueDomainOriginalStddev(original["originalStddev"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsExplanationSpecMetadataInputsFeatureValueDomainMinValue(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecMetadataInputsFeatureValueDomainMaxValue(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecMetadataInputsFeatureValueDomainOriginalMean(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecMetadataInputsFeatureValueDomainOriginalStddev(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecMetadataInputsIndicesTensorName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecMetadataInputsDenseShapeTensorName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecMetadataInputsIndexFeatureMapping(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecMetadataInputsEncodedTensorName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecMetadataInputsEncodedBaselines(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecMetadataInputsVisualization(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["type"] =
		flattenVertexAIModelsExplanationSpecMetadataInputsVisualizationType(original["type"], d, config)
	transformed["polarity"] =
		flattenVertexAIModelsExplanationSpecMetadataInputsVisualizationPolarity(original["polarity"], d, config)
	transformed["color_map"] =
		flattenVertexAIModelsExplanationSpecMetadataInputsVisualizationColorMap(original["colorMap"], d, config)
	transformed["clip_percent_upperbound"] =
		flattenVertexAIModelsExplanationSpecMetadataInputsVisualizationClipPercentUpperbound(original["clipPercentUpperbound"], d, config)
	transformed["clip_percent_lowerbound"] =
		flattenVertexAIModelsExplanationSpecMetadataInputsVisualizationClipPercentLowerbound(original["clipPercentLowerbound"], d, config)
	transformed["overlay_type"] =
		flattenVertexAIModelsExplanationSpecMetadataInputsVisualizationOverlayType(original["overlayType"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsExplanationSpecMetadataInputsVisualizationType(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecMetadataInputsVisualizationPolarity(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecMetadataInputsVisualizationColorMap(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecMetadataInputsVisualizationClipPercentUpperbound(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecMetadataInputsVisualizationClipPercentLowerbound(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIModelsExplanationSpecMetadataInputsVisualizationOverlayType(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecMetadataInputsGroupName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecMetadataOutputs(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.(map[string]interface{})
	transformed := make([]interface{}, 0, len(l))
	for k, raw := range l {
		original := raw.(map[string]interface{})
		transformed = append(transformed, map[string]interface{}{
			"name":                       k,
			"output_tensor_name":         flattenVertexAIModelsExplanationSpecMetadataOutputsOutputTensorName(original["outputTensorName"], d, config),
			"index_display_name_mapping": flattenVertexAIModelsExplanationSpecMetadataOutputsIndexDisplayNameMapping(original["indexDisplayNameMapping"], d, config),
			"display_name_mapping_key":   flattenVertexAIModelsExplanationSpecMetadataOutputsDisplayNameMappingKey(original["displayNameMappingKey"], d, config),
		})
	}
	return transformed
}
func flattenVertexAIModelsExplanationSpecMetadataOutputsOutputTensorName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecMetadataOutputsIndexDisplayNameMapping(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecMetadataOutputsDisplayNameMappingKey(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecMetadataFeatureAttributionsSchemaUri(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsExplanationSpecMetadataLatentSpaceSource(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsTerraformLabels(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}

	transformed := make(map[string]interface{})
	if l, ok := d.GetOkExists("terraform_labels"); ok {
		for k := range l.(map[string]interface{}) {
			transformed[k] = v.(map[string]interface{})[k]
		}
	}

	return transformed
}

func flattenVertexAIModelsEffectiveLabels(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func expandVertexAIModelsArtifactUri(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsDisplayName(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsVersionAliases(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsSourceModel(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsModelId(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsParentModel(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsDescription(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsMetadataSchemaUri(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsPredictSchemata(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedInstanceSchemaUri, err := expandVertexAIModelsPredictSchemataInstanceSchemaUri(original["instance_schema_uri"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedInstanceSchemaUri); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["instanceSchemaUri"] = transformedInstanceSchemaUri
	}

	transformedParametersSchemaUri, err := expandVertexAIModelsPredictSchemataParametersSchemaUri(original["parameters_schema_uri"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedParametersSchemaUri); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["parametersSchemaUri"] = transformedParametersSchemaUri
	}

	transformedPredictionSchemaUri, err := expandVertexAIModelsPredictSchemataPredictionSchemaUri(original["prediction_schema_uri"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPredictionSchemaUri); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["predictionSchemaUri"] = transformedPredictionSchemaUri
	}

	return transformed, nil
}

func expandVertexAIModelsPredictSchemataInstanceSchemaUri(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsPredictSchemataParametersSchemaUri(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsPredictSchemataPredictionSchemaUri(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsContainerSpec(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedImageUri, err := expandVertexAIModelsContainerSpecImageUri(original["image_uri"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedImageUri); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["imageUri"] = transformedImageUri
	}

	transformedCommand, err := expandVertexAIModelsContainerSpecCommand(original["command"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedCommand); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["command"] = transformedCommand
	}

	transformedArgs, err := expandVertexAIModelsContainerSpecArgs(original["args"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedArgs); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["args"] = transformedArgs
	}

	transformedEnv, err := expandVertexAIModelsContainerSpecEnv(original["env"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedEnv); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["env"] = transformedEnv
	}

	transformedPorts, err := expandVertexAIModelsContainerSpecPorts(original["ports"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPorts); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["ports"] = transformedPorts
	}

	transformedPredictRoute, err := expandVertexAIModelsContainerSpecPredictRoute(original["predict_route"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPredictRoute); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["predictRoute"] = transformedPredictRoute
	}

	transformedHealthRoute, err := expandVertexAIModelsContainerSpecHealthRoute(original["health_route"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedHealthRoute); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["healthRoute"] = transformedHealthRoute
	}

	transformedGrpcPorts, err := expandVertexAIModelsContainerSpecGrpcPorts(original["grpc_ports"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedGrpcPorts); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["grpcPorts"] = transformedGrpcPorts
	}

	transformedDeploymentTimeout, err := expandVertexAIModelsContainerSpecDeploymentTimeout(original["deployment_timeout"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedDeploymentTimeout); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["deploymentTimeout"] = transformedDeploymentTimeout
	}

	transformedSharedMemorySizeMb, err := expandVertexAIModelsContainerSpecSharedMemorySizeMb(original["shared_memory_size_mb"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedSharedMemorySizeMb); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["sharedMemorySizeMb"] = transformedSharedMemorySizeMb
	}

	transformedStartupProbe, err := expandVertexAIModelsContainerSpecStartupProbe(original["startup_probe"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedStartupProbe); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["startupProbe"] = transformedStartupProbe
	}

	transformedHealthProbe, err := expandVertexAIModelsContainerSpecHealthProbe(original["health_probe"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedHealthProbe); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["healthProbe"] = transformedHealthProbe
	}

	return transformed, nil
}

func expandVertexAIModelsContainerSpecImageUri(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsContainerSpecCommand(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsContainerSpecArgs(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsContainerSpecEnv(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	req := make([]interface{}, 0, len(l))
	for _, raw := range l {
		if raw == nil {
			continue
		}
		original := raw.(map[string]interface{})
		transformed := make(map[string]interface{})

		transformedName, err := expandVertexAIModelsContainerSpecEnvName(original["name"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedName); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["name"] = transformedName
		}

		transformedValue, err := expandVertexAIModelsContainerSpecEnvValue(original["value"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedValue); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["value"] = transformedValue
		}

		req = append(req, transformed)
	}
	return req, nil
}

func expandVertexAIModelsContainerSpecEnvName(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsContainerSpecEnvValue(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsContainerSpecPorts(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	req := make([]interface{}, 0, len(l))
	for _, raw := range l {
		if raw == nil {
			continue
		}
		original := raw.(map[string]interface{})
		transformed := make(map[string]interface{})

		transformedContainerPort, err := expandVertexAIModelsContainerSpecPortsContainerPort(original["container_port"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedContainerPort); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["containerPort"] = transformedContainerPort
		}

		req = append(req, transformed)
	}
	return req, nil
}

func expandVertexAIModelsContainerSpecPortsContainerPort(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsContainerSpecPredictRoute(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsContainerSpecHealthRoute(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsContainerSpecGrpcPorts(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	req := make([]interface{}, 0, len(l))
	for _, raw := range l {
		if raw == nil {
			continue
		}
		original := raw.(map[string]interface{})
		transformed := make(map[string]interface{})

		transformedContainerPort, err := expandVertexAIModelsContainerSpecGrpcPortsContainerPort(original["container_port"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedContainerPort); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["containerPort"] = transformedContainerPort
		}

		req = append(req, transformed)
	}
	return req, nil
}

func expandVertexAIModelsContainerSpecGrpcPortsContainerPort(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsContainerSpecDeploymentTimeout(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsContainerSpecSharedMemorySizeMb(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsContainerSpecStartupProbe(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedPeriodSeconds, err := expandVertexAIModelsContainerSpecStartupProbePeriodSeconds(original["period_seconds"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPeriodSeconds); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["periodSeconds"] = transformedPeriodSeconds
	}

	transformedTimeoutSeconds, err := expandVertexAIModelsContainerSpecStartupProbeTimeoutSeconds(original["timeout_seconds"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedTimeoutSeconds); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["timeoutSeconds"] = transformedTimeoutSeconds
	}

	transformedExec, err := expandVertexAIModelsContainerSpecStartupProbeExec(original["exec"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedExec); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["exec"] = transformedExec
	}

	return transformed, nil
}

func expandVertexAIModelsContainerSpecStartupProbePeriodSeconds(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsContainerSpecStartupProbeTimeoutSeconds(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsContainerSpecStartupProbeExec(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedCommand, err := expandVertexAIModelsContainerSpecStartupProbeExecCommand(original["command"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedCommand); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["command"] = transformedCommand
	}

	return transformed, nil
}

func expandVertexAIModelsContainerSpecStartupProbeExecCommand(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsContainerSpecHealthProbe(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedPeriodSeconds, err := expandVertexAIModelsContainerSpecHealthProbePeriodSeconds(original["period_seconds"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPeriodSeconds); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["periodSeconds"] = transformedPeriodSeconds
	}

	transformedTimeoutSeconds, err := expandVertexAIModelsContainerSpecHealthProbeTimeoutSeconds(original["timeout_seconds"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedTimeoutSeconds); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["timeoutSeconds"] = transformedTimeoutSeconds
	}

	transformedExec, err := expandVertexAIModelsContainerSpecHealthProbeExec(original["exec"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedExec); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["exec"] = transformedExec
	}

	return transformed, nil
}

func expandVertexAIModelsContainerSpecHealthProbePeriodSeconds(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsContainerSpecHealthProbeTimeoutSeconds(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsContainerSpecHealthProbeExec(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedCommand, err := expandVertexAIModelsContainerSpecHealthProbeExecCommand(original["command"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedCommand); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["command"] = transformedCommand
	}

	return transformed, nil
}

func expandVertexAIModelsContainerSpecHealthProbeExecCommand(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsEncryptionSpec(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedKmsKeyName, err := expandVertexAIModelsEncryptionSpecKmsKeyName(original["kms_key_name"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedKmsKeyName); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["kmsKeyName"] = transformedKmsKeyName
	}

	return transformed, nil
}

func expandVertexAIModelsEncryptionSpecKmsKeyName(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsMetadata(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedContentsDeltaUri, err := expandVertexAIModelsMetadataContentsDeltaUri(original["contents_delta_uri"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedContentsDeltaUri); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["contentsDeltaUri"] = transformedContentsDeltaUri
	}

	transformedIsCompleteOverwrite, err := expandVertexAIModelsMetadataIsCompleteOverwrite(original["is_complete_overwrite"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedIsCompleteOverwrite); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["isCompleteOverwrite"] = transformedIsCompleteOverwrite
	}

	transformedConfig, err := expandVertexAIModelsMetadataConfig(original["config"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedConfig); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["config"] = transformedConfig
	}

	return transformed, nil
}

func expandVertexAIModelsMetadataContentsDeltaUri(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsMetadataIsCompleteOverwrite(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsMetadataConfig(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedDimensions, err := expandVertexAIModelsMetadataConfigDimensions(original["dimensions"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedDimensions); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["dimensions"] = transformedDimensions
	}

	transformedApproximateNeighborsCount, err := expandVertexAIModelsMetadataConfigApproximateNeighborsCount(original["approximate_neighbors_count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedApproximateNeighborsCount); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["approximateNeighborsCount"] = transformedApproximateNeighborsCount
	}

	transformedShardSize, err := expandVertexAIModelsMetadataConfigShardSize(original["shard_size"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedShardSize); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["shardSize"] = transformedShardSize
	}

	transformedDistanceMeasureType, err := expandVertexAIModelsMetadataConfigDistanceMeasureType(original["distance_measure_type"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedDistanceMeasureType); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["distanceMeasureType"] = transformedDistanceMeasureType
	}

	transformedFeatureNormType, err := expandVertexAIModelsMetadataConfigFeatureNormType(original["feature_norm_type"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedFeatureNormType); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["featureNormType"] = transformedFeatureNormType
	}

	transformedAlgorithmConfig, err := expandVertexAIModelsMetadataConfigAlgorithmConfig(original["algorithm_config"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedAlgorithmConfig); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["algorithmConfig"] = transformedAlgorithmConfig
	}

	return transformed, nil
}

func expandVertexAIModelsMetadataConfigDimensions(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsMetadataConfigApproximateNeighborsCount(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsMetadataConfigShardSize(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsMetadataConfigDistanceMeasureType(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsMetadataConfigFeatureNormType(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsMetadataConfigAlgorithmConfig(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedTreeAhConfig, err := expandVertexAIModelsMetadataConfigAlgorithmConfigTreeAhConfig(original["tree_ah_config"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedTreeAhConfig); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["treeAhConfig"] = transformedTreeAhConfig
	}

	transformedBruteForceConfig, err := expandVertexAIModelsMetadataConfigAlgorithmConfigBruteForceConfig(original["brute_force_config"], d, config)
	if err != nil {
		return nil, err
	} else {
		transformed["bruteForceConfig"] = transformedBruteForceConfig
	}

	return transformed, nil
}

func expandVertexAIModelsMetadataConfigAlgorithmConfigTreeAhConfig(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedLeafNodeEmbeddingCount, err := expandVertexAIModelsMetadataConfigAlgorithmConfigTreeAhConfigLeafNodeEmbeddingCount(original["leaf_node_embedding_count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedLeafNodeEmbeddingCount); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["leafNodeEmbeddingCount"] = transformedLeafNodeEmbeddingCount
	}

	transformedLeafNodesToSearchPercent, err := expandVertexAIModelsMetadataConfigAlgorithmConfigTreeAhConfigLeafNodesToSearchPercent(original["leaf_nodes_to_search_percent"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedLeafNodesToSearchPercent); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["leafNodesToSearchPercent"] = transformedLeafNodesToSearchPercent
	}

	return transformed, nil
}

func expandVertexAIModelsMetadataConfigAlgorithmConfigTreeAhConfigLeafNodeEmbeddingCount(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsMetadataConfigAlgorithmConfigTreeAhConfigLeafNodesToSearchPercent(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsMetadataConfigAlgorithmConfigBruteForceConfig(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 {
		return nil, nil
	}

	if l[0] == nil {
		transformed := make(map[string]interface{})
		return transformed, nil
	}
	transformed := make(map[string]interface{})

	return transformed, nil
}

func expandVertexAIModelsModel(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	f, err := tpgresource.ParseRegionalFieldValue("models", v.(string), "project", "region", "zone", d, config, true)
	if err != nil {
		return nil, fmt.Errorf("Invalid value for model: %s", err)
	}
	return f.RelativeLink(), nil
}

func expandVertexAIModelsExplanationSpec(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedParameters, err := expandVertexAIModelsExplanationSpecParameters(original["parameters"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedParameters); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["parameters"] = transformedParameters
	}

	transformedMetadata, err := expandVertexAIModelsExplanationSpecMetadata(original["metadata"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMetadata); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["metadata"] = transformedMetadata
	}

	return transformed, nil
}

func expandVertexAIModelsExplanationSpecParameters(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedTopK, err := expandVertexAIModelsExplanationSpecParametersTopK(original["top_k"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedTopK); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["topK"] = transformedTopK
	}

	transformedOutputIndices, err := expandVertexAIModelsExplanationSpecParametersOutputIndices(original["output_indices"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedOutputIndices); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["outputIndices"] = transformedOutputIndices
	}

	transformedSampledShapleyAttribution, err := expandVertexAIModelsExplanationSpecParametersSampledShapleyAttribution(original["sampled_shapley_attribution"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedSampledShapleyAttribution); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["sampledShapleyAttribution"] = transformedSampledShapleyAttribution
	}

	transformedIntegratedGradientsAttribution, err := expandVertexAIModelsExplanationSpecParametersIntegratedGradientsAttribution(original["integrated_gradients_attribution"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedIntegratedGradientsAttribution); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["integratedGradientsAttribution"] = transformedIntegratedGradientsAttribution
	}

	transformedXraiAttribution, err := expandVertexAIModelsExplanationSpecParametersXraiAttribution(original["xrai_attribution"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedXraiAttribution); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["xraiAttribution"] = transformedXraiAttribution
	}

	transformedExamples, err := expandVertexAIModelsExplanationSpecParametersExamples(original["examples"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedExamples); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["examples"] = transformedExamples
	}

	return transformed, nil
}

func expandVertexAIModelsExplanationSpecParametersTopK(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecParametersOutputIndices(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecParametersSampledShapleyAttribution(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedPathCount, err := expandVertexAIModelsExplanationSpecParametersSampledShapleyAttributionPathCount(original["path_count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPathCount); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["pathCount"] = transformedPathCount
	}

	return transformed, nil
}

func expandVertexAIModelsExplanationSpecParametersSampledShapleyAttributionPathCount(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecParametersIntegratedGradientsAttribution(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedStepCount, err := expandVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionStepCount(original["step_count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedStepCount); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["stepCount"] = transformedStepCount
	}

	transformedSmoothGradConfig, err := expandVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionSmoothGradConfig(original["smooth_grad_config"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedSmoothGradConfig); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["smoothGradConfig"] = transformedSmoothGradConfig
	}

	transformedBlurBaselineConfig, err := expandVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionBlurBaselineConfig(original["blur_baseline_config"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedBlurBaselineConfig); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["blurBaselineConfig"] = transformedBlurBaselineConfig
	}

	return transformed, nil
}

func expandVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionStepCount(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionSmoothGradConfig(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedNoisySampleCount, err := expandVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionSmoothGradConfigNoisySampleCount(original["noisy_sample_count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedNoisySampleCount); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["noisySampleCount"] = transformedNoisySampleCount
	}

	transformedNoiseSigma, err := expandVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionSmoothGradConfigNoiseSigma(original["noise_sigma"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedNoiseSigma); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["noiseSigma"] = transformedNoiseSigma
	}

	transformedFeatureNoiseSigma, err := expandVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionSmoothGradConfigFeatureNoiseSigma(original["feature_noise_sigma"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedFeatureNoiseSigma); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["featureNoiseSigma"] = transformedFeatureNoiseSigma
	}

	return transformed, nil
}

func expandVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionSmoothGradConfigNoisySampleCount(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionSmoothGradConfigNoiseSigma(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionSmoothGradConfigFeatureNoiseSigma(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedNoiseSigma, err := expandVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionSmoothGradConfigFeatureNoiseSigmaNoiseSigma(original["noise_sigma"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedNoiseSigma); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["noiseSigma"] = transformedNoiseSigma
	}

	return transformed, nil
}

func expandVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionSmoothGradConfigFeatureNoiseSigmaNoiseSigma(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	req := make([]interface{}, 0, len(l))
	for _, raw := range l {
		if raw == nil {
			continue
		}
		original := raw.(map[string]interface{})
		transformed := make(map[string]interface{})

		transformedName, err := expandVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionSmoothGradConfigFeatureNoiseSigmaNoiseSigmaName(original["name"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedName); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["name"] = transformedName
		}

		transformedSigma, err := expandVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionSmoothGradConfigFeatureNoiseSigmaNoiseSigmaSigma(original["sigma"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedSigma); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["sigma"] = transformedSigma
		}

		req = append(req, transformed)
	}
	return req, nil
}

func expandVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionSmoothGradConfigFeatureNoiseSigmaNoiseSigmaName(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionSmoothGradConfigFeatureNoiseSigmaNoiseSigmaSigma(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionBlurBaselineConfig(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedMaxBlurSigma, err := expandVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionBlurBaselineConfigMaxBlurSigma(original["max_blur_sigma"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMaxBlurSigma); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["maxBlurSigma"] = transformedMaxBlurSigma
	}

	return transformed, nil
}

func expandVertexAIModelsExplanationSpecParametersIntegratedGradientsAttributionBlurBaselineConfigMaxBlurSigma(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecParametersXraiAttribution(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedStepCount, err := expandVertexAIModelsExplanationSpecParametersXraiAttributionStepCount(original["step_count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedStepCount); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["stepCount"] = transformedStepCount
	}

	transformedSmoothGradConfig, err := expandVertexAIModelsExplanationSpecParametersXraiAttributionSmoothGradConfig(original["smooth_grad_config"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedSmoothGradConfig); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["smoothGradConfig"] = transformedSmoothGradConfig
	}

	transformedBlurBaselineConfig, err := expandVertexAIModelsExplanationSpecParametersXraiAttributionBlurBaselineConfig(original["blur_baseline_config"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedBlurBaselineConfig); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["blurBaselineConfig"] = transformedBlurBaselineConfig
	}

	return transformed, nil
}

func expandVertexAIModelsExplanationSpecParametersXraiAttributionStepCount(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecParametersXraiAttributionSmoothGradConfig(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedNoisySampleCount, err := expandVertexAIModelsExplanationSpecParametersXraiAttributionSmoothGradConfigNoisySampleCount(original["noisy_sample_count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedNoisySampleCount); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["noisySampleCount"] = transformedNoisySampleCount
	}

	transformedNoiseSigma, err := expandVertexAIModelsExplanationSpecParametersXraiAttributionSmoothGradConfigNoiseSigma(original["noise_sigma"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedNoiseSigma); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["noiseSigma"] = transformedNoiseSigma
	}

	transformedFeatureNoiseSigma, err := expandVertexAIModelsExplanationSpecParametersXraiAttributionSmoothGradConfigFeatureNoiseSigma(original["feature_noise_sigma"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedFeatureNoiseSigma); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["featureNoiseSigma"] = transformedFeatureNoiseSigma
	}

	return transformed, nil
}

func expandVertexAIModelsExplanationSpecParametersXraiAttributionSmoothGradConfigNoisySampleCount(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecParametersXraiAttributionSmoothGradConfigNoiseSigma(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecParametersXraiAttributionSmoothGradConfigFeatureNoiseSigma(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedNoiseSigma, err := expandVertexAIModelsExplanationSpecParametersXraiAttributionSmoothGradConfigFeatureNoiseSigmaNoiseSigma(original["noise_sigma"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedNoiseSigma); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["noiseSigma"] = transformedNoiseSigma
	}

	return transformed, nil
}

func expandVertexAIModelsExplanationSpecParametersXraiAttributionSmoothGradConfigFeatureNoiseSigmaNoiseSigma(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	req := make([]interface{}, 0, len(l))
	for _, raw := range l {
		if raw == nil {
			continue
		}
		original := raw.(map[string]interface{})
		transformed := make(map[string]interface{})

		transformedName, err := expandVertexAIModelsExplanationSpecParametersXraiAttributionSmoothGradConfigFeatureNoiseSigmaNoiseSigmaName(original["name"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedName); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["name"] = transformedName
		}

		transformedSigma, err := expandVertexAIModelsExplanationSpecParametersXraiAttributionSmoothGradConfigFeatureNoiseSigmaNoiseSigmaSigma(original["sigma"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedSigma); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["sigma"] = transformedSigma
		}

		req = append(req, transformed)
	}
	return req, nil
}

func expandVertexAIModelsExplanationSpecParametersXraiAttributionSmoothGradConfigFeatureNoiseSigmaNoiseSigmaName(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecParametersXraiAttributionSmoothGradConfigFeatureNoiseSigmaNoiseSigmaSigma(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecParametersXraiAttributionBlurBaselineConfig(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedMaxBlurSigma, err := expandVertexAIModelsExplanationSpecParametersXraiAttributionBlurBaselineConfigMaxBlurSigma(original["max_blur_sigma"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMaxBlurSigma); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["maxBlurSigma"] = transformedMaxBlurSigma
	}

	return transformed, nil
}

func expandVertexAIModelsExplanationSpecParametersXraiAttributionBlurBaselineConfigMaxBlurSigma(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecParametersExamples(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedNeighborCount, err := expandVertexAIModelsExplanationSpecParametersExamplesNeighborCount(original["neighbor_count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedNeighborCount); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["neighborCount"] = transformedNeighborCount
	}

	transformedExampleGcsSource, err := expandVertexAIModelsExplanationSpecParametersExamplesExampleGcsSource(original["example_gcs_source"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedExampleGcsSource); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["exampleGcsSource"] = transformedExampleGcsSource
	}

	transformedNearestNeighborSearchConfig, err := expandVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfig(original["nearest_neighbor_search_config"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedNearestNeighborSearchConfig); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["nearestNeighborSearchConfig"] = transformedNearestNeighborSearchConfig
	}

	transformedPresets, err := expandVertexAIModelsExplanationSpecParametersExamplesPresets(original["presets"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPresets); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["presets"] = transformedPresets
	}

	return transformed, nil
}

func expandVertexAIModelsExplanationSpecParametersExamplesNeighborCount(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecParametersExamplesExampleGcsSource(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedDataFormat, err := expandVertexAIModelsExplanationSpecParametersExamplesExampleGcsSourceDataFormat(original["data_format"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedDataFormat); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["dataFormat"] = transformedDataFormat
	}

	transformedGcsSource, err := expandVertexAIModelsExplanationSpecParametersExamplesExampleGcsSourceGcsSource(original["gcs_source"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedGcsSource); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["gcsSource"] = transformedGcsSource
	}

	return transformed, nil
}

func expandVertexAIModelsExplanationSpecParametersExamplesExampleGcsSourceDataFormat(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecParametersExamplesExampleGcsSourceGcsSource(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedUris, err := expandVertexAIModelsExplanationSpecParametersExamplesExampleGcsSourceGcsSourceUris(original["uris"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedUris); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["uris"] = transformedUris
	}

	return transformed, nil
}

func expandVertexAIModelsExplanationSpecParametersExamplesExampleGcsSourceGcsSourceUris(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfig(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedContentsDeltaUri, err := expandVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigContentsDeltaUri(original["contents_delta_uri"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedContentsDeltaUri); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["contentsDeltaUri"] = transformedContentsDeltaUri
	}

	transformedIsCompleteOverwrite, err := expandVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigIsCompleteOverwrite(original["is_complete_overwrite"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedIsCompleteOverwrite); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["isCompleteOverwrite"] = transformedIsCompleteOverwrite
	}

	transformedConfig, err := expandVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfig(original["config"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedConfig); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["config"] = transformedConfig
	}

	return transformed, nil
}

func expandVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigContentsDeltaUri(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigIsCompleteOverwrite(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfig(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedDimensions, err := expandVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigDimensions(original["dimensions"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedDimensions); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["dimensions"] = transformedDimensions
	}

	transformedApproximateNeighborsCount, err := expandVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigApproximateNeighborsCount(original["approximate_neighbors_count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedApproximateNeighborsCount); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["approximateNeighborsCount"] = transformedApproximateNeighborsCount
	}

	transformedShardSize, err := expandVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigShardSize(original["shard_size"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedShardSize); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["shardSize"] = transformedShardSize
	}

	transformedDistanceMeasureType, err := expandVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigDistanceMeasureType(original["distance_measure_type"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedDistanceMeasureType); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["distanceMeasureType"] = transformedDistanceMeasureType
	}

	transformedFeatureNormType, err := expandVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigFeatureNormType(original["feature_norm_type"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedFeatureNormType); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["featureNormType"] = transformedFeatureNormType
	}

	transformedAlgorithmConfig, err := expandVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigAlgorithmConfig(original["algorithm_config"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedAlgorithmConfig); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["algorithmConfig"] = transformedAlgorithmConfig
	}

	return transformed, nil
}

func expandVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigDimensions(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigApproximateNeighborsCount(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigShardSize(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigDistanceMeasureType(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigFeatureNormType(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigAlgorithmConfig(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedTreeAhConfig, err := expandVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigAlgorithmConfigTreeAhConfig(original["tree_ah_config"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedTreeAhConfig); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["treeAhConfig"] = transformedTreeAhConfig
	}

	transformedBruteForceConfig, err := expandVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigAlgorithmConfigBruteForceConfig(original["brute_force_config"], d, config)
	if err != nil {
		return nil, err
	} else {
		transformed["bruteForceConfig"] = transformedBruteForceConfig
	}

	return transformed, nil
}

func expandVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigAlgorithmConfigTreeAhConfig(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedLeafNodeEmbeddingCount, err := expandVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigAlgorithmConfigTreeAhConfigLeafNodeEmbeddingCount(original["leaf_node_embedding_count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedLeafNodeEmbeddingCount); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["leafNodeEmbeddingCount"] = transformedLeafNodeEmbeddingCount
	}

	transformedLeafNodesToSearchPercent, err := expandVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigAlgorithmConfigTreeAhConfigLeafNodesToSearchPercent(original["leaf_nodes_to_search_percent"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedLeafNodesToSearchPercent); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["leafNodesToSearchPercent"] = transformedLeafNodesToSearchPercent
	}

	return transformed, nil
}

func expandVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigAlgorithmConfigTreeAhConfigLeafNodeEmbeddingCount(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigAlgorithmConfigTreeAhConfigLeafNodesToSearchPercent(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecParametersExamplesNearestNeighborSearchConfigConfigAlgorithmConfigBruteForceConfig(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 {
		return nil, nil
	}

	if l[0] == nil {
		transformed := make(map[string]interface{})
		return transformed, nil
	}
	transformed := make(map[string]interface{})

	return transformed, nil
}

func expandVertexAIModelsExplanationSpecParametersExamplesPresets(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedModality, err := expandVertexAIModelsExplanationSpecParametersExamplesPresetsModality(original["modality"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedModality); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["modality"] = transformedModality
	}

	transformedQuery, err := expandVertexAIModelsExplanationSpecParametersExamplesPresetsQuery(original["query"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedQuery); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["query"] = transformedQuery
	}

	return transformed, nil
}

func expandVertexAIModelsExplanationSpecParametersExamplesPresetsModality(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecParametersExamplesPresetsQuery(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecMetadata(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedInputs, err := expandVertexAIModelsExplanationSpecMetadataInputs(original["inputs"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedInputs); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["inputs"] = transformedInputs
	}

	transformedOutputs, err := expandVertexAIModelsExplanationSpecMetadataOutputs(original["outputs"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedOutputs); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["outputs"] = transformedOutputs
	}

	transformedFeatureAttributionsSchemaUri, err := expandVertexAIModelsExplanationSpecMetadataFeatureAttributionsSchemaUri(original["feature_attributions_schema_uri"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedFeatureAttributionsSchemaUri); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["featureAttributionsSchemaUri"] = transformedFeatureAttributionsSchemaUri
	}

	transformedLatentSpaceSource, err := expandVertexAIModelsExplanationSpecMetadataLatentSpaceSource(original["latent_space_source"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedLatentSpaceSource); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["latentSpaceSource"] = transformedLatentSpaceSource
	}

	return transformed, nil
}

func expandVertexAIModelsExplanationSpecMetadataInputs(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (map[string]interface{}, error) {
	if v == nil {
		return map[string]interface{}{}, nil
	}
	m := make(map[string]interface{})
	for _, raw := range v.(*schema.Set).List() {
		original := raw.(map[string]interface{})
		transformed := make(map[string]interface{})

		transformedInputTensorName, err := expandVertexAIModelsExplanationSpecMetadataInputsInputTensorName(original["input_tensor_name"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedInputTensorName); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["inputTensorName"] = transformedInputTensorName
		}

		transformedEncoding, err := expandVertexAIModelsExplanationSpecMetadataInputsEncoding(original["encoding"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedEncoding); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["encoding"] = transformedEncoding
		}

		transformedModality, err := expandVertexAIModelsExplanationSpecMetadataInputsModality(original["modality"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedModality); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["modality"] = transformedModality
		}

		transformedFeatureValueDomain, err := expandVertexAIModelsExplanationSpecMetadataInputsFeatureValueDomain(original["feature_value_domain"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedFeatureValueDomain); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["featureValueDomain"] = transformedFeatureValueDomain
		}

		transformedIndicesTensorName, err := expandVertexAIModelsExplanationSpecMetadataInputsIndicesTensorName(original["indices_tensor_name"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedIndicesTensorName); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["indicesTensorName"] = transformedIndicesTensorName
		}

		transformedDenseShapeTensorName, err := expandVertexAIModelsExplanationSpecMetadataInputsDenseShapeTensorName(original["dense_shape_tensor_name"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedDenseShapeTensorName); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["denseShapeTensorName"] = transformedDenseShapeTensorName
		}

		transformedIndexFeatureMapping, err := expandVertexAIModelsExplanationSpecMetadataInputsIndexFeatureMapping(original["index_feature_mapping"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedIndexFeatureMapping); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["indexFeatureMapping"] = transformedIndexFeatureMapping
		}

		transformedEncodedTensorName, err := expandVertexAIModelsExplanationSpecMetadataInputsEncodedTensorName(original["encoded_tensor_name"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedEncodedTensorName); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["encodedTensorName"] = transformedEncodedTensorName
		}

		transformedEncodedBaselines, err := expandVertexAIModelsExplanationSpecMetadataInputsEncodedBaselines(original["encoded_baselines"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedEncodedBaselines); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["encodedBaselines"] = transformedEncodedBaselines
		}

		transformedVisualization, err := expandVertexAIModelsExplanationSpecMetadataInputsVisualization(original["visualization"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedVisualization); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["visualization"] = transformedVisualization
		}

		transformedGroupName, err := expandVertexAIModelsExplanationSpecMetadataInputsGroupName(original["group_name"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedGroupName); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["groupName"] = transformedGroupName
		}

		transformedName, err := tpgresource.ExpandString(original["name"], d, config)
		if err != nil {
			return nil, err
		}
		m[transformedName] = transformed
	}
	return m, nil
}

func expandVertexAIModelsExplanationSpecMetadataInputsInputTensorName(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecMetadataInputsEncoding(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecMetadataInputsModality(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecMetadataInputsFeatureValueDomain(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedMinValue, err := expandVertexAIModelsExplanationSpecMetadataInputsFeatureValueDomainMinValue(original["min_value"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMinValue); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["minValue"] = transformedMinValue
	}

	transformedMaxValue, err := expandVertexAIModelsExplanationSpecMetadataInputsFeatureValueDomainMaxValue(original["max_value"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedMaxValue); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["maxValue"] = transformedMaxValue
	}

	transformedOriginalMean, err := expandVertexAIModelsExplanationSpecMetadataInputsFeatureValueDomainOriginalMean(original["original_mean"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedOriginalMean); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["originalMean"] = transformedOriginalMean
	}

	transformedOriginalStddev, err := expandVertexAIModelsExplanationSpecMetadataInputsFeatureValueDomainOriginalStddev(original["original_stddev"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedOriginalStddev); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["originalStddev"] = transformedOriginalStddev
	}

	return transformed, nil
}

func expandVertexAIModelsExplanationSpecMetadataInputsFeatureValueDomainMinValue(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecMetadataInputsFeatureValueDomainMaxValue(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecMetadataInputsFeatureValueDomainOriginalMean(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecMetadataInputsFeatureValueDomainOriginalStddev(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecMetadataInputsIndicesTensorName(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecMetadataInputsDenseShapeTensorName(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecMetadataInputsIndexFeatureMapping(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecMetadataInputsEncodedTensorName(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecMetadataInputsEncodedBaselines(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecMetadataInputsVisualization(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedType, err := expandVertexAIModelsExplanationSpecMetadataInputsVisualizationType(original["type"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedType); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["type"] = transformedType
	}

	transformedPolarity, err := expandVertexAIModelsExplanationSpecMetadataInputsVisualizationPolarity(original["polarity"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPolarity); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["polarity"] = transformedPolarity
	}

	transformedColorMap, err := expandVertexAIModelsExplanationSpecMetadataInputsVisualizationColorMap(original["color_map"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedColorMap); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["colorMap"] = transformedColorMap
	}

	transformedClipPercentUpperbound, err := expandVertexAIModelsExplanationSpecMetadataInputsVisualizationClipPercentUpperbound(original["clip_percent_upperbound"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedClipPercentUpperbound); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["clipPercentUpperbound"] = transformedClipPercentUpperbound
	}

	transformedClipPercentLowerbound, err := expandVertexAIModelsExplanationSpecMetadataInputsVisualizationClipPercentLowerbound(original["clip_percent_lowerbound"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedClipPercentLowerbound); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["clipPercentLowerbound"] = transformedClipPercentLowerbound
	}

	transformedOverlayType, err := expandVertexAIModelsExplanationSpecMetadataInputsVisualizationOverlayType(original["overlay_type"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedOverlayType); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["overlayType"] = transformedOverlayType
	}

	return transformed, nil
}

func expandVertexAIModelsExplanationSpecMetadataInputsVisualizationType(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecMetadataInputsVisualizationPolarity(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecMetadataInputsVisualizationColorMap(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecMetadataInputsVisualizationClipPercentUpperbound(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecMetadataInputsVisualizationClipPercentLowerbound(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecMetadataInputsVisualizationOverlayType(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecMetadataInputsGroupName(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecMetadataOutputs(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (map[string]interface{}, error) {
	if v == nil {
		return map[string]interface{}{}, nil
	}
	m := make(map[string]interface{})
	for _, raw := range v.(*schema.Set).List() {
		original := raw.(map[string]interface{})
		transformed := make(map[string]interface{})

		transformedOutputTensorName, err := expandVertexAIModelsExplanationSpecMetadataOutputsOutputTensorName(original["output_tensor_name"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedOutputTensorName); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["outputTensorName"] = transformedOutputTensorName
		}

		transformedIndexDisplayNameMapping, err := expandVertexAIModelsExplanationSpecMetadataOutputsIndexDisplayNameMapping(original["index_display_name_mapping"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedIndexDisplayNameMapping); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["indexDisplayNameMapping"] = transformedIndexDisplayNameMapping
		}

		transformedDisplayNameMappingKey, err := expandVertexAIModelsExplanationSpecMetadataOutputsDisplayNameMappingKey(original["display_name_mapping_key"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedDisplayNameMappingKey); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["displayNameMappingKey"] = transformedDisplayNameMappingKey
		}

		transformedName, err := tpgresource.ExpandString(original["name"], d, config)
		if err != nil {
			return nil, err
		}
		m[transformedName] = transformed
	}
	return m, nil
}

func expandVertexAIModelsExplanationSpecMetadataOutputsOutputTensorName(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecMetadataOutputsIndexDisplayNameMapping(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecMetadataOutputsDisplayNameMappingKey(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecMetadataFeatureAttributionsSchemaUri(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsExplanationSpecMetadataLatentSpaceSource(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsEffectiveLabels(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (map[string]string, error) {
	if v == nil {
		return map[string]string{}, nil
	}
	m := make(map[string]string)
	for k, val := range v.(map[string]interface{}) {
		m[k] = val.(string)
	}
	return m, nil
}

func resourceVertexAIModelsEncoder(d *schema.ResourceData, meta interface{}, obj map[string]interface{}) (map[string]interface{}, error) {
	newObj := make(map[string]interface{})
	name := ""
	if v, ok := d.GetOk("model_id"); ok {
		name = v.(string)
		delete(obj, "modelId")
	} else {
		name = d.Get("name").(string)
	}
	newObj["modelId"] = name
	if v, ok := d.GetOk("parent_model"); ok {
		newObj["parentModel"] = v
		delete(obj, "parentModel")
	}
	newObj["model"] = obj
	return newObj, nil
}

func resourceVertexAIModelsDecoder(d *schema.ResourceData, meta interface{}, res map[string]interface{}) (map[string]interface{}, error) {
	if res["model"] != "" {
		res["name"] = res["model"]
	}

	return res, nil
}
