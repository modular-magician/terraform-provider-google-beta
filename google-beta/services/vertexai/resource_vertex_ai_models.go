// Copyright (c) HashiCorp, Inc.
// SPDX-License-Identifier: MPL-2.0

// ----------------------------------------------------------------------------
//
//     ***     AUTO GENERATED CODE    ***    Type: MMv1     ***
//
// ----------------------------------------------------------------------------
//
//     This file is automatically generated by Magic Modules and manual
//     changes will be clobbered when the file is regenerated.
//
//     Please read more about how to change this file in
//     .github/CONTRIBUTING.md.
//
// ----------------------------------------------------------------------------

package vertexai

import (
	"fmt"
	"log"
	"net/http"
	"reflect"
	"regexp"
	"time"

	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/customdiff"
	"github.com/hashicorp/terraform-plugin-sdk/v2/helper/schema"

	"github.com/hashicorp/terraform-provider-google-beta/google-beta/tpgresource"
	transport_tpg "github.com/hashicorp/terraform-provider-google-beta/google-beta/transport"
	"github.com/hashicorp/terraform-provider-google-beta/google-beta/verify"
)

func ResourceVertexAIModels() *schema.Resource {
	return &schema.Resource{
		Create: resourceVertexAIModelsCreate,
		Read:   resourceVertexAIModelsRead,
		Update: resourceVertexAIModelsUpdate,
		Delete: resourceVertexAIModelsDelete,

		Timeouts: &schema.ResourceTimeout{
			Create: schema.DefaultTimeout(20 * time.Minute),
			Update: schema.DefaultTimeout(20 * time.Minute),
			Delete: schema.DefaultTimeout(20 * time.Minute),
		},

		CustomizeDiff: customdiff.All(
			tpgresource.SetLabelsDiff,
			tpgresource.DefaultProviderProject,
		),

		Schema: map[string]*schema.Schema{
			"artifact_uri": {
				Type:        schema.TypeString,
				Optional:    true,
				ForceNew:    true,
				Description: `The path to the directory containing the Model artifact and any of its supporting files.`,
			},
			"container_spec": {
				Type:        schema.TypeList,
				Optional:    true,
				Description: `The specification of the container that is to be used when deploying this Model.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"image_uri": {
							Type:        schema.TypeString,
							Required:    true,
							ForceNew:    true,
							Description: `URI of the Docker image to be used as the custom container for serving predictions.`,
						},
						"args": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `Specifies arguments for the command that runs when the container starts.`,
							Elem: &schema.Schema{
								Type: schema.TypeString,
							},
						},
						"command": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `Specifies the command that runs when the container starts. This overrides the container's ENTRYPOINT.`,
							Elem: &schema.Schema{
								Type: schema.TypeString,
							},
						},
						"deployment_timeout": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: `Deployment timeout. Limit for deployment timeout is 2 hours.`,
						},
						"env": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `List of environment variables to set in the container.`,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"name": {
										Type:        schema.TypeString,
										Required:    true,
										Description: `name of the environment variable. Must be a valid C identifier.`,
									},
									"value": {
										Type:        schema.TypeString,
										Required:    true,
										Description: `Variables that reference a $(VAR_NAME) are expanded using the previous defined environment variables in the container and any service environment variables.`,
									},
								},
							},
						},
						"grpc_ports": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `List of ports to expose from the container.`,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"container_port": {
										Type:        schema.TypeInt,
										Required:    true,
										Description: `The number of the port to expose on the pod's IP address. Must be a valid port number, between 1 and 65535 inclusive.`,
									},
								},
							},
						},
						"health_probe": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `Specification for Kubernetes readiness probe.`,
							MaxItems:    1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"exec": {
										Type:        schema.TypeList,
										Optional:    true,
										Description: `Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. Must be greater or equal to periodSeconds.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"command": {
													Type:        schema.TypeString,
													Optional:    true,
													Description: `Command is the command line to execute inside the container, the working directory for the command is root ('/') in the container's filesystem.`,
												},
											},
										},
									},
									"period_seconds": {
										Type:        schema.TypeInt,
										Optional:    true,
										ForceNew:    true,
										Description: `How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1. Must be less than timeoutSeconds.`,
										Default:     10,
									},
									"timeout_seconds": {
										Type:        schema.TypeInt,
										Optional:    true,
										ForceNew:    true,
										Description: `Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. Must be greater or equal to periodSeconds.`,
										Default:     1,
									},
								},
							},
						},
						"health_route": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: `HTTP path on the container to send health checks to.`,
						},
						"ports": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `List of ports to expose from the container.`,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"container_port": {
										Type:        schema.TypeInt,
										Required:    true,
										Description: `The number of the port to expose on the pod's IP address. Must be a valid port number, between 1 and 65535 inclusive.`,
									},
								},
							},
						},
						"predict_route": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: `HTTP path on the container to send prediction requests to.`,
						},
						"shared_memory_size_mb": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: `The amount of the VM memory to reserve as the shared memory for the model in megabytes.`,
						},
						"startup_probe": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `Specification for Kubernetes startup probe.`,
							MaxItems:    1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"exec": {
										Type:        schema.TypeList,
										Optional:    true,
										Description: `Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. Must be greater or equal to periodSeconds.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"command": {
													Type:        schema.TypeString,
													Optional:    true,
													Description: `Command is the command line to execute inside the container, the working directory for the command is root ('/') in the container's filesystem.`,
												},
											},
										},
									},
									"period_seconds": {
										Type:        schema.TypeInt,
										Optional:    true,
										ForceNew:    true,
										Description: `How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1. Must be less than timeoutSeconds.`,
										Default:     10,
									},
									"timeout_seconds": {
										Type:        schema.TypeInt,
										Optional:    true,
										ForceNew:    true,
										Description: `Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. Must be greater or equal to periodSeconds.`,
										Default:     1,
									},
								},
							},
						},
					},
				},
			},
			"description": {
				Type:        schema.TypeString,
				Optional:    true,
				ForceNew:    true,
				Description: `Description of the Model.`,
			},
			"display_name": {
				Type:        schema.TypeString,
				Optional:    true,
				Description: `The display name of the Model. The name can be up to 128 characters long and can consist of any UTF-8 characters.`,
			},
			"encryption_spec": {
				Type:        schema.TypeList,
				Optional:    true,
				ForceNew:    true,
				Description: `Customer-managed encryption key spec for a MetadataStore. If set, this MetadataStore and all sub-resources of this MetadataStore will be secured by this key.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"kms_key_name": {
							Type:     schema.TypeString,
							Optional: true,
							ForceNew: true,
							Description: `Required. The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource.
Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the resource is created.`,
						},
					},
				},
			},
			"labels": {
				Type:     schema.TypeMap,
				Optional: true,
				Description: `The labels with user-defined metadata to organize your Models.

**Note**: This field is non-authoritative, and will only manage the labels present in your configuration.
Please refer to the field 'effective_labels' for all of the labels present on the resource.`,
				Elem: &schema.Schema{Type: schema.TypeString},
			},
			"metadata": {
				Type:        schema.TypeList,
				Optional:    true,
				Description: `An additional information about the Index`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"contents_delta_uri": {
							Type:     schema.TypeString,
							Required: true,
							Description: `Allows inserting, updating  or deleting the contents of the Matching Engine Index.
The string must be a valid Cloud Storage directory path. If this
field is set when calling IndexService.UpdateIndex, then no other
Index field can be also updated as part of the same call.
The expected structure and format of the files this URI points to is
described at https://cloud.google.com/vertex-ai/docs/matching-engine/using-matching-engine#input-data-format`,
						},
						"config": {
							Type:        schema.TypeList,
							Optional:    true,
							ForceNew:    true,
							Description: `The configuration of the Matching Engine Index.`,
							MaxItems:    1,
							Elem: &schema.Resource{
								Schema: map[string]*schema.Schema{
									"dimensions": {
										Type:        schema.TypeInt,
										Required:    true,
										Description: `The number of dimensions of the input vectors.`,
									},
									"algorithm_config": {
										Type:        schema.TypeList,
										Optional:    true,
										Description: `The configuration with regard to the algorithms used for efficient search.`,
										MaxItems:    1,
										Elem: &schema.Resource{
											Schema: map[string]*schema.Schema{
												"brute_force_config": {
													Type:     schema.TypeList,
													Optional: true,
													Description: `Configuration options for using brute force search, which simply implements the
standard linear search in the database for each query.`,
													MaxItems: 1,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{},
													},
													ExactlyOneOf: []string{},
												},
												"tree_ah_config": {
													Type:     schema.TypeList,
													Optional: true,
													Description: `Configuration options for using the tree-AH algorithm (Shallow tree + Asymmetric Hashing).
Please refer to this paper for more details: https://arxiv.org/abs/1908.10396`,
													MaxItems: 1,
													Elem: &schema.Resource{
														Schema: map[string]*schema.Schema{
															"leaf_node_embedding_count": {
																Type:        schema.TypeInt,
																Optional:    true,
																Description: `Number of embeddings on each leaf node. The default value is 1000 if not set.`,
																Default:     1000,
															},
															"leaf_nodes_to_search_percent": {
																Type:     schema.TypeInt,
																Optional: true,
																Description: `The default percentage of leaf nodes that any query may be searched. Must be in
range 1-100, inclusive. The default value is 10 (means 10%) if not set.`,
																Default: 10,
															},
														},
													},
													ExactlyOneOf: []string{},
												},
											},
										},
									},
									"approximate_neighbors_count": {
										Type:     schema.TypeInt,
										Optional: true,
										Description: `The default number of neighbors to find via approximate search before exact reordering is
performed. Exact reordering is a procedure where results returned by an
approximate search algorithm are reordered via a more expensive distance computation.
Required if tree-AH algorithm is used.`,
									},
									"distance_measure_type": {
										Type:     schema.TypeString,
										Optional: true,
										Description: `The distance measure used in nearest neighbor search. The value must be one of the followings:
* SQUARED_L2_DISTANCE: Euclidean (L_2) Distance
* L1_DISTANCE: Manhattan (L_1) Distance
* COSINE_DISTANCE: Cosine Distance. Defined as 1 - cosine similarity.
* DOT_PRODUCT_DISTANCE: Dot Product Distance. Defined as a negative of the dot product`,
										Default: "DOT_PRODUCT_DISTANCE",
									},
									"feature_norm_type": {
										Type:     schema.TypeString,
										Optional: true,
										Description: `Type of normalization to be carried out on each vector. The value must be one of the followings:
* UNIT_L2_NORM: Unit L2 normalization type
* NONE: No normalization type is specified.`,
										Default: "NONE",
									},
									"shard_size": {
										Type:     schema.TypeString,
										Computed: true,
										Optional: true,
										ForceNew: true,
										Description: `Index data is split into equal parts to be processed. These are called "shards".
The shard size must be specified when creating an index. The value must be one of the followings:
* SHARD_SIZE_SMALL: Small (2GB)
* SHARD_SIZE_MEDIUM: Medium (20GB)
* SHARD_SIZE_LARGE: Large (50GB)`,
									},
								},
							},
						},
						"is_complete_overwrite": {
							Type:     schema.TypeBool,
							Optional: true,
							Description: `If this field is set together with contentsDeltaUri when calling IndexService.UpdateIndex,
then existing content of the Index will be replaced by the data from the contentsDeltaUri.`,
							Default: false,
						},
					},
				},
			},
			"metadata_schema_uri": {
				Type:        schema.TypeString,
				Optional:    true,
				ForceNew:    true,
				Description: `Points to a YAML file stored on Google Cloud Storage describing additional information about the Model, that is specific to it.`,
			},
			"model": {
				Type:             schema.TypeString,
				Optional:         true,
				DiffSuppressFunc: tpgresource.CompareSelfLinkOrResourceName,
				Description:      `The Model to create.`,
				AtLeastOneOf:     []string{"source_model", "model"},
			},
			"model_id": {
				Type:        schema.TypeString,
				Optional:    true,
				Description: `Copy sourceModel into a new Model with this ID. The ID will become the final component of the model resource name.`,
			},
			"pipeline_job": {
				Type:        schema.TypeString,
				Optional:    true,
				Description: `This field is populated if the model is produced by a pipeline job.`,
			},
			"predict_schemata": {
				Type:        schema.TypeList,
				Optional:    true,
				Description: `The schemata that describe formats of the Model's predictions and explanations as given and returned via PredictionService.Predict and PredictionService.Explain.`,
				MaxItems:    1,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"instance_schema_uri": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: `Points to a YAML file stored on Google Cloud Storage describing the format of a single instance, which are used in PredictRequest.instances, ExplainRequest.instances and BatchPredictionJob.input_config.`,
						},
						"parameters_schema_uri": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: `Points to a YAML file stored on Google Cloud Storage describing the parameters of prediction and explanation via PredictRequest.parameters, ExplainRequest.parameters and BatchPredictionJob.model_parameters.`,
						},
						"prediction_schema_uri": {
							Type:        schema.TypeString,
							Optional:    true,
							ForceNew:    true,
							Description: `Points to a YAML file stored on Google Cloud Storage describing the format of a single prediction produced by this Model, which are returned via PredictResponse.predictions, ExplainResponse.explanations, and BatchPredictionJob.output_config.`,
						},
					},
				},
			},
			"region": {
				Type:        schema.TypeString,
				Computed:    true,
				Optional:    true,
				ForceNew:    true,
				Description: `The region of the Model. eg us-central1`,
			},
			"source_model": {
				Type:         schema.TypeString,
				Optional:     true,
				Description:  `The resource name of the Model to copy. That Model must be in the same Project`,
				AtLeastOneOf: []string{"model", "source_model"},
			},
			"version_aliases": {
				Type:        schema.TypeList,
				Optional:    true,
				Description: `user provided version aliases so that a model version can be referenced via alias.`,
				Elem: &schema.Schema{
					Type: schema.TypeString,
				},
			},
			"create_time": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The timestamp of when the Model was created in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits.`,
			},
			"effective_labels": {
				Type:        schema.TypeMap,
				Computed:    true,
				Description: `All of labels (key/value pairs) present on the resource in GCP, including the labels configured through Terraform, other clients and services.`,
				Elem:        &schema.Schema{Type: schema.TypeString},
			},
			"etag": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `Used to perform consistent read-modify-write updates.`,
			},
			"name": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The name of the Model. This value may be up to 60 characters, and valid characters are [a-z0-9_]. The first character cannot be a number.`,
			},
			"supported_export_formats": {
				Type:        schema.TypeList,
				Computed:    true,
				Description: `The formats in which this Model may be exported. If empty, this Model is not available for export.`,
				Elem: &schema.Resource{
					Schema: map[string]*schema.Schema{
						"exportable_content": {
							Type:         schema.TypeString,
							Optional:     true,
							ForceNew:     true,
							ValidateFunc: verify.ValidateEnum([]string{":ARTIFACT", ":IMAGE", ""}),
							Description:  `The content of this Model that may be exported. Possible values: [":ARTIFACT", ":IMAGE"]`,
						},
						"id": {
							Type:        schema.TypeString,
							Computed:    true,
							Description: `The ID of the export format.`,
						},
					},
				},
			},
			"terraform_labels": {
				Type:     schema.TypeMap,
				Computed: true,
				Description: `The combination of labels configured directly on the resource
 and default labels configured on the provider.`,
				Elem: &schema.Schema{Type: schema.TypeString},
			},
			"trainer_pipeline": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The resource name of the TrainingPipeline that uploaded this Model, if any.`,
			},
			"update_time": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The timestamp of when the MetadataStore was last updated in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits.`,
			},
			"version_create_time": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The timestamp of when this version was created in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits.`,
			},
			"version_description": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The description of this version`,
			},
			"version_id": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The version ID of the Model.`,
			},
			"version_update_time": {
				Type:        schema.TypeString,
				Computed:    true,
				Description: `The timestamp of when this version was created in RFC3339 UTC "Zulu" format, with nanosecond resolution and up to nine fractional digits.`,
			},
			"project": {
				Type:     schema.TypeString,
				Optional: true,
				Computed: true,
				ForceNew: true,
			},
		},
		UseJSONNumber: true,
	}
}

func resourceVertexAIModelsCreate(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*transport_tpg.Config)
	userAgent, err := tpgresource.GenerateUserAgentString(d, config.UserAgent)
	if err != nil {
		return err
	}

	obj := make(map[string]interface{})
	descriptionProp, err := expandVertexAIModelsDescription(d.Get("description"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("description"); !tpgresource.IsEmptyValue(reflect.ValueOf(descriptionProp)) && (ok || !reflect.DeepEqual(v, descriptionProp)) {
		obj["description"] = descriptionProp
	}
	metadataSchemaUriProp, err := expandVertexAIModelsMetadataSchemaUri(d.Get("metadata_schema_uri"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("metadata_schema_uri"); !tpgresource.IsEmptyValue(reflect.ValueOf(metadataSchemaUriProp)) && (ok || !reflect.DeepEqual(v, metadataSchemaUriProp)) {
		obj["metadataSchemaUri"] = metadataSchemaUriProp
	}
	predictSchemataProp, err := expandVertexAIModelsPredictSchemata(d.Get("predict_schemata"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("predict_schemata"); !tpgresource.IsEmptyValue(reflect.ValueOf(predictSchemataProp)) && (ok || !reflect.DeepEqual(v, predictSchemataProp)) {
		obj["predictSchemata"] = predictSchemataProp
	}
	pipelineJobProp, err := expandVertexAIModelsPipelineJob(d.Get("pipeline_job"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("pipeline_job"); !tpgresource.IsEmptyValue(reflect.ValueOf(pipelineJobProp)) && (ok || !reflect.DeepEqual(v, pipelineJobProp)) {
		obj["pipelineJob"] = pipelineJobProp
	}
	containerSpecProp, err := expandVertexAIModelsContainerSpec(d.Get("container_spec"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("container_spec"); !tpgresource.IsEmptyValue(reflect.ValueOf(containerSpecProp)) && (ok || !reflect.DeepEqual(v, containerSpecProp)) {
		obj["containerSpec"] = containerSpecProp
	}
	artifactUriProp, err := expandVertexAIModelsArtifactUri(d.Get("artifact_uri"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("artifact_uri"); !tpgresource.IsEmptyValue(reflect.ValueOf(artifactUriProp)) && (ok || !reflect.DeepEqual(v, artifactUriProp)) {
		obj["artifactUri"] = artifactUriProp
	}
	displayNameProp, err := expandVertexAIModelsDisplayName(d.Get("display_name"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("display_name"); !tpgresource.IsEmptyValue(reflect.ValueOf(displayNameProp)) && (ok || !reflect.DeepEqual(v, displayNameProp)) {
		obj["displayName"] = displayNameProp
	}
	versionAliasesProp, err := expandVertexAIModelsVersionAliases(d.Get("version_aliases"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("version_aliases"); !tpgresource.IsEmptyValue(reflect.ValueOf(versionAliasesProp)) && (ok || !reflect.DeepEqual(v, versionAliasesProp)) {
		obj["versionAliases"] = versionAliasesProp
	}
	encryptionSpecProp, err := expandVertexAIModelsEncryptionSpec(d.Get("encryption_spec"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("encryption_spec"); !tpgresource.IsEmptyValue(reflect.ValueOf(encryptionSpecProp)) && (ok || !reflect.DeepEqual(v, encryptionSpecProp)) {
		obj["encryptionSpec"] = encryptionSpecProp
	}
	metadataProp, err := expandVertexAIModelsMetadata(d.Get("metadata"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("metadata"); !tpgresource.IsEmptyValue(reflect.ValueOf(metadataProp)) && (ok || !reflect.DeepEqual(v, metadataProp)) {
		obj["metadata"] = metadataProp
	}
	sourceModelProp, err := expandVertexAIModelsSourceModel(d.Get("source_model"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("source_model"); !tpgresource.IsEmptyValue(reflect.ValueOf(sourceModelProp)) && (ok || !reflect.DeepEqual(v, sourceModelProp)) {
		obj["sourceModel"] = sourceModelProp
	}
	modelProp, err := expandVertexAIModelsModel(d.Get("model"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("model"); !tpgresource.IsEmptyValue(reflect.ValueOf(modelProp)) && (ok || !reflect.DeepEqual(v, modelProp)) {
		obj["model"] = modelProp
	}
	modelIdProp, err := expandVertexAIModelsModelId(d.Get("model_id"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("model_id"); !tpgresource.IsEmptyValue(reflect.ValueOf(modelIdProp)) && (ok || !reflect.DeepEqual(v, modelIdProp)) {
		obj["modelId"] = modelIdProp
	}
	labelsProp, err := expandVertexAIModelsEffectiveLabels(d.Get("effective_labels"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("effective_labels"); !tpgresource.IsEmptyValue(reflect.ValueOf(labelsProp)) && (ok || !reflect.DeepEqual(v, labelsProp)) {
		obj["labels"] = labelsProp
	}

	obj, err = resourceVertexAIModelsEncoder(d, meta, obj)
	if err != nil {
		return err
	}

	url, err := tpgresource.ReplaceVars(d, config, "{{VertexAIBasePath}}projects/{{project}}/locations/{{region}}/modelsPRE_CREATE_REPLACE_ME")
	if err != nil {
		return err
	}

	log.Printf("[DEBUG] Creating new Models: %#v", obj)
	billingProject := ""

	project, err := tpgresource.GetProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for Models: %s", err)
	}
	billingProject = project

	// err == nil indicates that the billing_project value was found
	if bp, err := tpgresource.GetBillingProject(d, config); err == nil {
		billingProject = bp
	}

	headers := make(http.Header)
	var postRequestType string
	if _, ok := d.GetOk("model"); ok {
		postRequestType = ":upload"
	}

	if _, ok := d.GetOk("source_model"); ok {
		postRequestType = ":copy"
	}
	url = regexp.MustCompile("PRE_CREATE_REPLACE_ME").ReplaceAllLiteralString(url, postRequestType)
	res, err := transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
		Config:    config,
		Method:    "POST",
		Project:   billingProject,
		RawURL:    url,
		UserAgent: userAgent,
		Body:      obj,
		Timeout:   d.Timeout(schema.TimeoutCreate),
		Headers:   headers,
	})
	if err != nil {
		return fmt.Errorf("Error creating Models: %s", err)
	}
	if err := d.Set("name", flattenVertexAIModelsName(res["name"], d, config)); err != nil {
		return fmt.Errorf(`Error setting computed identity field "name": %s`, err)
	}

	// Store the ID now
	id, err := tpgresource.ReplaceVars(d, config, "{{name}}")
	if err != nil {
		return fmt.Errorf("Error constructing id: %s", err)
	}
	d.SetId(id)

	log.Printf("[DEBUG] Finished creating Models %q: %#v", d.Id(), res)

	return resourceVertexAIModelsRead(d, meta)
}

func resourceVertexAIModelsRead(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*transport_tpg.Config)
	userAgent, err := tpgresource.GenerateUserAgentString(d, config.UserAgent)
	if err != nil {
		return err
	}

	url, err := tpgresource.ReplaceVars(d, config, "{{VertexAIBasePath}}{{name}}")
	if err != nil {
		return err
	}

	billingProject := ""

	project, err := tpgresource.GetProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for Models: %s", err)
	}
	billingProject = project

	// err == nil indicates that the billing_project value was found
	if bp, err := tpgresource.GetBillingProject(d, config); err == nil {
		billingProject = bp
	}

	headers := make(http.Header)
	res, err := transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
		Config:    config,
		Method:    "GET",
		Project:   billingProject,
		RawURL:    url,
		UserAgent: userAgent,
		Headers:   headers,
	})
	if err != nil {
		return transport_tpg.HandleNotFoundError(err, d, fmt.Sprintf("VertexAIModels %q", d.Id()))
	}

	res, err = resourceVertexAIModelsDecoder(d, meta, res)
	if err != nil {
		return err
	}

	if res == nil {
		// Decoding the object has resulted in it being gone. It may be marked deleted
		log.Printf("[DEBUG] Removing VertexAIModels because it no longer exists.")
		d.SetId("")
		return nil
	}

	if err := d.Set("project", project); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}

	if err := d.Set("name", flattenVertexAIModelsName(res["name"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("description", flattenVertexAIModelsDescription(res["description"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("version_id", flattenVertexAIModelsVersionId(res["versionId"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("version_create_time", flattenVertexAIModelsVersionCreateTime(res["versionCreateTime"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("version_update_time", flattenVertexAIModelsVersionUpdateTime(res["versionUpdateTime"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("version_description", flattenVertexAIModelsVersionDescription(res["versionDescription"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("metadata_schema_uri", flattenVertexAIModelsMetadataSchemaUri(res["metadataSchemaUri"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("predict_schemata", flattenVertexAIModelsPredictSchemata(res["predictSchemata"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("supported_export_formats", flattenVertexAIModelsSupportedExportFormats(res["supportedExportFormats"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("trainer_pipeline", flattenVertexAIModelsTrainerPipeline(res["trainerPipeline"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("pipeline_job", flattenVertexAIModelsPipelineJob(res["pipelineJob"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("container_spec", flattenVertexAIModelsContainerSpec(res["containerSpec"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("artifact_uri", flattenVertexAIModelsArtifactUri(res["artifactUri"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("create_time", flattenVertexAIModelsCreateTime(res["createTime"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("update_time", flattenVertexAIModelsUpdateTime(res["updateTime"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("display_name", flattenVertexAIModelsDisplayName(res["displayName"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("version_aliases", flattenVertexAIModelsVersionAliases(res["versionAliases"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("labels", flattenVertexAIModelsLabels(res["labels"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("encryption_spec", flattenVertexAIModelsEncryptionSpec(res["encryptionSpec"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("metadata", flattenVertexAIModelsMetadata(res["metadata"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("source_model", flattenVertexAIModelsSourceModel(res["sourceModel"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("model", flattenVertexAIModelsModel(res["model"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("model_id", flattenVertexAIModelsModelId(res["modelId"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("terraform_labels", flattenVertexAIModelsTerraformLabels(res["labels"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}
	if err := d.Set("effective_labels", flattenVertexAIModelsEffectiveLabels(res["labels"], d, config)); err != nil {
		return fmt.Errorf("Error reading Models: %s", err)
	}

	return nil
}

func resourceVertexAIModelsUpdate(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*transport_tpg.Config)
	userAgent, err := tpgresource.GenerateUserAgentString(d, config.UserAgent)
	if err != nil {
		return err
	}

	billingProject := ""

	project, err := tpgresource.GetProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for Models: %s", err)
	}
	billingProject = project

	obj := make(map[string]interface{})
	descriptionProp, err := expandVertexAIModelsDescription(d.Get("description"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("description"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, descriptionProp)) {
		obj["description"] = descriptionProp
	}
	metadataSchemaUriProp, err := expandVertexAIModelsMetadataSchemaUri(d.Get("metadata_schema_uri"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("metadata_schema_uri"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, metadataSchemaUriProp)) {
		obj["metadataSchemaUri"] = metadataSchemaUriProp
	}
	predictSchemataProp, err := expandVertexAIModelsPredictSchemata(d.Get("predict_schemata"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("predict_schemata"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, predictSchemataProp)) {
		obj["predictSchemata"] = predictSchemataProp
	}
	pipelineJobProp, err := expandVertexAIModelsPipelineJob(d.Get("pipeline_job"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("pipeline_job"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, pipelineJobProp)) {
		obj["pipelineJob"] = pipelineJobProp
	}
	containerSpecProp, err := expandVertexAIModelsContainerSpec(d.Get("container_spec"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("container_spec"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, containerSpecProp)) {
		obj["containerSpec"] = containerSpecProp
	}
	artifactUriProp, err := expandVertexAIModelsArtifactUri(d.Get("artifact_uri"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("artifact_uri"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, artifactUriProp)) {
		obj["artifactUri"] = artifactUriProp
	}
	displayNameProp, err := expandVertexAIModelsDisplayName(d.Get("display_name"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("display_name"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, displayNameProp)) {
		obj["displayName"] = displayNameProp
	}
	versionAliasesProp, err := expandVertexAIModelsVersionAliases(d.Get("version_aliases"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("version_aliases"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, versionAliasesProp)) {
		obj["versionAliases"] = versionAliasesProp
	}
	encryptionSpecProp, err := expandVertexAIModelsEncryptionSpec(d.Get("encryption_spec"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("encryption_spec"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, encryptionSpecProp)) {
		obj["encryptionSpec"] = encryptionSpecProp
	}
	metadataProp, err := expandVertexAIModelsMetadata(d.Get("metadata"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("metadata"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, metadataProp)) {
		obj["metadata"] = metadataProp
	}
	sourceModelProp, err := expandVertexAIModelsSourceModel(d.Get("source_model"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("source_model"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, sourceModelProp)) {
		obj["sourceModel"] = sourceModelProp
	}
	modelProp, err := expandVertexAIModelsModel(d.Get("model"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("model"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, modelProp)) {
		obj["model"] = modelProp
	}
	modelIdProp, err := expandVertexAIModelsModelId(d.Get("model_id"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("model_id"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, modelIdProp)) {
		obj["modelId"] = modelIdProp
	}
	labelsProp, err := expandVertexAIModelsEffectiveLabels(d.Get("effective_labels"), d, config)
	if err != nil {
		return err
	} else if v, ok := d.GetOkExists("effective_labels"); !tpgresource.IsEmptyValue(reflect.ValueOf(v)) && (ok || !reflect.DeepEqual(v, labelsProp)) {
		obj["labels"] = labelsProp
	}

	obj, err = resourceVertexAIModelsEncoder(d, meta, obj)
	if err != nil {
		return err
	}

	url, err := tpgresource.ReplaceVars(d, config, "{{VertexAIBasePath}}{{name}}")
	if err != nil {
		return err
	}

	log.Printf("[DEBUG] Updating Models %q: %#v", d.Id(), obj)
	headers := make(http.Header)

	// err == nil indicates that the billing_project value was found
	if bp, err := tpgresource.GetBillingProject(d, config); err == nil {
		billingProject = bp
	}

	res, err := transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
		Config:    config,
		Method:    "PUT",
		Project:   billingProject,
		RawURL:    url,
		UserAgent: userAgent,
		Body:      obj,
		Timeout:   d.Timeout(schema.TimeoutUpdate),
		Headers:   headers,
	})

	if err != nil {
		return fmt.Errorf("Error updating Models %q: %s", d.Id(), err)
	} else {
		log.Printf("[DEBUG] Finished updating Models %q: %#v", d.Id(), res)
	}

	return resourceVertexAIModelsRead(d, meta)
}

func resourceVertexAIModelsDelete(d *schema.ResourceData, meta interface{}) error {
	config := meta.(*transport_tpg.Config)
	userAgent, err := tpgresource.GenerateUserAgentString(d, config.UserAgent)
	if err != nil {
		return err
	}

	billingProject := ""

	project, err := tpgresource.GetProject(d, config)
	if err != nil {
		return fmt.Errorf("Error fetching project for Models: %s", err)
	}
	billingProject = project

	url, err := tpgresource.ReplaceVars(d, config, "{{VertexAIBasePath}}{{name}}")
	if err != nil {
		return err
	}

	var obj map[string]interface{}

	// err == nil indicates that the billing_project value was found
	if bp, err := tpgresource.GetBillingProject(d, config); err == nil {
		billingProject = bp
	}

	headers := make(http.Header)

	log.Printf("[DEBUG] Deleting Models %q", d.Id())
	res, err := transport_tpg.SendRequest(transport_tpg.SendRequestOptions{
		Config:    config,
		Method:    "DELETE",
		Project:   billingProject,
		RawURL:    url,
		UserAgent: userAgent,
		Body:      obj,
		Timeout:   d.Timeout(schema.TimeoutDelete),
		Headers:   headers,
	})
	if err != nil {
		return transport_tpg.HandleNotFoundError(err, d, "Models")
	}

	err = VertexAIOperationWaitTime(
		config, res, project, "Deleting Models", userAgent,
		d.Timeout(schema.TimeoutDelete))

	if err != nil {
		return err
	}

	log.Printf("[DEBUG] Finished deleting Models %q: %#v", d.Id(), res)
	return nil
}

func flattenVertexAIModelsName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if d.Get("name").(string) == "" {
		return v.(string)
	}

	return d.Get("name")
}

func flattenVertexAIModelsDescription(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsVersionId(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsVersionCreateTime(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsVersionUpdateTime(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsVersionDescription(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsMetadataSchemaUri(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsPredictSchemata(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["instance_schema_uri"] =
		flattenVertexAIModelsPredictSchemataInstanceSchemaUri(original["instanceSchemaUri"], d, config)
	transformed["parameters_schema_uri"] =
		flattenVertexAIModelsPredictSchemataParametersSchemaUri(original["parametersSchemaUri"], d, config)
	transformed["prediction_schema_uri"] =
		flattenVertexAIModelsPredictSchemataPredictionSchemaUri(original["predictionSchemaUri"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsPredictSchemataInstanceSchemaUri(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsPredictSchemataParametersSchemaUri(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsPredictSchemataPredictionSchemaUri(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsSupportedExportFormats(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"id":                 flattenVertexAIModelsSupportedExportFormatsId(original["id"], d, config),
			"exportable_content": flattenVertexAIModelsSupportedExportFormatsExportableContent(original["exportableContent"], d, config),
		})
	}
	return transformed
}
func flattenVertexAIModelsSupportedExportFormatsId(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsSupportedExportFormatsExportableContent(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsTrainerPipeline(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsPipelineJob(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsContainerSpec(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["image_uri"] =
		flattenVertexAIModelsContainerSpecImageUri(original["imageUri"], d, config)
	transformed["command"] =
		flattenVertexAIModelsContainerSpecCommand(original["command"], d, config)
	transformed["args"] =
		flattenVertexAIModelsContainerSpecArgs(original["args"], d, config)
	transformed["env"] =
		flattenVertexAIModelsContainerSpecEnv(original["env"], d, config)
	transformed["ports"] =
		flattenVertexAIModelsContainerSpecPorts(original["ports"], d, config)
	transformed["predict_route"] =
		flattenVertexAIModelsContainerSpecPredictRoute(original["predictRoute"], d, config)
	transformed["health_route"] =
		flattenVertexAIModelsContainerSpecHealthRoute(original["healthRoute"], d, config)
	transformed["grpc_ports"] =
		flattenVertexAIModelsContainerSpecGrpcPorts(original["grpcPorts"], d, config)
	transformed["deployment_timeout"] =
		flattenVertexAIModelsContainerSpecDeploymentTimeout(original["deploymentTimeout"], d, config)
	transformed["shared_memory_size_mb"] =
		flattenVertexAIModelsContainerSpecSharedMemorySizeMb(original["sharedMemorySizeMb"], d, config)
	transformed["startup_probe"] =
		flattenVertexAIModelsContainerSpecStartupProbe(original["startupProbe"], d, config)
	transformed["health_probe"] =
		flattenVertexAIModelsContainerSpecHealthProbe(original["healthProbe"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsContainerSpecImageUri(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsContainerSpecCommand(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsContainerSpecArgs(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsContainerSpecEnv(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"name":  flattenVertexAIModelsContainerSpecEnvName(original["name"], d, config),
			"value": flattenVertexAIModelsContainerSpecEnvValue(original["value"], d, config),
		})
	}
	return transformed
}
func flattenVertexAIModelsContainerSpecEnvName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsContainerSpecEnvValue(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsContainerSpecPorts(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"container_port": flattenVertexAIModelsContainerSpecPortsContainerPort(original["containerPort"], d, config),
		})
	}
	return transformed
}
func flattenVertexAIModelsContainerSpecPortsContainerPort(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIModelsContainerSpecPredictRoute(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsContainerSpecHealthRoute(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsContainerSpecGrpcPorts(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	l := v.([]interface{})
	transformed := make([]interface{}, 0, len(l))
	for _, raw := range l {
		original := raw.(map[string]interface{})
		if len(original) < 1 {
			// Do not include empty json objects coming back from the api
			continue
		}
		transformed = append(transformed, map[string]interface{}{
			"container_port": flattenVertexAIModelsContainerSpecGrpcPortsContainerPort(original["containerPort"], d, config),
		})
	}
	return transformed
}
func flattenVertexAIModelsContainerSpecGrpcPortsContainerPort(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIModelsContainerSpecDeploymentTimeout(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsContainerSpecSharedMemorySizeMb(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsContainerSpecStartupProbe(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["period_seconds"] =
		flattenVertexAIModelsContainerSpecStartupProbePeriodSeconds(original["periodSeconds"], d, config)
	transformed["timeout_seconds"] =
		flattenVertexAIModelsContainerSpecStartupProbeTimeoutSeconds(original["timeoutSeconds"], d, config)
	transformed["exec"] =
		flattenVertexAIModelsContainerSpecStartupProbeExec(original["exec"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsContainerSpecStartupProbePeriodSeconds(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIModelsContainerSpecStartupProbeTimeoutSeconds(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIModelsContainerSpecStartupProbeExec(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["command"] =
		flattenVertexAIModelsContainerSpecStartupProbeExecCommand(original["command"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsContainerSpecStartupProbeExecCommand(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsContainerSpecHealthProbe(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["period_seconds"] =
		flattenVertexAIModelsContainerSpecHealthProbePeriodSeconds(original["periodSeconds"], d, config)
	transformed["timeout_seconds"] =
		flattenVertexAIModelsContainerSpecHealthProbeTimeoutSeconds(original["timeoutSeconds"], d, config)
	transformed["exec"] =
		flattenVertexAIModelsContainerSpecHealthProbeExec(original["exec"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsContainerSpecHealthProbePeriodSeconds(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIModelsContainerSpecHealthProbeTimeoutSeconds(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIModelsContainerSpecHealthProbeExec(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["command"] =
		flattenVertexAIModelsContainerSpecHealthProbeExecCommand(original["command"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsContainerSpecHealthProbeExecCommand(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsArtifactUri(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsCreateTime(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsUpdateTime(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsDisplayName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsVersionAliases(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsLabels(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}

	transformed := make(map[string]interface{})
	if l, ok := d.GetOkExists("labels"); ok {
		for k := range l.(map[string]interface{}) {
			transformed[k] = v.(map[string]interface{})[k]
		}
	}

	return transformed
}

func flattenVertexAIModelsEncryptionSpec(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["kms_key_name"] =
		flattenVertexAIModelsEncryptionSpecKmsKeyName(original["kmsKeyName"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsEncryptionSpecKmsKeyName(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsMetadata(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["contents_delta_uri"] =
		flattenVertexAIModelsMetadataContentsDeltaUri(original["contentsDeltaUri"], d, config)
	transformed["is_complete_overwrite"] =
		flattenVertexAIModelsMetadataIsCompleteOverwrite(original["isCompleteOverwrite"], d, config)
	transformed["config"] =
		flattenVertexAIModelsMetadataConfig(original["config"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsMetadataContentsDeltaUri(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// We want to ignore read on this field, but cannot because it is nested
	return d.Get("metadata.0.contents_delta_uri")
}

func flattenVertexAIModelsMetadataIsCompleteOverwrite(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// We want to ignore read on this field, but cannot because it is nested
	return d.Get("metadata.0.is_complete_overwrite")
}

func flattenVertexAIModelsMetadataConfig(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["dimensions"] =
		flattenVertexAIModelsMetadataConfigDimensions(original["dimensions"], d, config)
	transformed["approximate_neighbors_count"] =
		flattenVertexAIModelsMetadataConfigApproximateNeighborsCount(original["approximateNeighborsCount"], d, config)
	transformed["shard_size"] =
		flattenVertexAIModelsMetadataConfigShardSize(original["shardSize"], d, config)
	transformed["distance_measure_type"] =
		flattenVertexAIModelsMetadataConfigDistanceMeasureType(original["distanceMeasureType"], d, config)
	transformed["feature_norm_type"] =
		flattenVertexAIModelsMetadataConfigFeatureNormType(original["featureNormType"], d, config)
	transformed["algorithm_config"] =
		flattenVertexAIModelsMetadataConfigAlgorithmConfig(original["algorithmConfig"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsMetadataConfigDimensions(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIModelsMetadataConfigApproximateNeighborsCount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIModelsMetadataConfigShardSize(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsMetadataConfigDistanceMeasureType(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsMetadataConfigFeatureNormType(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return d.Get("metadata.0.config.0.feature_norm_type")
}

func flattenVertexAIModelsMetadataConfigAlgorithmConfig(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["tree_ah_config"] =
		flattenVertexAIModelsMetadataConfigAlgorithmConfigTreeAhConfig(original["treeAhConfig"], d, config)
	transformed["brute_force_config"] =
		flattenVertexAIModelsMetadataConfigAlgorithmConfigBruteForceConfig(original["bruteForceConfig"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsMetadataConfigAlgorithmConfigTreeAhConfig(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	original := v.(map[string]interface{})
	if len(original) == 0 {
		return nil
	}
	transformed := make(map[string]interface{})
	transformed["leaf_node_embedding_count"] =
		flattenVertexAIModelsMetadataConfigAlgorithmConfigTreeAhConfigLeafNodeEmbeddingCount(original["leafNodeEmbeddingCount"], d, config)
	transformed["leaf_nodes_to_search_percent"] =
		flattenVertexAIModelsMetadataConfigAlgorithmConfigTreeAhConfigLeafNodesToSearchPercent(original["leafNodesToSearchPercent"], d, config)
	return []interface{}{transformed}
}
func flattenVertexAIModelsMetadataConfigAlgorithmConfigTreeAhConfigLeafNodeEmbeddingCount(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIModelsMetadataConfigAlgorithmConfigTreeAhConfigLeafNodesToSearchPercent(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	// Handles the string fixed64 format
	if strVal, ok := v.(string); ok {
		if intVal, err := tpgresource.StringToFixed64(strVal); err == nil {
			return intVal
		}
	}

	// number values are represented as float64
	if floatVal, ok := v.(float64); ok {
		intVal := int(floatVal)
		return intVal
	}

	return v // let terraform core handle it otherwise
}

func flattenVertexAIModelsMetadataConfigAlgorithmConfigBruteForceConfig(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return nil
	}
	transformed := make(map[string]interface{})
	return []interface{}{transformed}
}

func flattenVertexAIModelsSourceModel(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsModel(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}
	return tpgresource.ConvertSelfLinkToV1(v.(string))
}

func flattenVertexAIModelsModelId(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func flattenVertexAIModelsTerraformLabels(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	if v == nil {
		return v
	}

	transformed := make(map[string]interface{})
	if l, ok := d.GetOkExists("terraform_labels"); ok {
		for k := range l.(map[string]interface{}) {
			transformed[k] = v.(map[string]interface{})[k]
		}
	}

	return transformed
}

func flattenVertexAIModelsEffectiveLabels(v interface{}, d *schema.ResourceData, config *transport_tpg.Config) interface{} {
	return v
}

func expandVertexAIModelsDescription(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsMetadataSchemaUri(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsPredictSchemata(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedInstanceSchemaUri, err := expandVertexAIModelsPredictSchemataInstanceSchemaUri(original["instance_schema_uri"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedInstanceSchemaUri); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["instanceSchemaUri"] = transformedInstanceSchemaUri
	}

	transformedParametersSchemaUri, err := expandVertexAIModelsPredictSchemataParametersSchemaUri(original["parameters_schema_uri"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedParametersSchemaUri); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["parametersSchemaUri"] = transformedParametersSchemaUri
	}

	transformedPredictionSchemaUri, err := expandVertexAIModelsPredictSchemataPredictionSchemaUri(original["prediction_schema_uri"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPredictionSchemaUri); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["predictionSchemaUri"] = transformedPredictionSchemaUri
	}

	return transformed, nil
}

func expandVertexAIModelsPredictSchemataInstanceSchemaUri(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsPredictSchemataParametersSchemaUri(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsPredictSchemataPredictionSchemaUri(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsPipelineJob(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsContainerSpec(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedImageUri, err := expandVertexAIModelsContainerSpecImageUri(original["image_uri"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedImageUri); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["imageUri"] = transformedImageUri
	}

	transformedCommand, err := expandVertexAIModelsContainerSpecCommand(original["command"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedCommand); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["command"] = transformedCommand
	}

	transformedArgs, err := expandVertexAIModelsContainerSpecArgs(original["args"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedArgs); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["args"] = transformedArgs
	}

	transformedEnv, err := expandVertexAIModelsContainerSpecEnv(original["env"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedEnv); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["env"] = transformedEnv
	}

	transformedPorts, err := expandVertexAIModelsContainerSpecPorts(original["ports"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPorts); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["ports"] = transformedPorts
	}

	transformedPredictRoute, err := expandVertexAIModelsContainerSpecPredictRoute(original["predict_route"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPredictRoute); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["predictRoute"] = transformedPredictRoute
	}

	transformedHealthRoute, err := expandVertexAIModelsContainerSpecHealthRoute(original["health_route"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedHealthRoute); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["healthRoute"] = transformedHealthRoute
	}

	transformedGrpcPorts, err := expandVertexAIModelsContainerSpecGrpcPorts(original["grpc_ports"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedGrpcPorts); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["grpcPorts"] = transformedGrpcPorts
	}

	transformedDeploymentTimeout, err := expandVertexAIModelsContainerSpecDeploymentTimeout(original["deployment_timeout"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedDeploymentTimeout); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["deploymentTimeout"] = transformedDeploymentTimeout
	}

	transformedSharedMemorySizeMb, err := expandVertexAIModelsContainerSpecSharedMemorySizeMb(original["shared_memory_size_mb"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedSharedMemorySizeMb); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["sharedMemorySizeMb"] = transformedSharedMemorySizeMb
	}

	transformedStartupProbe, err := expandVertexAIModelsContainerSpecStartupProbe(original["startup_probe"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedStartupProbe); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["startupProbe"] = transformedStartupProbe
	}

	transformedHealthProbe, err := expandVertexAIModelsContainerSpecHealthProbe(original["health_probe"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedHealthProbe); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["healthProbe"] = transformedHealthProbe
	}

	return transformed, nil
}

func expandVertexAIModelsContainerSpecImageUri(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsContainerSpecCommand(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsContainerSpecArgs(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsContainerSpecEnv(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	req := make([]interface{}, 0, len(l))
	for _, raw := range l {
		if raw == nil {
			continue
		}
		original := raw.(map[string]interface{})
		transformed := make(map[string]interface{})

		transformedName, err := expandVertexAIModelsContainerSpecEnvName(original["name"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedName); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["name"] = transformedName
		}

		transformedValue, err := expandVertexAIModelsContainerSpecEnvValue(original["value"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedValue); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["value"] = transformedValue
		}

		req = append(req, transformed)
	}
	return req, nil
}

func expandVertexAIModelsContainerSpecEnvName(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsContainerSpecEnvValue(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsContainerSpecPorts(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	req := make([]interface{}, 0, len(l))
	for _, raw := range l {
		if raw == nil {
			continue
		}
		original := raw.(map[string]interface{})
		transformed := make(map[string]interface{})

		transformedContainerPort, err := expandVertexAIModelsContainerSpecPortsContainerPort(original["container_port"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedContainerPort); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["containerPort"] = transformedContainerPort
		}

		req = append(req, transformed)
	}
	return req, nil
}

func expandVertexAIModelsContainerSpecPortsContainerPort(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsContainerSpecPredictRoute(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsContainerSpecHealthRoute(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsContainerSpecGrpcPorts(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	req := make([]interface{}, 0, len(l))
	for _, raw := range l {
		if raw == nil {
			continue
		}
		original := raw.(map[string]interface{})
		transformed := make(map[string]interface{})

		transformedContainerPort, err := expandVertexAIModelsContainerSpecGrpcPortsContainerPort(original["container_port"], d, config)
		if err != nil {
			return nil, err
		} else if val := reflect.ValueOf(transformedContainerPort); val.IsValid() && !tpgresource.IsEmptyValue(val) {
			transformed["containerPort"] = transformedContainerPort
		}

		req = append(req, transformed)
	}
	return req, nil
}

func expandVertexAIModelsContainerSpecGrpcPortsContainerPort(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsContainerSpecDeploymentTimeout(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsContainerSpecSharedMemorySizeMb(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsContainerSpecStartupProbe(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedPeriodSeconds, err := expandVertexAIModelsContainerSpecStartupProbePeriodSeconds(original["period_seconds"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPeriodSeconds); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["periodSeconds"] = transformedPeriodSeconds
	}

	transformedTimeoutSeconds, err := expandVertexAIModelsContainerSpecStartupProbeTimeoutSeconds(original["timeout_seconds"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedTimeoutSeconds); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["timeoutSeconds"] = transformedTimeoutSeconds
	}

	transformedExec, err := expandVertexAIModelsContainerSpecStartupProbeExec(original["exec"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedExec); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["exec"] = transformedExec
	}

	return transformed, nil
}

func expandVertexAIModelsContainerSpecStartupProbePeriodSeconds(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsContainerSpecStartupProbeTimeoutSeconds(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsContainerSpecStartupProbeExec(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedCommand, err := expandVertexAIModelsContainerSpecStartupProbeExecCommand(original["command"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedCommand); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["command"] = transformedCommand
	}

	return transformed, nil
}

func expandVertexAIModelsContainerSpecStartupProbeExecCommand(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsContainerSpecHealthProbe(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedPeriodSeconds, err := expandVertexAIModelsContainerSpecHealthProbePeriodSeconds(original["period_seconds"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedPeriodSeconds); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["periodSeconds"] = transformedPeriodSeconds
	}

	transformedTimeoutSeconds, err := expandVertexAIModelsContainerSpecHealthProbeTimeoutSeconds(original["timeout_seconds"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedTimeoutSeconds); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["timeoutSeconds"] = transformedTimeoutSeconds
	}

	transformedExec, err := expandVertexAIModelsContainerSpecHealthProbeExec(original["exec"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedExec); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["exec"] = transformedExec
	}

	return transformed, nil
}

func expandVertexAIModelsContainerSpecHealthProbePeriodSeconds(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsContainerSpecHealthProbeTimeoutSeconds(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsContainerSpecHealthProbeExec(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedCommand, err := expandVertexAIModelsContainerSpecHealthProbeExecCommand(original["command"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedCommand); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["command"] = transformedCommand
	}

	return transformed, nil
}

func expandVertexAIModelsContainerSpecHealthProbeExecCommand(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsArtifactUri(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsDisplayName(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsVersionAliases(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsEncryptionSpec(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedKmsKeyName, err := expandVertexAIModelsEncryptionSpecKmsKeyName(original["kms_key_name"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedKmsKeyName); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["kmsKeyName"] = transformedKmsKeyName
	}

	return transformed, nil
}

func expandVertexAIModelsEncryptionSpecKmsKeyName(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsMetadata(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedContentsDeltaUri, err := expandVertexAIModelsMetadataContentsDeltaUri(original["contents_delta_uri"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedContentsDeltaUri); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["contentsDeltaUri"] = transformedContentsDeltaUri
	}

	transformedIsCompleteOverwrite, err := expandVertexAIModelsMetadataIsCompleteOverwrite(original["is_complete_overwrite"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedIsCompleteOverwrite); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["isCompleteOverwrite"] = transformedIsCompleteOverwrite
	}

	transformedConfig, err := expandVertexAIModelsMetadataConfig(original["config"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedConfig); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["config"] = transformedConfig
	}

	return transformed, nil
}

func expandVertexAIModelsMetadataContentsDeltaUri(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsMetadataIsCompleteOverwrite(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsMetadataConfig(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedDimensions, err := expandVertexAIModelsMetadataConfigDimensions(original["dimensions"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedDimensions); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["dimensions"] = transformedDimensions
	}

	transformedApproximateNeighborsCount, err := expandVertexAIModelsMetadataConfigApproximateNeighborsCount(original["approximate_neighbors_count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedApproximateNeighborsCount); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["approximateNeighborsCount"] = transformedApproximateNeighborsCount
	}

	transformedShardSize, err := expandVertexAIModelsMetadataConfigShardSize(original["shard_size"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedShardSize); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["shardSize"] = transformedShardSize
	}

	transformedDistanceMeasureType, err := expandVertexAIModelsMetadataConfigDistanceMeasureType(original["distance_measure_type"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedDistanceMeasureType); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["distanceMeasureType"] = transformedDistanceMeasureType
	}

	transformedFeatureNormType, err := expandVertexAIModelsMetadataConfigFeatureNormType(original["feature_norm_type"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedFeatureNormType); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["featureNormType"] = transformedFeatureNormType
	}

	transformedAlgorithmConfig, err := expandVertexAIModelsMetadataConfigAlgorithmConfig(original["algorithm_config"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedAlgorithmConfig); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["algorithmConfig"] = transformedAlgorithmConfig
	}

	return transformed, nil
}

func expandVertexAIModelsMetadataConfigDimensions(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsMetadataConfigApproximateNeighborsCount(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsMetadataConfigShardSize(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsMetadataConfigDistanceMeasureType(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsMetadataConfigFeatureNormType(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsMetadataConfigAlgorithmConfig(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedTreeAhConfig, err := expandVertexAIModelsMetadataConfigAlgorithmConfigTreeAhConfig(original["tree_ah_config"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedTreeAhConfig); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["treeAhConfig"] = transformedTreeAhConfig
	}

	transformedBruteForceConfig, err := expandVertexAIModelsMetadataConfigAlgorithmConfigBruteForceConfig(original["brute_force_config"], d, config)
	if err != nil {
		return nil, err
	} else {
		transformed["bruteForceConfig"] = transformedBruteForceConfig
	}

	return transformed, nil
}

func expandVertexAIModelsMetadataConfigAlgorithmConfigTreeAhConfig(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 || l[0] == nil {
		return nil, nil
	}
	raw := l[0]
	original := raw.(map[string]interface{})
	transformed := make(map[string]interface{})

	transformedLeafNodeEmbeddingCount, err := expandVertexAIModelsMetadataConfigAlgorithmConfigTreeAhConfigLeafNodeEmbeddingCount(original["leaf_node_embedding_count"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedLeafNodeEmbeddingCount); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["leafNodeEmbeddingCount"] = transformedLeafNodeEmbeddingCount
	}

	transformedLeafNodesToSearchPercent, err := expandVertexAIModelsMetadataConfigAlgorithmConfigTreeAhConfigLeafNodesToSearchPercent(original["leaf_nodes_to_search_percent"], d, config)
	if err != nil {
		return nil, err
	} else if val := reflect.ValueOf(transformedLeafNodesToSearchPercent); val.IsValid() && !tpgresource.IsEmptyValue(val) {
		transformed["leafNodesToSearchPercent"] = transformedLeafNodesToSearchPercent
	}

	return transformed, nil
}

func expandVertexAIModelsMetadataConfigAlgorithmConfigTreeAhConfigLeafNodeEmbeddingCount(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsMetadataConfigAlgorithmConfigTreeAhConfigLeafNodesToSearchPercent(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsMetadataConfigAlgorithmConfigBruteForceConfig(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	l := v.([]interface{})
	if len(l) == 0 {
		return nil, nil
	}

	if l[0] == nil {
		transformed := make(map[string]interface{})
		return transformed, nil
	}
	transformed := make(map[string]interface{})

	return transformed, nil
}

func expandVertexAIModelsSourceModel(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsModel(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsModelId(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (interface{}, error) {
	return v, nil
}

func expandVertexAIModelsEffectiveLabels(v interface{}, d tpgresource.TerraformResourceData, config *transport_tpg.Config) (map[string]string, error) {
	if v == nil {
		return map[string]string{}, nil
	}
	m := make(map[string]string)
	for k, val := range v.(map[string]interface{}) {
		m[k] = val.(string)
	}
	return m, nil
}

func resourceVertexAIModelsEncoder(d *schema.ResourceData, meta interface{}, obj map[string]interface{}) (map[string]interface{}, error) {
	if obj["sourceModel"] != nil {
		delete(obj, "labels")
	}

	return obj, nil
}

func resourceVertexAIModelsDecoder(d *schema.ResourceData, meta interface{}, res map[string]interface{}) (map[string]interface{}, error) {
	if res["model"] != "" {
		res["name"] = res["model"]
	}
	return res, nil
}
